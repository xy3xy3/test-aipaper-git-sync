# 1. 论文基本信息

## 1.1. 标题

**ModRWKV: Transformer Multimodality in Linear Time**

该论文的核心主题是提出并研究一种基于线性复杂度递归神经网络（RNN）架构的多模态大语言模型框架，旨在突破传统二次复杂度的 Transformer 模型限制，实现高效地跨模态信息融合与推理。

## 1.2. 作者

本文作者包括 Jiale Kang, Ziyin Yue, Qingyu Yin, Jiang Rui, Weile Li, Zening Lu 和 Zhouran Ji，来自 RWKvOS 公司、浙江大学和香港科技大学。主要作者均为 RNN 及大语言模型相关研究方向的专家，机构具备丰富的机器学习和多模态模型开发背景。

## 1.3. 发表期刊/会议

论文以预印本形式发表于 arXiv，尚未明确发表在正式期刊或会议。arXiv 是计算机科学领域公认的预发表平台，论文在该平台能获得初步社区反馈。

## 1.4. 发表年份

论文发表于 2025 年 5 月。

## 1.5. 摘要

目前，多模态研究主要基于具有二次复杂度的 Transformer 大语言模型。相比之下，线性复杂度模型如 RNN 推理成本低，但多局限于文本单模态。论文提出基于 RWKV7 现代 RNN 架构的多模态框架 ModRWKV，通过动态适配的异构模态编码器实现多源信息融合。ModRWKV 架构极度轻量，通过大量实验确定了性能与效率的平衡配置。利用 RWKV7 预训练权重初始化显著加速训练，实验证明预训练权重对多模态信号理解能力提升关键。综上，现代 RNN 架构可作为多模态大语言模型变革中 Transformer 的有效替代方案，论文系统探索并提出了 ModRWKV 的最优架构。

## 1.6. 原文链接

- 预印本网址：https://arxiv.org/abs/2505.14505
- PDF 链接：https://arxiv.org/pdf/2505.14505v1.pdf
- 状态：arXiv 预印本，未正式发表

# 2. 整体概括

## 2.1. 研究背景与动机

大语言模型（LLM）中的 Transformer 架构因其强大的建模能力成为主流，但其自注意力机制存在二次时间和空间复杂度，推理时的 KV 缓存大小随序列增长线性增加，导致长序列或多模态输入计算负担沉重。而基于递归神经网络（RNN）的线性复杂度模型，如 RWKV，具有常数内存使用且低推理成本，逐渐受到关注。

当前多模态大语言模型广泛基于 Transformer，利用视觉、音频、文本等多模态数据实现跨模态语义对齐，但线性模型在多模态领域的应用极少，主要受限于单一文本模态。此现状造成了技术具有潜力的线性模型领域多模态研究的明显空缺。

论文的切入点在于利用 RWKV7 现代 RNN 架构的时间混合机制，设计并验证一个统一的多模态框架，使得 RNN 模型在多模态理解与推理上展现竞争力，突破 Transformer 模型的范式限制，同时利用其高效推理特性。

## 2.2. 核心贡献/主要发现

1. **提出 ModRWKV 框架**：基于 RWKV7 架构，首次建立了统一多模态训练范式，采用可插拔的模态编码器设计，实现轻量级多模态信息融合，增强跨模态扩展性。

2. **全面性能评估**：通过丰富多模态评测体系验证 ModRWKV 的跨模态理解能力，奠定 RNN 结构多模态模型的基准范式。

3. **架构消融与优化**：系统性调研并找到平衡模型性能与计算效率的最优模块规模和配置，如适当的编码器选择、卷积序列压缩策略，以及预训练权重初始化带来的优势。

4. 实验表明现代 RNN 是 Transformer 在多模态大语言模型任务的可行替代方案，预训练策略对提升多模态理解能力至关重要。

# 3. 预备知识与相关工作

## 3.1. 基础概念

- <strong>大语言模型 (Large Language Model, LLM)</strong>：基于深度神经网络，特别是 Transformer 的自然语言处理模型，具备海量语言理解和生成能力，如 GPT、LLaMA、Vicuna 等。

- <strong>Transformer 架构 (Transformer architecture)</strong>：2017 年提出的基于自注意力机制的神经网络架构，以并行化训练和强大表达能力著称，但自注意力机制的时间和空间复杂度为 $O(n^2)$，$n$ 为序列长度。

- <strong>递归神经网络 (Recurrent Neural Networks, RNNs)</strong>：通过时间步递归处理序列元素的神经网络，适用于顺序数据，传统上因梯度消失问题而有限，但现代变体和优化方法提升其能力。RWKV 属于高效线性复杂度的现代 RNN。

- <strong>多模态学习 (Multimodal Learning)</strong>：融合和处理多种类型数据（如文本、图像、音频等）的学习范式，要求模型能理解并整合不同模态的信息。

- <strong>模态编码器 (Modality Encoder)</strong>：针对不同类型数据的特征提取模块，如视觉编码器（CLIP、SigLIP2）、音频编码器（WavLM、Whisper）、时间序列编码器（WaveNet、Timer）。

- <strong>适配器 (Adapter)</strong>：轻量级的模块插入模型中，用于模态对齐或任务适配，避免大规模参数调整。

- <strong>序列压缩 (Sequence Compression)</strong>：通过卷积等操作减少输入序列长度，降低计算成本，保持关键信息。

## 3.2. 前人工作

- **Transformer 与自注意力机制**：Vaswani 等人提出详细公式：

  $$
  \mathrm{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
  $$

  其中，$Q$、$K$ 和 $V$ 是查询、键、值矩阵，$d_k$ 是键的维度。自注意力用于捕获全局依赖，代价是 $O(n^2)$ 复杂度。

- **RWKV 模型**：（Peng 等，2025）提出结合线性 RNN 和 Transformer 优势的模型，采用时间混合机制；其核心状态更新为

  $$
  s_t = G_t s_{t-1} + a_t k_t v_t^T
  $$

  其中，$G_t$ 是动态通道衰减矩阵，$a_t$ 是上下文感知的学习率，$k_t$ 和 $v_t$ 是从输入中投影得到的键和值。

- <strong>多模态大语言模型 (MLLM)</strong>：LLaVA、VL-Mamba、ScienceQA 等基于 Transformer 结构，利用视觉/语音编码器结合交叉注意力、统一词元化或特征融合完成多模态任务。

- **序列压缩技术**：1D 卷积广泛用于图像 patch 序列压缩、音频谱图压缩，降低序列长度，缓解 Transformer 大序列计算缺陷。

## 3.3. 技术演进

- 早期 RNN 和 LSTM 存在梯度消失限制长序列处理，Transformer 引入自注意力机制大幅提升建模能力。

- Transformer 随规模增长表现优异，但因 $O(n^2)$ 复杂度限制长序列和多模态扩展。

- 近年来，出现多种线性复杂度建模方法：线性 Transformer 变种和基于 RNN 的 RWKV，兼顾效率和性能。

- 多模态大模型由单一文本向图像、音频等多模态融合演进，重点在模态编码器设计及上下游任务对齐。

- 本文即站在 RNN 高效推理与多模态交互的结合点，推动线性 RNN 在多模态大模型中的应用开拓。

## 3.4. 差异化分析

- 与传统基于 Transformer 的多模态模型相比，ModRWKV 创新地将 MRNN（RWKV7）作为主干网络，彻底避免高计算复杂度自注意力。

- 模态编码器独立插拔，符合同一 RWKV7 主干模型，强化共享参数基础，确保跨模态依赖学习。

- 采用极简适配器设计，减少模型额外参数，让主干模型承担跨模态推理责任。

- 通过系统性序列压缩实验，找出性能与效率最优的平衡点，提升实际部署能力。

- 实验验证预训练权重初始化（尤其“g1” 后训练版）对多模态信号理解能力提升尤为关键，强调多模态大模型预训练策略重要性。

# 4. 方法论

## 4.1. 方法原理

ModRWKV 设计目标是在保持线性时间复杂度推理优势的同时，实现对多模态信息的有效融合和理解。其核心思想是利用现代 RNN 架构 RWKV7 强大的序列建模能力，通过可更换的模态编码器将各类输入预处理成统一向量序列，再用轻量级适配器进行维度匹配，最终输入 RNN 主干进行跨模态捕获和推理。

这样一方面利用 RNN 天然的因果时序处理特性，另一方面通过多模态前端独立编码最大程度保留模态专属信息，形成模块化、灵活高效的多模态跨模态学习流水线。

## 4.2. 核心方法详解

### 4.2.1. RWKV7 架构状态更新机制

RWKV7 架构基于现代线性 RNN，并在传统递归公式基础上融入改进。经典 RNN 递归公式：

$h_t = W h_{t-1} + U x_t$

其中，$h_t$ 是隐藏状态，$x_t$ 是输入，$W$ 和 $U$ 是权重矩阵。

RWKV7 在此基础上引入了时间混合机制，用输入依赖的衰减和门控方式强化记忆状态更新。其状态更新公式：

$$
s_t = G_t s_{t-1} + a_t k_t v_t^T
$$

- $s_t \in \mathbb{R}^d$：模型当前状态向量。

- $G_t = (I - a_t k_t k_t^T) \mathrm{diag}(e^{-e^{w_t}})$：动态状态转移矩阵，通过输入控制通道特异的衰减。

- $a_t = W_a x_t$：上下文感知的向量学习率，控制新信息对状态的影响力度。

- $k_t$, $v_t$：分别是键和值，源于输入 $\Delta x_t$ 通过线性投影得到。

- $w_t = W_w x_t$：输入驱动的向量门控参数，决定状态的记忆衰减速度。

  此复杂设计核心是融入输入条件的通道式衰减和动态学习率，使模型长短期依赖捕获具备更强表达能力及适应性。

### 4.2.2. 多模态编码器设计

针对不同模态，ModRWKV 设计不同编码器：

- **视觉编码器:** 选择 CLIP 与 SigLIP2 两种视觉语言模型。两者输入原始图像，输出等长顺序特征向量序列（如 `577` token），以确保多模态输入兼容。

- **音频编码器:** 采 WavLM (base+、large) 和 Whisper (small、medium) 变体，输入音频采样率 `16,000` Hz，输出 `50` Hz 频率的特征序列。

- **时间序列编码器:** 采用 WaveNet 和 Timer，WaveNet 采用因果膨胀卷积擅长长依赖，Timer 基于预训练 Transformer，在此冻结权重。

  编码器输出均通过轻量级一层卷积压缩序列长度（详见 4.2.4），并通过适配器映射至 RWKV 输入空间。

### 4.2.3. 适配器结构

为了简化参数，适配器设计为单层多层感知机（MLP）：

$$
\boldsymbol{h} = \mathrm{Linear}_2(\mathrm{ReLU}(\mathrm{Linear}_1(\boldsymbol{x})))
$$

其中，$\boldsymbol{x}$ 是编码器生成的特征输入，经过两层线性映射和 ReLU 激活后，变换为与 RWKV 语言模型嵌入维度匹配的表示。

此设计使跨模态推理大部分负载落在 RWKV 主干，既考验其多模态建模能力，也保证适配器轻量。

### 4.2.4. 序列压缩机制

多模态编码器输出的序列长度较大，增加了计算成本。借助 1D 卷积实现高效压缩，显著降低后续模型处理负担。

具体公式为：

$$
\boldsymbol{y}_c = \sum_{i=1}^{C_{\mathrm{in}}} \left( \sum_{j=0}^{k-1} \boldsymbol{W}_{c, i, j} \cdot \boldsymbol{x}_{i, s \cdot t + j} \right) + b_c
$$

- $c = 1, \ldots, C_{\mathrm{out}}$ 表示输出通道。

- $\boldsymbol{x} \in \mathbb{R}^{C_{\mathrm{in}} \times L}$ 是输入序列，$L$ 为长度。

- $\boldsymbol{W} \in \mathbb{R}^{C_{\mathrm{out}} \times C_{\mathrm{in}} \times k}$ 是卷积核权重。

- $s$ 是步幅，$p$ 是填充，输出序列长度计算为：

  $$
L' = \left\lfloor \frac{L + 2p - k}{s} \right\rfloor +1
$$

调整卷积核大小 $k$ 和步幅 $s$ 可折中模型性能与计算效率。

### 4.2.5. 模态切换与统一训练

ModRWKV 采用“主干–适配器–编码器”分层设计，主干 RWKV7 固定，编码器根据模态切换，适配器轻量微调。训练分两阶段：

- 阶段一：冻结主干、编码器，仅训练适配器参数，实现模态特征向量空间对齐。

- 阶段二：解冻适配器与 RWKV，联合微调提升跨模态推理能力，编码器冻结以保持模态固有信息。

  该设计保证了训练效率与泛化能力。

# 5. 实验设置

## 5.1. 数据集

论文选用多模态及多领域数据以全面考察模型能力，涉及视觉、音频和时间序列。

### 视觉数据集

- **LLaVA-595K、LLaVA-665K**：包含丰富的图文对话数据，用于训练视觉语言理解。第一阶段训练用595K样本，第二阶段用665K样本。

- **多模态推理和理解基准：**

  - VQA-v2 (Goyal et al., 2017)：视觉问答，覆盖基础图像理解和问题回答。

  - TextVQA (Singh et al., 2019)：强调图像中的文本识别和理解。

  - GQA (Hudson and Manning, 2019)：现实世界图像的组合推理。

  - ScienceQA (Lu et al., 2022)：科学多模态推理。

  - POPE (Li et al., 2023)：检测视觉对象幻觉的二分类任务。

  - MMMU (Yue et al., 2024)：跨学科多模态复杂推理。

  - MMBench (Liu et al., 2024c)：多样化多模态综合能力评测框架。

### 音频数据集

- **LibriSpeech** (Panayotov et al., 2015): 960 小时英文阅读音频，用于语音识别。

- **Aishell-1** (Bu et al., 2017): 170 小时中文普通话音频，用于中语种语音识别测试。

### 时间序列数据集

- **GIFT-Eval** (Aksu et al., 2024): 综合时间序列预测基准。

- **UTSD** (Liu et al., 2024b): 多元时间序列数据集，用于零样本泛化。

- 额外增广训练数据集包含 GIFT-Eval 与部分 UTSD 数据。

## 5.2. 评估指标

### 视觉任务指标

- <strong>准确率 (Accuracy)</strong>：分类任务中正确预测的样本比例。

  $$
  \mathrm{Accuracy} = \frac{\text{正确预测样本数}}{\text{总样本数}}
  $$

  反映模型理解和推理能力。

### 语音识别指标

- <strong>词错误率 (Word Error Rate, WER)</strong>:

  - **概念定义**：衡量语音识别系统输出的文本与参考(真实)文本的差异，覆盖替换、插入和删除错误，值越低性能越好。

  - **数学公式**：

    $$
    \mathrm{WER} = \frac{S + D + I}{N} \times 100\%
    $$

  - **符号解释**：

    - $S$：替换错误数。

    - $D$：删除错误数。

    - $I$：插入错误数。

    - $N$：参考文本中的词总数。

- <strong>字符错误率 (Character Error Rate, CER)</strong>：基于字符粒度的错误率，定义同 WER。

### 时间序列预测指标

- <strong>均方误差 (Mean Squared Error, MSE)</strong>：

  - **概念定义**：度量预测序列与真实序列之间的平方误差平均，越小说明预测越准确。

  - **数学公式**：

    $$
    \mathrm{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
    $$

  - **符号解释**：

    - $n$：样本数量。

    - $y_i$：真实值。

    - $\hat{y}_i$：预测值。

## 5.3. 对比基线

论文选择多模态领域代表性最先进模型做对比，包括：

- **LLaVA 系列**：基于 Vicuna LLM 的多模态语言理解系统，代表业界主流 Transformer-based 多模态方法。

- **VL-Mamba**：基于 Mamba LLM，结合视觉序列建模的多模态方案。

- **MobileVLM**：关注轻量化设计的多模态大模型实例。

- 多个时间序列预测模型如 **TimeFM**、**Timer**、**UniTS**、**TTM**、**MOIRAI**、**ROSE**，评估 ModRWKV 时间序列任务性能。

  对比基线涵盖不同模型规模和多种模态，能有效证明 ModRWKV 的竞争力。

# 6. 实验结果与分析

## 6.1. 核心结果分析

### 视觉理解任务

下文中的表格引用了原文 Table 2 的结果。ModRWKV-3B 在多个常用多模态视觉任务（如 VQA-v2、GQA、ScienceQA-IMG、TextVQA、POPE、MMBench、MMMU）上表现出色，超越大小相近的 VL-Mamba、MobileVLM，接近甚至优于参数更大的 LLaVA-1.5-7B。

表现尤其突出的是 ScienceQA-IMG、POPE 及 MMMU，展现出良好的科学推理和长尾知识理解能力，并且在推理综合能力挑战 MMMU 中取得最高得分。

这一些结果验证了线性 RNN 架构在多模态认知场景下具备强大潜力，且模型设计实现了良好的性能与计算效率平衡。

### 语音识别任务

论文用 WER 和 CER 两组指标考察语音识别效果（详见原文 Table 3）：

- 在 LibriSpeech 数据集，使用 WavLM large 编码器的模型在 test_clean 达到 2.43% WER，test_other 达到 6.51% WER，属于高精度水平。

- 在 Aishell-1 汉语数据集，Whisper medium 编码器的模型在开发集和测试集上证实 CER 分别为 5.08% 和 5.83%，显示出非英语语音识别能力。

  这些在有限训练数据及无数据增强条件下取得的成绩，说明 MoDRWKV 多模态适配器在音频编码到语言模型映射上效果良好。

### 时间序列预测任务

表 4 对比了多种时间序列模型在 GIFT-Eval 等数据集上的性能：

- MoDRWKV 在多数据集表现稳定，尤其在 ETTm1 任务上以 0.227 MSE 超越其他模型，体现其在多样时间序列预测的泛化能力。

- 适配器放缩比例 4× 最佳（见表 5），超越 2× 和 8× 配置，说明隐藏层尺寸的调节对模型表现有明显影响。

- 使用更多 gift-eval 数据训练明显提升了预测性能，证明数据规模对模型泛化的促进作用。

  这些结果支持了 ModRWKV 作为多任务、多模态序列建模方案的有效性。

## 6.2. 数据呈现（表格）

以下是原文 Table 2 的结果：

<table>
<thead>
<tr><th>Method</th><th>LLM</th><th>PT</th><th>IT</th><th>VQAV2</th><th>GQA</th><th>SQAI</th><th>VQAT</th><th>POPE</th><th>MMB</th><th>MMMU</th></tr>
</thead>
<tbody>
<tr><td>LLaVA-1.5</td><td>Vicuna-7B</td><td>558K</td><td>665K</td><td>78.5</td><td>62.0</td><td>66.8</td><td>58.2</td><td>86.5</td><td>64.3</td><td>-</td></tr>
<tr><td>LLaVA-1.5</td><td>Vicuna-13B</td><td>558K</td><td>665K</td><td>80.0</td><td>63.3</td><td>71.6</td><td>61.3</td><td>86.2</td><td>67.7</td><td>-</td></tr>
<tr><td>LLaVA-1.6</td><td>Vicuna-7B</td><td>558K</td><td>665K</td><td>81.8</td><td>64.2</td><td>72.8</td><td>65.7</td><td>86.7</td><td>67.7</td><td>35.8</td></tr>
<tr><td>LLaVA-Phi</td><td>Phi-2-2.7B</td><td>558K</td><td>665K</td><td>71.4</td><td>-</td><td>68.4</td><td>48.6</td><td>85.0</td><td>59.8</td><td>-</td></tr>
<tr><td>MobileVLM-3B</td><td>MobileLLaMA-2.7B</td><td>558K</td><td>665K</td><td>-</td><td>59.0</td><td>61.2</td><td>47.5</td><td>84.9</td><td>59.6</td><td>-</td></tr>
<tr><td>VL-Mamba</td><td>Mamba LLM-2.8B</td><td>558K</td><td>665K</td><td>76.6</td><td>56.2</td><td>65.4</td><td>48.9</td><td>84.4</td><td>57.0</td><td></td></tr>
<tr><td>MoDRWKV</td><td>RWKV7 LLM-3B</td><td>558K</td><td>665K</td><td>78.3</td><td>60.8</td><td>70.9</td><td>51.1</td><td>87.1</td><td>66.6</td><td>38.7</td></tr>
</tbody>
</table>

以下是原文 Table 3 语音识别结果：

<table>
<thead>
<tr><th>Dataset</th><th>Data (h)</th><th>Encoder</th><th>Clean WER(%)</th><th>Other WER(%)</th><th>Dev CER(%)</th><th>Test CER(%)</th></tr>
</thead>
<tbody>
<tr><td rowspan="4">Librispeech</td><td rowspan="4">960</td><td>wavlm large</td><td>2.43</td><td>6.51</td><td>-</td><td>-</td></tr>
<tr><td>wavlm base+</td><td>3.08</td><td>10.38</td><td></td><td></td></tr>
<tr><td>whisper medium</td><td>5.33</td><td>12.28</td><td></td><td></td></tr>
<tr><td>whisper small</td><td>6.24</td><td>16.92</td><td></td><td>-</td></tr>
<tr><td rowspan="4">Aishell-1</td><td rowspan="4">178</td><td>wavlm large</td><td>-</td><td></td><td>9.68</td><td>10.33</td></tr>
<tr><td>wavlm base+</td><td></td><td></td><td>12.40</td><td>13.46</td></tr>
<tr><td>whisper medium</td><td></td><td></td><td>5.08</td><td>5.83</td></tr>
<tr><td>whisper small</td><td></td><td></td><td>6.29</td><td>6.95</td></tr>
</tbody>
</table>

## 6.3. 消融实验/参数分析

- **视觉编码器选择消融**（表 6）：

  SigLIP2 较 CLIP 在所有视觉基准上均表现更优，尤其体现在需要细粒度语义对齐的任务上，如文本视觉问答。

  这表明编码器设计及预训练策略在性能提升中作用远大于参数规模。

- **序列压缩卷积核与步幅调节消融**（表 7）：

  通过不同 (kernel, stride) 组合控制序列长度，发现压缩50%时性能下降微小，运行效率大幅提升。

  进一步加大压缩比例导致性能明显下降，表明存在性能与效率间的权衡点。

- **预训练权重差异影响分析**（表 8）：

  使用“g1”改进版权重初始化的模型在多项指标显著优于原始 base 初始化模型，尤其是在 ScienceQA 任务中提升达 28%。

  说明针对“思考”数据的预训练能极大加强多模态推理能力。

- **时间序列适配器尺度影响实验**（表 5）：

  适配器隐藏层尺度从 $2\times$ 提升至 $4\times$ 显著降低 MSE，进一步增加至 $8\times$ 不见提升，甚至受性能不稳影响。

  推荐设置为隐藏层维度为输入的四倍，平衡拟合能力与稳定性。

# 7. 总结与思考

## 7.1. 结论总结

本文提出 ModRWKV，多模态理解框架，实现可插拔编码器与统一 RNN 主干的灵活切换，首次突破了基于 RWKV7 结构的线性复杂度多模态大语言模型设计。系统性多模态评测和消融验证了 RNN 架构在跨模态语义融合中的有效性和计算优势。预训练权重对多模态任务的承载力至关重要，序列压缩技术提升了模型实际应用的推理效率。

因此，ModRWKV 不仅弥补了当前多模态领域基于线性模型研究的空白，也为未来多模态大模型发展提供了新的技术路径和设计范式。

## 7.2. 局限性与未来工作

- 目前仅支持二模态及单一编码器过程，尚未扩展到更复杂的多模态融合场景，如同时融合语音、视觉与文本的“三模态”任务。

- 模态间融合方式较为基础，未引入高级交叉注意力或多层解耦机制。

- 未来可探索多模态动态融合机制、更高效的序列压缩策略及大规模跨模态预训练策略；

- 计划扩展更多模态组合，提供端到端多模态统一训练方法。

## 7.3. 个人启发与批判

该论文展示了现代 RNN 架构在多模态大语言模型领域的潜力，突破 Transformer 二次复杂度瓶颈，提出轻量可插拔的编码器与适配器设计，兼顾性能与计算效率，对于实际部署有很强吸引力。

其预训练权重对多模态理解大幅影响的实验证据，启示多模态模型设计需重视预训练策略差异，而非单纯依赖架构放大。

潜在改进方向包括：

- 多模态融合更深层次机制的设计，以显著提升信息交互质量。

- 对长序列压缩的研究进一步深入，针对视频、语音长段序列提供更优方案。

- 适配器陷入收敛困难的状况值得关注，可尝试更稳定的初始化或优化策略提升训练稳定性。

- 评估更多复杂下游任务，验证模型在真实多模态场景的泛化和鲁棒性。

  总之，ModRWKV 提供了一条切实可行的多模态线性复杂度建模路线，值得后续研究广泛关注和发展。
