# ModRWKV: Transformer Multimodality in Linear Time

Jiale $\mathbf { K a n g ^ { 1 * } }$ , Ziyin $\mathbf { Y u e ^ { 1 * } }$ , Qingyu $\mathbf { Y i n ^ { 2 * } }$ , Jiang ${ \bf R u i } ^ { 1 , 3 }$ , Weile $\mathbf { L i } ^ { 1 }$ , Zening $\mathbf { L u } ^ { 1 }$ , Zhouran $\mathbf { J i } ^ { 1 }$ 1RWKvOS, 2Zhejiang University , 3The Hong Kong University of Science and Technology jiale@rwkvos.com

# Abstract

Currently, most multimodal studies are based on large language models (LLMs) with quadratic-complexity Transformer architectures. While linear models like RNNs enjoy low inference costs, their application has been largely limited to the text-only modality. This work explores the capabilities of modern RNN architectures in multimodal contexts. We propose ModRWKV—a decoupled multimodal framework built upon the RWKV7 architecture as its LLM backbone—which achieves multisource information fusion through dynamically adaptable heterogeneous modality encoders. We designed the multimodal modules in ModRWKV with an extremely lightweight architecture and, through extensive experiments, identified a configuration that achieves an optimal balance between performance and computational efficiency. ModRWKV leverages the pretrained weights of the RWKV7 LLM for initialization, which significantly accelerates multimodal training. Comparative experiments with different pretrained checkpoints further demonstrate that such initialization plays a crucial role in enhancing the model's ability to understand multimodal signals. Supported by extensive experiments, we conclude that modern RNN architectures present a viable alternative to Transformers in the domain of multimodal large language models (MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV architecture through systematic exploration. https://github.com/JL-er/ModRWKV.git

# 1 Introduction

Linear complexity model (Peng et al., 2025; Gu and Dao, 2024; Yang et al., 2024a, 2025, 2024b) have emerged as an efficient alternative to the attentionbased Transformer architecture (Vaswani et al., 2023; Yin et al., 2024) in Large Language Models (LLMs) (Touvron et al., 2023; Achiam et al., 2023). Among various linear models, recurrent neural networks (RNNs) (Peng et al., 2025) have become a competitive approach. Characterized by constant memory usage, RNNs can perform inference at a lower cost compared to the linearly increasing KV cache of Transformers. Recent research has also enabled their parallel training capabilities (Yang et al., 2024a, 2025), facilitated by hardware-aware designs optimized for modern GPU architectures (Dao et al., 2022). Currently, LLMs are undergoing a paradigm shift—from single-modality processing to crossmodal collaboration (Liu et al., 2023; Fang et al., 2025; Chen et al., 2022; Défossez et al., 2024). By leveraging transfer learning from pre-trained LLM weights, these models achieve cross-modal semantic alignment in tasks such as visual question answering and speech dialogue. However, this practice has primarily been employed within the traditional Transformer architecture. In the context of linear models, few works have expanded their understanding to modalities beyond natural language. This disparity highlights a crucial gap in the current landscape of linear models. In this paper, we describe MoDRWKV. It is the first RNN-based linear model that extends its capabilities to the cross-modal domain. MoDRWKV is based on RWKV7, a RNN-based architecture powered by generalized delta rule with vector values gating, in-context learning rates, and relaxed value replacement rule. We hypothesize that the inherent sequential processing capabilities of RNNs, coupled with a carefully designed shared parameter base, can effectively capture both intra-modal and inter-modal dependencies across diverse data types. We take advantage of the RWKV7 architecture to propose an innovative unified training paradigm for multimodal fusion. MoDRWKV adopts a lightweight shared parameter base with a modalityspecific encoder framework, where simply switching the front-end encoder enables seamless transfer across multimodal tasks. This approach systematically explores the representation capabilities of RNN architectures within cross-modal semantic spaces, aiming to break the Transformerdominated research paradigm. It offers new theoretical and practical insights into the deployment of large RNN-based models in the multimodal domain. Our contributions can be summarized as threefold: 1. Proposed the MoDRWKV framework, pioneering a unified multimodal training paradigm based on an RNN architecture. By adopting a plug-and-play design for modality encoders, it significantly enhances cross-modal scalability and integration efficiency. 2. Conducted a comprehensive and systematic evaluation of MoDRWKV's full-modality understanding capabilities, establishing a benchmark paradigm for assessing the cross-modal performance of RNN-based architectures. 3. Extensive Ablation experiments validate the optimal multimodal processing design that achieves a desirable balance between performance and computational efficiency.

# 2 Background

RWKV7: Modern RNN Architecture Simple linear RNNs (Qiao et al., 2024; Gu and Dao, 2024) can be written in the following recurrent form:

$$
h _ { t } = W h _ { t - 1 } + U x _ { t } ,
$$

which enables parallelized training but lacks strong language performance and long-term dependency preservation. RWKV combines the efficiency of linear RNNs (constant memory and time complexity during inference) with powerful modeling capabilities through its time-mixing block. It uses keys $k _ { t }$ and values ${ \mathbf { } } v _ { t }$ , linearly projected from $\mathbf { \Delta } \mathbf { x } _ { t }$ , and updates the state $\scriptstyle { s _ { t } }$ with input-dependent decay ${ \pmb w } _ { t }$ and receptance $\mathbf { \nabla } r _ { t }$ :

$$
\begin{array} { r } { s _ { t } = e ^ { - w _ { t } } \cdot s _ { t - 1 } + k _ { t } v _ { t } ^ { T } , } \end{array}
$$

In RWKV7, the state update is enhanced for greater expressiveness with the form:

$$
\begin{array} { r } { s _ { t } = G _ { t } s _ { t - 1 } + { a } _ { t } k _ { t } v _ { t } ^ { T } , } \end{array}
$$

where employed a generalized delta rule with two improvements: (1) In-context learning rate. the term $\mathbf { \delta } \mathbf { a } _ { t }$ , a vector-valued learning rate projected as $a _ { t } ~ = ~ W _ { a } x _ { t }$ , controls the influence of the new information $k _ { t } v _ { t } ^ { T }$ on the state. (2) Vector value gating. The dynamic transition matrix $G _ { t } ~ = ~ ( I - a _ { t } k _ { t } k _ { t } ^ { T } ) \mathrm { d i a g } ( e ^ { - e ^ { w _ { t } } } )$ incorporates ${ \pmb w } _ { t }$ , a vector-valued gating parameter from $\mathbf { \mathbf { \Psi } } \mathbf { \mathbf { w } } _ { t } \mathrm { ~ = ~ } W _ { w } \mathbf { \mathbf { \mathbf { x } } } _ { t }$ , enabling channel-specific decay rates. This input-dependent design makes $\mathbf { \delta } _ { s _ { t } }$ highly adaptive to context. Multimodal Large Language Models LLMs have traditionally been trained on natural language data and are primarily designed to understand and generate text. These models excel in text-based tasks but are inherently limited to the domain of human language. Recently, many works have begun to explore the potential of large language models beyond their linguistic roots, pushing their capabilities into other modalities. From a modality perspective, MLLMs now handle a variety of data types beyond text, including images (Liu et al. 2024a), audio (Défossez et al., 2024), and video. Structurally, these models adapt by incorporating modality-specific encoders, such as visual transformers for images or audio transformers for sound. Input integration varies between unified tokenization, where all modalities are converted into a single token sequence, and cross-modal attention, where the model attends to features across modalities.

# 3 Methodology

MoDRWKV is the first RNN-based multimodal architecture that integrates the MLLM training paradigm with a linear model, achieving exceptional hardware efficiency. In Section 3.1, we present the encoder selection design of MoDRWKV. In Section 3.2, we detail the adapter design of MoDRWKV. In Section 3.3, we describe the sequence compression method for efficiently processing diverse multimodal data.

# 3.1 Multimodal Encoder

Vision Encoder. We evaluated CLIP (Radford et al., 2021) and SigLIP2 (Tschannen et al., 2025) as alternative visual encoders for MoDRWKV, applying identical adaptation frameworks to each model independently. Each vision-language encoder processes raw images to generate sequential feature embeddings that are then aligned with the

![](images/1.jpg)  
FigureModRWKVnetworkarchitecture Multimodality data stresundergoinitl processin v a encode, a Convolutional layer, and an adapte. The 1 Convolutional layers employed to compress the sequencenth ouloaputs whiiantu uaiaveeurraigCnet ex data is transformed through a Text Embedding module. The outputs from the adapter and Text Embedding layers are subsequently concatenated.

RWKV large language model through lightweight adapter layers. Our experiments validated MoDRWKV's strong inherent capacity for visual information processing, with this framework demonstrating excellent cross-modal adaptability even without architectural modifications to the base language model. Audio Encoder. In our study, we employ WavLM (Chen et al., 2022) and Whisper (Radford et al., 2022) as audio encoders for MoDRWKV. We select encoder models with sizes ranging from approximately 100M to 400M parameters, specifically choosing WavLM base+, WavLM large, Whisper small, and Whisper medium for evaluation. These encoders process audio sampled at $1 6 { , } 0 0 0 \mathrm { H z }$ and generate feature vectors at a frequency of $5 0 \mathrm { H z }$ For the Whisper encoder, each audio segment is padded to a duration of 30 seconds. Time Series Encoder. We adopt WaveNet (Van Den Oord et al., 2016) and Timer (Liu et al., 2024b) as alternative temporal encoders for MoDRWKV. Timer is initialized with pre-trained weights, with the weights frozen during training, while WaveNet is trained from scratch without pre-trained weights. However, during inference, both encoders are frozen to enable zero-shot evaluation. Each encoder transforms raw time-series data into highlevel feature embeddings, which are then aligned with the RWKV blocks via lightweight adapters.

# 3.2 Adapter Design

We introduce a single-MLP adapter (Liu et al., 2023) for dimension alignment between modalities, reducing the adapter's parameter. This forces the RWKV7 backbone to handle the majority of cross-modal reasoning, providing a rigorous test of RNN-based architectures in multimodal settings:

$$
\begin{array} { r } { \pmb { h } = \mathrm { L i n e a r } _ { 2 } ( \mathrm { R e L U } ( \mathrm { L i n e a r } _ { 1 } ( \pmb { x } ) ) ) . } \end{array}
$$

Table 1: Multimodal Benchmark Evaluation   

<table><tr><td>Benchmark</td><td>Description</td></tr><tr><td>VQA-v2 (Goyal et al., 2017) TextVQA (Singh et al., 2019) GQA (Hudson and Manning, 2019)</td><td>Image Understanding Text-Image Integration Reasoning</td></tr><tr><td>ScienceQA (Lu et al., 2022) POPE (Li et al., 2023)</td><td>Scientific Reasoning Hallucination</td></tr><tr><td></td><td></td></tr><tr><td>MMMU (Yue et al., 2024)</td><td>Reasoning</td></tr><tr><td>MMBench (Liu et al., 2024c)</td><td>Assessment</td></tr><tr><td>LibriSpeech (Panayotov et al., 2015) Speech Recognition</td><td></td></tr><tr><td>Aishell-1 (Bu et al., 2017)</td><td></td></tr><tr><td></td><td>Speech Recognition</td></tr><tr><td>GIFT-Eval (Aksu et al., 2024)</td><td>Time Series</td></tr><tr><td>UTSD (Liu et al., 2024b)</td><td>Time Series</td></tr></table>

# 3.3 Sequence Compression

To address the computational challenges of long sequences in LLMs, we employ 1D convolution to effectively compress multimodal sequences (e.g., image patches, audio spectrograms). This approach significantly reduces processing overhead while maintaining model performance. For an input $\pmb { x } \in$ $\mathbb { R } ^ { C _ { \mathrm { i n } } \times L }$ , a convolutional kernel ${ \boldsymbol { W } } \in \mathbb { R } ^ { C _ { \mathrm { { o u t } } } \times C _ { \mathrm { { i n } } } \times k }$ , stride $s \geq 1$ , padding $p$ , the $c$ -th output channel $\boldsymbol { Y } \in \mathbb { R } ^ { C _ { \mathrm { o u t } } \times L ^ { \prime } }$ is computed as:

$$
\pmb { y } _ { c } = \underbrace { \sum _ { i = 1 } ^ { C _ { \mathrm { i n } } } \left( \sum _ { j = 0 } ^ { k - 1 } \pmb { W } _ { c , i , j } \cdot \pmb { x } _ { i , s \cdot t + j } \right) } _ { \mathrm { C o n v 1 D } } + b _ { c } ,
$$

where $t = 0 , \ldots , L ^ { \prime } - 1$ and $L ^ { \prime }$ is computed as $\begin{array} { r } { L ^ { \prime } = \left\lfloor \frac { L + 2 p - k } { s } \right\rfloor + 1 } \end{array}$ .

Table 2: Comparison with SoTA methods on 7 benchmarks. Benchmark names are abbreviated due to space limits. VQA-v2; GQA; SQAI: ScienceQA-IMG; VQAT: TextVQA; POPE; MMB: MMBench; MMMU. PT and IT indicate the number of samples in the pretraining and instruction tuning stages, respectively.   

<table><tr><td>Method</td><td>LLM</td><td>PT</td><td>IT</td><td>VQAV2</td><td>GQA</td><td>SQAI</td><td>VQAT</td><td>POPE</td><td>MMB</td><td>MMMU</td></tr><tr><td>LLaVA-1.5</td><td>Vicuna-7B</td><td>558K</td><td>665K</td><td>78.5</td><td>62.0</td><td>66.8</td><td>58.2</td><td>86.5</td><td>64.3</td><td>-</td></tr><tr><td>LLaVA-1.5</td><td>Vicuna-13B</td><td>558K</td><td>665K</td><td>80.0</td><td>63.3</td><td>71.6</td><td>61.3</td><td>86.2</td><td>67.7</td><td>-</td></tr><tr><td>LLaVA-1.6</td><td>Vicuna-7B</td><td>558K</td><td>665K</td><td>81.8</td><td>64.2</td><td>72.8</td><td>65.7</td><td>86.7</td><td>67.7</td><td>35.8</td></tr><tr><td>LLaVA-Phi</td><td>Phi-2-2.7B</td><td>558K</td><td>665K</td><td>71.4</td><td>-</td><td>68.4</td><td>48.6</td><td>85.0</td><td>59.8</td><td>-</td></tr><tr><td>MobileVLM-3B</td><td>MobileLLaMA-2.7B</td><td>558K</td><td>665K</td><td>-</td><td>59.0</td><td>61.2</td><td>47.5</td><td>84.9</td><td>59.6</td><td>-</td></tr><tr><td>VL-Mamba</td><td>Mamba LLM-2.8B</td><td>558K</td><td>665K</td><td>76.6</td><td>56.2</td><td>65.4</td><td>48.9</td><td>84.4</td><td>57.0</td><td></td></tr><tr><td>MoDRWKV</td><td>RWKV7 LLM-3B</td><td>558K</td><td>665K</td><td>78.3</td><td>60.8</td><td>70.9</td><td>51.1</td><td>87.1</td><td>66.6</td><td>38.7</td></tr></table>

Table 3: Model's $\mathrm { W E R } ( \% )$ on Librispeech dataset and $\mathrm { C E R } ( \% )$ on Aishell-1 dataset.   

<table><tr><td>Dataset</td><td>Data (h)</td><td>Encoder</td><td>Clean WER(%)</td><td>Other WER(%)</td><td>Dev CER(%)</td><td>Test CER(%)</td></tr><tr><td rowspan="4">Librispeech</td><td rowspan="4">960</td><td>wavlm large</td><td>2.43</td><td>6.51</td><td>-</td><td>-</td></tr><tr><td>wavlm base+</td><td>3.08</td><td>10.38</td><td></td><td></td></tr><tr><td>whisper medium</td><td>5.33</td><td>12.28</td><td></td><td></td></tr><tr><td>whisper small</td><td>6.24</td><td>16.92</td><td></td><td>-</td></tr><tr><td rowspan="4">Aishell-1</td><td rowspan="4">178</td><td>wavlm large</td><td>-</td><td></td><td>9.68</td><td>10.33</td></tr><tr><td>wavlm base+</td><td></td><td></td><td>12.40</td><td>13.46</td></tr><tr><td>whisper medium</td><td></td><td></td><td>5.08</td><td>5.83</td></tr><tr><td>whisper small</td><td></td><td></td><td>6.29</td><td>6.95</td></tr></table>

# 4 Experiments

# 4.1 Experimental Details

Training Settings (1) Vision. Our implementation follows the phased training paradigm of LLaVA (Liu et al., 2023) for both vision and audio understanding. In Phase I, we first freeze the encoder and the RWKV model, training only a linear adapter with a single MLP and layer norm to project multimodal features into the embedding space of the language model. In Phase II, we then unfreeze both the adapter and RWKV parameters, while the encoder remains frozen to preserve pretrained representations. To comprehensively assess the impact of encoder choice and model scale on RWKV7 performance, we performed experiments on four vision languagemarks using three model sizes (0.4B, 1.5B and 3B) for each encoder. Our models are trained on $8 \times$ NVIDIA A800 GPUs. Details of training settings can be found at Appendix 9. (2) Audio. Training was conducted in two phases: Phase I trained only the audio adapter $\mathrm { L R } { = } 1 \mathrm { e } { \cdot }$ 4), while Phase II jointly trained the adapter and RWKV (LR decayed from 1e-4 to 5e-5). For LibriSpeech, we ran 1 epoch in each phase; for Aishell1, 2 epochs in Phase I and 4 in Phase II. The default batch size was 32, reduced to 16 for the Whisper encoder due to GPU constraints, with epochs halved accordingly to match training steps. All experiments used $4 4 \times 0 9 0$ GPUs. (3) Time series. In the Time series task, We conducted experiments using dual NVIDIA RTX 4090 (24GB) GPUs, training on a 441,725-sample short-duration univariate dataset.

Datasets We consider diverse datasets in vision, audio, and time series (Refer to Table 1). For vision understanding ability, we use LLaVA-595K as training dataset for Phase I, and LLaVa-665k for Phase II. For audio, We train our MoDRWKVmodel using two open-source datasets: (1) LibriSpeech (Panayotov et al., 2015), which comprises 960 hours of English reading audio data; and (2) Aishell-1 (Bu et al., 2017), which includes 170 hours of Chinese audio data. For each, we trained our model exclusively on the respective training dataset. In the time series task, we utilized public datasets from GIFT-Eval (Aksu et al., 2024). After thorough sorting and cleaning, we derived a small number of univariate datasets. Additionally, we incorporated UTSD (Liu et al., 2024b) public datasets later in the process.

Benchmarks To rigorously evaluate our model's capabilities across diverse reasoning scenarios, we employed a comprehensive evaluation framework spanning from basic visual recognition to advanced knowledge-intensive tasks. This framework systematically verifies our model's cross-modal competence at various cognitive levels by assessing it on seven multimodal benchmarks: VQA-v2 (Goyal et al., 2017) for fundamental image understanding and question-answering, TextVQA (Singh et al., 2019) to evaluate optical character recognition (OCR) and text-image integration, GQA (Hudson and Manning, 2019) for compositional reasoning and real-world visual understanding, ScienceQA (Lu et al., 2022) to assess scientific multimodal reasoning through multiple-choice questions, POPE (Li et al., 2023) to quantify object hallucination via binary classification tasks, MMMU (Yue et al., 2024) to challenge models with college-level, cross-discipline problems, and MMBench (Liu et al., 2024c), which represents a systematically designed, objective evaluation framework for comprehensive assessment that uses circularEval strategy for assessment stability, ETT (Qiu et al., 2024), which focuses on long-term multivariate time-series forecasting using electricity transformer temperature data, serving as a standard benchmark for evaluating temporal modeling capabilities under various sequence lengths and prediction horizons, WeatherBench (Rasp et al., 2020) to evaluate spatiotemporal forecasting using global atmospheric data as a standard benchmark for data-driven weather prediction, etc.. Additionally, we evaluated our MoDRWKVmodel using the corresponding open-source datasets: LibriSpeech (Panayotov et al., 2015), which comprises 960 hours of English reading audio data, and Aishell-1 (Bu et al., 2017), which includes 170 hours of Chinese audio data.

Table 4: Zero-shot MSE with Adapter Scaling $4 \times$ use gift-eval datasets (WaveNet Encoder) (Qiu et al., 2024)   

<table><tr><td>Model</td><td>LB-FL</td><td>ECL</td><td>ETTh1</td><td>ETTh2</td><td>ETTm1</td><td>ETTm2</td><td>WTH</td><td>Traffic</td></tr><tr><td>TimeFM</td><td>720-96</td><td>0.119</td><td>0.421</td><td>0.326</td><td>0.363</td><td>0.206</td><td>0.123</td><td>0.327</td></tr><tr><td>Timer</td><td>720-96</td><td>0.221</td><td>0.414</td><td>0.305</td><td>0.440</td><td>0.203</td><td>0.178</td><td>0.526</td></tr><tr><td>UniTS</td><td>720-96</td><td>0.175</td><td>0.377</td><td>0.323</td><td>0.761</td><td>0.249</td><td>0.194</td><td>0.481</td></tr><tr><td>TTM</td><td>720-96</td><td>0.170</td><td>0.368</td><td>0.286</td><td>0.415</td><td>0.186</td><td>0.152</td><td>0.509</td></tr><tr><td>MOIRAI</td><td>720-96</td><td>0.212</td><td>0.394</td><td>0.285</td><td>0.516</td><td>0.222</td><td>0.208</td><td>1.359</td></tr><tr><td>ROSE</td><td>720-96</td><td>0.209</td><td>0.382</td><td>0.298</td><td>0.512</td><td>0.224</td><td>0.200</td><td>0.572</td></tr><tr><td>MoDRWKV(25% gift-eval)</td><td>720-96</td><td>0.342</td><td>0.746</td><td>0.633</td><td>0.754</td><td>0.559</td><td>0.797</td><td>0.512</td></tr><tr><td>MoDRWKV(100% gift-eval)</td><td>720-96</td><td>0.342</td><td>0.648</td><td>0.453</td><td>0.227</td><td>0.426</td><td>0.203</td><td>0.342</td></tr></table>

# 4.2 Qualitative Evaluation

Vision Understanding As summarized in Table 2, MoDRWKV demonstrates strong overall performance across eight widely-used multimodal benchmarks, outperforming existing state-of-theart (SoTA) methods in its parameter range. Compared to VL-Mamba-2.8B, MoDRWKV-3B consistently achieves higher scores on all evaluated tasks, reflecting its superior capability in visual question answering, compositional reasoning, and image-conditioned instruction following. Notably, despite having a significantly smaller language backbone than LLaVA-1.5-7B, MoDRWKV achieves competitive or superior results on several benchmarks. It surpasses LLaVA-1.5-7B in ScienceQA-IMG, POPE, and MMBench, while maintaining comparable performance on VQAv2. Furthermore, MoDRWKV attains the highest reported score among peers on the MMMU benchmark, highlighting its generalization ability in challenging multi-modal understanding scenarios. These results collectively suggest that MoDRWKV offers a favorable trade-off between performance and model size. Its effectiveness stems not merely from scale, but from architectural efficiency and a well-designed multimodal integration strategy, positioning it as a competitive alternative to larger vision-language models. Vision Knowledge The following examples in Table 10 showcase the capabilities of the MoDRWKV QA chatbot. These examples illustrate how MoDRWKV effectively integrates visual information with general knowledge, while also performing basic logical reasoning to address common user queries. Audio Recognition Table 3 presents the Word Error Rate (WER) for the LibriSpeech test_clean and test_other test sets, as well as the Character Error Rate (CER) for the Aishell-1 development and test sets. For the LibriSpeech dataset, the model achieved a WER of $2 . 4 3 \%$ on the test_clean subset, indicating a high level of precision in recognizing clear speech. On the test_other subset, the model attained a WER of $6 . 5 1 \%$ , demonstrating reasonable performance in handling more challenging noisy speech samples without data augmentation. For the Aishell-1 dataset, the model achieved CERs of $5 . 0 8 \%$ on the development set and $5 . 8 3 \%$ on the test set, using the Whisper medium encoder. These results reflect the model's effectiveness in handling non-English speech recognition tasks with limited training data.

<table><tr><td>Dataset Size</td><td>Adapter Scaling</td><td>ECL</td><td>ETTh1</td><td>ETTh2</td><td>ETTm1</td><td>ETTm2</td><td>WTH</td><td>Traffic</td></tr><tr><td>Gift-Evel</td><td>2×</td><td>0.641</td><td>0.785</td><td>0.882</td><td>0.949</td><td>0.719</td><td>0.633</td><td>0.988</td></tr><tr><td>Gift-Evel + UTSD</td><td>2×</td><td>0.516</td><td>0.637</td><td>0.848</td><td>0.891</td><td>0.672</td><td>0.512</td><td>0.683</td></tr><tr><td>Gift-Evel + UTSD</td><td>4×</td><td>0.453</td><td>0.629</td><td>0.547</td><td>0.843</td><td>0.648</td><td>0.461</td><td>0.641</td></tr><tr><td>Gift-Evel + UTSD</td><td>8×</td><td>0.535</td><td>0.629</td><td>0.652</td><td>0.828</td><td>0.762</td><td>0.566</td><td>0.617</td></tr></table>

Table 6: MoDRWKV Visual Models with different Encoders and parameters tested on benchmarks.   

<table><tr><td>Vision</td><td>Size</td><td>VQAV2</td><td>VQAT</td><td>GQA</td><td>SQAI</td></tr><tr><td rowspan="3">CLIP</td><td>0.4B</td><td>62.04</td><td>31.72</td><td>49.32</td><td>51.10</td></tr><tr><td>1.5B</td><td>72.31</td><td>40.27</td><td>54.56</td><td>62.77</td></tr><tr><td>3B</td><td>73.13</td><td>45.56</td><td>57.00</td><td>70.66</td></tr><tr><td rowspan="3">SigLIP2</td><td>0.4B</td><td>72.04</td><td>38.75</td><td>55.52</td><td>43.32</td></tr><tr><td>1.5B</td><td>76.95</td><td>44.96</td><td>58.88</td><td>63.10</td></tr><tr><td>3B</td><td>78.30</td><td>51.09</td><td>60.75</td><td>70.93</td></tr></table>

During adapter training, we observed a phenomenon akin to the capability emergence described by (Ma et al., 2024). However, the timing of this emergence was inconsistent and heavily influenced by the initialization of the adapter's weights. In some instances, the adapter failed to converge during Phase I.

Time Series Forecasting We conducted comparative experiments on two temporal encoder architectures: Timer and WaveNet. Results (See Table 4) show that although Timer has a larger parameter count (based on pre-trained weights), it consistently underperforms WaveNet on downstream time-series forecasting tasks. We hypothesize that this performance gap arises from WaveNet's use of causal dilated convolutions, which effectively capture long-range temporal dependencies through hierarchically expanding receptive fields. Additionally, unlike Timer's patch-wise embedding, WaveNet adopts a point-wise embedding strategy, allowing it to extract finer-grained temporal features. For training data preparation, we constructed two fine-tuning datasets: a baseline dataset (GIFTEval)(Aksu et al., 2024) and an augmented dataset composed of GIFT-Eval and a partially processed subset of UTSD(Liu et al., 2024b). Experiments indicate that models trained on the augmented dataset—containing anomalous samples—achieved superior generalization in zero-shot evaluations across public benchmarks including ECL, ETT, WTH, and Traffic. Notably, this training strategy enables the model to maintain stable predictions even under distribution shifts, demonstrating strong robustness and generalization. Architecture ablation studies further revealed that the scaling factor of the adapter modules plays a significant role in performance. A scaling factor of $4 \times$ yielded the best overall results on the validation set (see Table 5), outperforming the $8 \times$ and $2 \times$ settings by approximately $1 0 . 0 \%$ and $1 3 . 5 \%$ , respectively. Overall, even under constrained conditions—no data augmentation, limited training data, and fewer training steps—the MoDRWKV model achieved competitive accuracy on time-series forecasting tasks, providing empirical evidence for its applicability in real-world, complex scenarios.

# 4.3 Ablation Study

The Effect of Different Vision Encoders In order to evaluate the impact of different vision encoders on the performance of multimodal models, this study designed rigorous comparative experiments. We selected two representative visual encoder architectures for comparison: the contrastive learning-based CLIP and the recently proposed SigLIP2. In the experimental design, we specifically controlled the following variables: the length of the encoded visual feature sequences for both (google/siglip2-base-patch16-384 and openai/clipvit-large-patch14-336) was set to 577, to eliminate any potential confusion caused by differences in sequence length that could affect the understanding ability of LLMs; cross-validation was conducted on LLMs of different scales (ranging from 0.4B to 3B parameters) to ensure the generalizability of the experimental conclusions.

Table 7: By controlling the kernel and stride of conv1d, control the sequence length of multimodal signals to compare performance differences.   

<table><tr><td>Size</td><td>(k,s)</td><td>Token</td><td>VQAV2</td><td>VQAT</td><td>GQA</td><td>SQA1</td></tr><tr><td rowspan="4">1.5B</td><td>(0,0)</td><td>577</td><td>76.95</td><td>44.96</td><td>58.88</td><td>63.10</td></tr><tr><td>(3,2)</td><td>288</td><td>75.21</td><td>45.75</td><td>58.28</td><td>66.02</td></tr><tr><td>(4,3)</td><td>192</td><td>74.17</td><td>44.27</td><td>57.53</td><td>65.72</td></tr><tr><td>(5,4)</td><td>144</td><td>73.21</td><td>42.65</td><td>57.07</td><td>65.29</td></tr></table>

As shown in Table 6, SigLIP2 encoder consistently outperforms CLIP encoder all evaluated benchmarks, including VQAv2, TextVQA, GQA, and ScienceQA. Notably, the SigLIP2-based model achieves significant improvements in both general and text-based visual question answering tasks, as well as in compositional reasoning. Despite its encoder containing only 90M parameters—approximately $30 \%$ of the CLIP encoder's size—SigLIP2 demonstrates superior performance, particularly in tasks requiring fine-grained visualtext alignment and semantic understanding. These results underscore that model effectiveness in multimodal understanding is influenced more by encoder design and pretraining methodology than by parameter scale alone. Efficiency of Sequence Compression via 1D Convolution It is well known that the efficiency problem in processing long sequences has long been one of the main bottlenecks limiting the performance of LLMs. This challenge is particularly prominent in multimodal tasks, where signals from different modalities often generate a large number of tokens after encoding. For example, in the MoDRWKVmodel, a single image encoded through the SigLIP2 encoder generates 577 tokens, and when extended to video sequences, the length increases by an order of magnitude. To address this issue, this section systematically investigates the optimization effects of convolutional dimensionality reduction (Conv1D), aiming to provide new technical insights for sequence compression research.

Table 8: Performance differences under different pretraining weights   

<table><tr><td>Size</td><td>Model</td><td>VQAV2</td><td>VQAT</td><td>GQA</td><td>SQA1</td></tr><tr><td rowspan="2">0.4B</td><td>base</td><td>72.04</td><td>38.75</td><td>55.52</td><td>43.32</td></tr><tr><td>g1</td><td>73.21</td><td>41.13</td><td>57.34</td><td>55.58</td></tr><tr><td rowspan="2">1.5B</td><td>base</td><td>76.95</td><td>44.96</td><td>58.88</td><td>63.10</td></tr><tr><td>g1</td><td>77.87</td><td>50.91</td><td>60.18</td><td>64.63</td></tr></table>

on the ScienceQA task. Further research reveals that as the kernel size and stride increase, although the model performance exhibits a gradual decline, the computational efficiency is significantly improved. We tested MoDRWKV-1.5B on single 4090 GPU without any acceleration; The results indicate that increasing the compression ratio of token sequences can substantially accelerate inference speed, showing a clear efficiency gain. This highlights an effective strategy for balancing computational efficiency and model performance, offering valuable insights for practical deployment. G1 reasoning model We experimentally validate the effect of text pretraining weights on the ability of large language models to understand multimodal information by comparing two pretraining weights (base and g1) of the RWKV7-0.4B model. It is important to note that the g1 model is an improved version of the base model, obtained through posttraining by introducing a large amount of 'think'- type data. Although both models perform similarly in pure text NLP benchmark tests, as shown in Table 8, fine-tuning with the g1 pretraining weights significantly outperforms the base model across all metrics, with an exceptionally significant improvement observed in the SQA metric (specific improvement is $28 \%$ ). This empirical result strongly confirms that an appropriate text pretraining strategy can effectively enhance the language model's ability to understand multimodal information, thereby improving its overall performance in downstream tasks. We conducted empirical research (See Table 7 and visualization in Figure 2) on the ModRWKV1.5B model architecture using the LLaVA training dataset, and performed comprehensive evaluations across multiple benchmark datasets, including VQAv2, TextVQA, GQA, and ScienceQA. The experimental results show that when the sequence length is compressed by $50 \%$ , the model exhibits only a slight decrease in performance (on average) while achieving a $4 . 6 \%$ accuracy improvement Time Series Forecasting encoder Compare with Timer and WaveNet In a feedforward neural network (FFN), activation functions such as ReLU introduce sparsity by setting some outputs to zero, which in turn reduces the rank of the output matrix and may impact the model's representational capacity. Through both theoretical analysis and empirical experiments using the results in Table 5, we observed that the effect is suboptimal when the hidden layer dimension is set to $2 \mathbf { x }$ or $8 \mathbf { { X } }$ the input dimension.

![](images/2.jpg)  

Figure 2: Performance and effciency of ModRWKV. Left. The scaling curve of tokens with the performance score. Right. The inference time of MoDRWKV with the number of tokens.

Table 5 presents the zero-shot mean squared error (MSE) performance of different adapter scaling configurations on multiple public datasets, including ECL, ETTh, ETTm, WTH, and Traffic. The results indicate that increasing the adapter scaling factor from $2 \times$ to $4 \times$ significantly improves performance across most datasets, with the lowest MSE values observed at $4 \times$ scaling. Specifically, the Gift-Evel $^ +$ UTSD model with $4 \times$ scaling achieves the best results on ECL (0.453), ETTh1 (0.629), ETTh2 (0.547), ETTm2 (0.648), WTH (0.461), and Traffic (0.641), demonstrating that this configuration effectively enhances model accuracy. However, further increasing the scaling factor to $8 \times$ does not consistently improve performance, with some datasets showing increased error values. This suggests that excessively large hidden layer dimensions may introduce instability or diminish representational efficiency. Based on these findings, we recommend setting the hidden layer dimension to at least four times the input dimension to preserve sufficient rank, thereby enhancing the model's representational power and stability.

$$
p = 1 - \frac { \sum _ { i = m } ^ { n } \binom { n } { i } } { 2 ^ { n } }
$$

Table 4 presents the zero-shot mean squared error (MSE) results for various models using the WaveNet encoder with adapter scaling $4 \times$ on public datasets, leveraging the gift-eval dataset. The models are evaluated on multiple time-series forecasting benchmarks, including ECL, ETTh, ETTm, WTH, and Traffic, with a lookback length of 720 and a forecast length of 96. From the results, TimeFM achieves the best performance on ECL (0.119), WTH (0.123), and Traffic (0.327), demonstrating strong predictive capabilities on these datasets. TTM performs best on ETTh1 (0.368) and ETTm2 (0.186), while MOIRAI achieves the lowest error on ETTh2 (0.285). Our proposed model, MoDRWKV ( $100 \%$ gift-eval), outperforms other models on ETTm1 (0.227), showing its effectiveness in short-term forecasting for this dataset. Comparing MoDRWKV $2 5 \%$ gift-eval) and MODRWKV $100 \%$ gift-eval), we observe that increasing the proportion of gift-eval data significantly improves performance across most datasets, particularly on ETTh2 $\mathrm { f r o m } 0 . 6 3 3$ to 0.453) and ETTm1 (from 0.754 to 0.227). This suggests that leveraging a larger portion of the gift-eval dataset enhances ours model generalization and stability. Overall, the results highlight the varying strengths of different models across datasets, emphasizing the importance of dataset composition and model architecture in achieving optimal forecasting performance.

# 5 Conclusion

In this paper, we propose MoDRWKV, a multimodal understanding framework that enables modality switching via interchangeable encoders. Built upon RWKV7, MoDRWKV provides a comprehensive analysis and evaluation of the capabilities of modern RNN architectures in the multimodal domain.

# 6 Limitations

This paper presents a systematic evaluation of the proposed MoDRWKV framework across a range of benchmark tasks involving different modalities, demonstrating the feasibility of applying linearstructured models to multi-modal large language models (MLLMs). Nonetheless, this work does not yet explore more complex multi-modal fusion scenarios, such as tri-modal tasks involving speech, vision, and language. Future work will aim to address these richer multi-modal settings.

# References

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Taha Aksu, Gerald Woo, Juncheng Liu, Xu Liu, Chenghao Liu, Silvio Savarese, Caiming Xiong, and Doyen Sahoo. 2024. Gift-eval: A benchmark for general time series forecasting model evaluation. arxiv preprint arxiv:2410.10393. Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. 2017. Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. 2024. Moshi: a speechtext foundation model for real-time dialogue. Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. 2025. Llama-omni: Seamless speech interaction with large language models. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. Albert Gu and Tri Dao. 2024. Mamba: Linear-time sequence modeling with selective state spaces. Drew A. Hudson and Christopher D. Manning. 2019. Gqa: A new dataset for real-world visual reasoning and compositional question answering. pages 6693 6702. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji rong Wen. 2023. Evaluating object hallucination in large vision-language models. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024a. Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, and Mingsheng Long. 2024b. Timer: Generative pre-trained transformers are large time series models. arXiv preprint arXiv:2402.02368. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. 2024c. Mmbench: Is your multi-modal model an all-around player? Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and A. Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. ArXiv, abs/2209.09513. Ziyang Ma, Guanrou Yang, Yifan Yang, Zhifu Gao, Jiaming Wang, Zhihao Du, Fan Yu, Qian Chen, Siqi Zheng, Shiliang Zhang, and Xie Chen. 2024. An embarrassingly simple approach for llm with strong asr capacity. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 52065210. Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Xingjian Du, Haowen Hou, Jiaju Lin, Jiaxing Liu, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, Nathan Wilce, Johan S. Wind, Tianyi Wu, Daniel Wuttke, and Christian ZhouZheng. 2025. Rwkv-7 "goose" with expressive dynamic state evolution. Yanyuan Qiao, Zheng Yu, Longteng Guo, Sihan Chen, Zijia Zhao, Mingzhen Sun, Qi Wu, and Jing Liu. 2024. Vl-mamba: Exploring state space models for multimodal learning. Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang Zhang, Chenjuan Guo, Aoying Zhou, Christian S Jensen, Zhenli Sheng, et al. 2024. Tfb: Towards comprehensive and fair benchmarking of time series forecasting methods. arXiv preprint arXiv:2403.20150. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. Robust speech recognition via large-scale weak supervision. Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2024. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Stephan Rasp, Peter D Dueben, Sebastian Scher, Jonathan A Weyn, Soukayna Mouatadid, and Nils Thuerey. 2020. Weatherbench: a benchmark data set for data-driven weather forecasting. Journal of Advances in Modeling Earth Systems, 12(11):e2020MS002203. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. Towards vqa models that can read. pages 83098318. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. 2025. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, et al. 2016. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 12. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention is all you need. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. 2024a. Gated linear attention transformers with hardware-efficient training. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. 2024b. Parallelizing linear transformers with the delta rule over sequence length. arXiv preprint arXiv:2406.06484. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. 2025. Parallelizing linear transformers with the delta rule over sequence length. Qingyu Yin, Xuzheng He, Xiang Zhuang, Yu Zhao, Jianhua Yao, Xiaoyu Shen, and Qiang Zhang. 2024. Stablemask: Refining causal masking in decoderonly transformer. arXiv preprint arXiv:2402.04779. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan

# A Example Appendix

Table 9: Hyperparameter Settings for Visual Instruction Tuning of MoDRWKV   

<table><tr><td>Hyperparameter</td><td>Step1</td><td>Step2</td></tr><tr><td>bsz</td><td>256</td><td>128</td></tr><tr><td>ctx_len</td><td></td><td>2048</td></tr><tr><td>br</td><td>1e-3</td><td>2e-5</td></tr><tr><td>warmup_steps</td><td></td><td>100</td></tr><tr><td>beta1</td><td></td><td>0.9</td></tr><tr><td>beta2</td><td></td><td>0.99</td></tr><tr><td>adam_eps</td><td></td><td>1e-8</td></tr><tr><td>precision</td><td></td><td>bf16</td></tr><tr><td>lr_schedule</td><td></td><td>wsd</td></tr></table>

# A.1 Examples

# Visual Examples

![](images/3.jpg)

# User MODRWKV

How many Cars in this picture? There are two cars in the picture, with one being the taxi cab the man is standing on. [Start a new conversation, and clear the history]

![](images/4.jpg)

User   
MODRWKV User   
MODRWKV What is the name of this picture? The name of this picture is "Starry Night " Who drew it? Vincent van Gogh drew this picture.

![](images/5.jpg)

User Is this real in current life?   
MoDRWKV No, this is not a real animal. It is a computer-generated image (CGI) of a dinosaur, specifically a T-Rex, walking through a forest.   
User Depict the environment   
MODRWKV The environment in the image is a forest with trees, rocks, and a dirt path. The CGI dinosaur is walking through this natural setting.   
User Based on the facial expression, what is it's emotion?   
MODRWKV The dinosaur appears to be angry, as it is depicted with its mouth open and teeth showing.   
User Based on its emotion and the environment, what do you think this dinosaur is doing right now?   
MODRWKV Based on its angry expression and the forest setting, it is possible that the dinosaur is either searching for food, defending its territory, or reacting to a perceived threat. The fact that it is walking through the forest suggests that it might be exploring its habitat or looking for prey. However, without more context or information, it is difficult to determine the exact reason for its current behavior.

Table 11: Interactive Audio Examples Demonstrating Question-Answer Capabilities   

<table><tr><td colspan="2">Audio Examples</td></tr><tr><td>User MoDRWKV</td><td>What is your name? Hello! My name is RWKV, and I&#x27;m your friendly voice assistant here to help with any questions or tasks you have.</td></tr><tr><td>User MoDRWKV</td><td>4p Can I drink coffee before I go to bed? Yes, you can drink coffee before you go to bed, but it&#x27;s important to do so in moderation. Drinking coffee too close to bedtime can interfere with your sleep quality. Try to have your last cup of coffee at least 6 hours before bedtime. If you&#x27;re sensitive to caffeine, you might want to avoid it altogether. Also, consider other relaxing activities before bed to help you wind down</td></tr><tr><td>User MODRWKV</td><td>4p Tell me something about machine learning. Machine learning is a branch of artificial intelligence that focuses on creating systems that can learn from and make decisions based on data. It involves algorithms that enable computers to identify patterns and make predictions or decisions without being explicitly programmed for each task. This technology is widely used in various applications, such as recommendation systems, speech recognition, and autonomous vehicles. Machine learning models can improve over time as more data is processed, making them more accurate and efficient. It&#x27;s a powerful tool for automating tasks, enhancing user experiences, and driving innovation across many industries.</td></tr></table>