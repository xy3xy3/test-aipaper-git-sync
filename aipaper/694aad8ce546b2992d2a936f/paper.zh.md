# DrVoice：通过双分辨率语音表示实现的并行语音-文本语音对话模型

陈超宏，陈倩，王文，邓崇，张青林， 程璐瑶，余海，张鑫，吕翔，赵天宇，张崇， 马玉坤，陈亚峰，王辉，刘嘉琦，李向刚，叶捷平 同一实验室，阿里巴巴集团 {tanchaohong.ch, tanqing.cq, w.wang}@alibaba-inc.com

# 摘要

近期关于基于大语言模型 (LLMs) 的端到端 (E2E) 语音生成的研究引起了广泛的关注，多个研究扩展了基于文本的 LLM，生成离散的语音词元。现有的 E2E 方法主要分为两类：(1) 独立生成离散语音词元的方法，这些方法未将其纳入 LLM 的自回归过程，导致文本生成无法意识到同时进行的语音合成。(2) 通过联合自回归建模生成交错或并行的语音文本词元的模型，使得生成过程中各模态相互意识。本文提出了 DrVoice，这是一种基于联合自回归建模的并行语音文本对话模型，具有双重分辨率的语音表示。值得注意的是，当前方法主要使用 $1 2 . 5 \mathrm { H z }$ 的输入音频表示，而我们提出的双重分辨率机制将输入频率降低至 $\mathbf { 5 H z }$，显著降低计算成本，并缓解语音与文本词元之间的频率差异，从而更好地发挥 LLM 的能力。实验结果表明，DRVoICE-7B 在 OpenAudioBench 和 Big Bench Audio 基准上建立了新的最先进 (SOTA) 性能，同时在 VoiceBench 和 UltraEval-Audio 基准上的表现也达到了与 SOTA 相当的水平，使其成为约 ${ \sim } 7 \mathrm { B }$ 模型中的领先开源语音基础模型。

# 1 引言

口语对话系统的发展对人机交互至关重要，因为自然的人类沟通本质上依赖于语言交流。最近，基于大型语言模型（LLM）的口语对话系统，如 GPT-4o（OpenAI, 2024b），展现了与用户进行无缝、自然互动的巨大潜力。基于 LLM 的口语对话系统大致可以分为级联系统和端到端（E2E）系统，区别在于主干 LLM 是否能够直接理解语音表示并生成语音输出。早期的级联系统将单独训练的自动语音识别（ASR）、LLM 和文本转语音（TTS）模块整合在一起。这类系统固有地面临误差累积、音频细节（如情感）丢失以及显著的延迟等问题。另一方面，可以通过引入音频理解基础模型来消除 ASR 模块（Chu 等，2023，2024）。E2E 系统的出现旨在进一步消除 ASR 和 TTS 模块，并在语音表示与 LLM 之间建立直接连接。然而，使 LLM 生成高度智能的语音输出仍然具有挑战性。E2E 系统在语音词元生成的质量上面临困难，主要是由于在语音词元生成过程中未能有效利用文本信息。

最近针对这些挑战的端到端模型研究主要集中在两个方向（Chen et al., 2024a）：文本驱动的语音模型（Yao et al., 2024；Li et al., 2025）和联合语音-文本模型。文本驱动的语音模型是指系统中，大语言模型（LLM）处理语音表示作为输入，产生文本响应，并利用LLM的表示作为输入生成语音的解码器进行语音生成。相比之下，联合语音-文本模型则是让LLM将语音表示作为输入，同时生成文本词元和语音词元。关键的区别在于语音词元的生成是否能够影响LLM后续文本词元的生成：在联合模型中存在语音词元反馈回LLM的循环，而在文本驱动模型中则不存在。联合语音-文本模型可以进一步分为交错的语音-文本建模（Zeng et al., 2025；Zhang et al., 2024a）和并行的语音-文本建模（Défossez et al., 2024；Chen et al., 2024a；KimiTeam et al., 2025）。交错的语音-文本建模将语音和文本表示交替作为LLM的输入，而并行的语音-文本建模则在将语音和文本表示输入LLM之前融合这两种表示。

尽管文本驱动语音模型和联合语音-文本模型明确将文本指导纳入语音词元生成，以利用大型语言模型（LLM）的能力，但它们存在明显的局限性。文本驱动语音模型的架构创建了单向的信息流，其中文本生成在语音合成开始之前完成。这使得LLM无法基于生成的语音词元调整其文本输出，从而限制了其探索精细的副语言属性（如情感和韵律）的能力。另一方面，联合语音-文本模型由于语音词元的干扰而降低了原始的文本生成能力，使文本能力的保留成为一个关键挑战。尽管如此，联合语音-文本模型强制执行多模态交互和生成，并赋予更大的潜力（Chen et al., 2024a）；因此，在本研究中，我们重点提升联合语音-文本建模。最近，Kimi-Audio（KimiTeam et al., 2025）在联合语音-文本建模领域设定了新的最先进水平（SOTA）。然而，该方法仍然存在显著的局限性。它不仅需要大量的训练数据，还因为其$1 2 . 5 \mathrm { H z }$的音频表示而产生显著的计算成本。此外，高词元率与文本词元（$\sim 3 \mathrm { { H z } }$）的低率之间存在频率不匹配，这可能稀释语义信息，进而妨碍LLM核心能力的充分发挥。在本工作中，我们提出了DrVoiCE，一种具有双分辨率语音表示（DRSR）的新颖并行语音-文本对话模型。为了实现语音理解，我们引入了一种分组机制，将$2 5 \mathrm { H z }$的离散音频词元映射到$5 \mathrm { H z }$的语音表示，有效缓解语音和文本词元之间的时间分辨率差异。在生成过程中，组合的语音-文本嵌入形成助理的自回归输入。从共享的LLM层捕获的隐藏状态随后并行传递给文本头以进行文本词元预测，并传递给语音精化头（SRH）以生成相应的未分组语音词元。为了进一步增强模型的推理能力和输出的一致性，我们结合了一种模态链（CoM）策略（Zhang et al., 2023a）。CoM促使模型首先生成完整的文本响应，使其能够在进行并行语音-文本生成之前理顺思路。这一中间推理步骤改善了模态对齐，减少了错误。我们设计系统提示以控制输出模式，支持文本输出、直接并行语音-文本输出或增强的并行输出。我们还制定了一种核心-鸡尾酒训练策略，以优化模型并保留LLM知识。

我们的贡献有三个方面：1）我们提出了DrVoICE，一种新颖的并行语音-文本对话模型，具有双重分辨率语音表示（DRSR）。这一核心架构创新有效缓解了语音和文本词元之间的时间分辨率不匹配，降低了计算成本，并更好地保留了大语言模型的语义处理能力。2）我们引入了两种新的训练策略，包括一种作为课程的CoM-Mixing训练策略，利用结构化的CoM推理来支撑语音生成，以及一种用于保留大语言模型知识的Core-Cocktail训练策略。3）我们全面的实验结果揭示，DrVoICE-7B在OpenAudioBench（音频理解）和Big Bench Audio（音频处理模型的推理和理解能力）等重要基准上达到了新的最先进性能，同时在VoiceBench（基准评估基于LLM的语音助手）和UltraEval-Audio（语音理解与生成）等其他基准上的表现也相当于最先进水平，巩固了其作为约${ \sim } 7 \mathrm { B }$模型中的顶尖开源语音基础模型的地位。

# 2 相关工作

语音分词。将连续语音信号转换为可处理序列的主要方向有两个：连续表征，例如 Whisper（Radford 等，2022），和离散表征。由于大型语言模型（LLMs）生成离散词元，连续表征在与 LLMs 进行语音生成时面临直接集成的挑战。相比之下，离散词元使 LLMs 能够像处理文本词元一样处理语音，通常可分为两类：1）声学词元，旨在通过神经音频编解码器重建高质量音频（Défossez 等，2023），2）语义词元，通常源自自监督预训练模型，采用掩码语言建模作为训练目标（Hassid 等，2023）或使用监督学习的 S3Tokenizer，在 CosyVoice 中应用（Du 等，2024a,b, 2025），并优先考虑语言内容（Hsu 等，2021）。虽然声学词元在声音重建中实现了更高的声学保真度，但语义词元与语义表征的对齐更强（Zhang 等，2023b），从而促进了 MLLMs 在语音理解和生成方面能力的更有效扩展（Zeng 等，2025；Zhang 等，2024b, 2023a；Borsos 等，2023）。在本研究中，我们使用 S3Tokenizer 作为语音分词器，因为它具有稳健的语义能力，并且与 CosyVoice 中强大的语音去词化器兼容，以进行合成。S3Tokenizer 是基于预训练的 SenseVoice-Large 模型（An 等，2024）的监督语义语音分词器，增强了提取词元的语义关系。S3Tokenizer 对数据噪声具有鲁棒性，降低了对干净数据的依赖。端到端语音基础模型。文本驱动的语音模型（Fang 等，2024；Wang 等，2024；Fu 等，2025；Ya0 等，2024；Huang 等，2025；Chen 等，2025）集成了语音编码器、适配器、LLM 和流式语音解码器，能够同时生成文本和语音。Qwen2.5-Omni（Xu 等，2025）采用 Thinker-Talker 架构，其中 Thinker 负责多模态理解和文本生成，而 Talker 负责语音词元生成。由于 Thinker 在生成期间无法接收语音词元，该框架本质上限制了对语音词元生成状态的意识，从而限制了全双工语音对话等应用。

联合语音-文本模型探索了两种不同的架构，交错模型和并行模型。交错模型（Zhang et al., 2024a; Zeng et al., 2025; Li et al., 2025; Long et al., 2025; Wu et al., 2025）采用交错解码以支持语音和文本词元的同时生成，而并行模型（Défossez et al., 2024; Xie and Wu, 2024a,b; Chen et al., 2024a; KimiTeam et al., 2025）则进行并行解码。并行模型Moshi（Défossez et al., 2024）采用紧凑型深度Transformer来预测$k$个词元，同时仅依赖于当前步骤的表示。为了减轻离散词元在音频理解中的局限性，Kimi-Audio（KimiTeam et al., 2025）引入了双词元器，将离散语义词元与连续的Whisper（Radford et al., 2023）特征相结合，保留语义和声学信息。为了进一步提高训练和推理的效率，并缓解文本与语音模态之间粒度不对齐的问题，我们的方法利用DRSR将LLM输入帧率降低到$5 \mathrm { H z }$，而不牺牲性能。

# 3 方法论

图1展示了DrVoICE的架构。该系统由三个主要组件组成：（1）语音编码器和语音词元化器分别将语音波形处理成用户端和助手端的隐藏表示；（2）多模态大语言模型（MLLM）包含共享的LLM层、文本头和语音精炼头（SRH）用于生成词元；（3）语音反词元化器用于从语音词元生成波形。该系统通过多模态输入编码和协调的语音-文本输出生成进行操作。在推理过程中，用户输入（文本或语音）首先映射到统一的语义空间，通过MLLM处理以通过SRH和文本头生成并行的语音-文本响应。为了有效训练该系统，我们引入了CoM-Mixing训练和Core-Cocktail训练策略。

# 3.1 语音分词与重组

为了增强音频理解，我们采用 Whisper-Large-v3 语音编码器在用户端提取连续音频表示。随后，引入适配器对这些表示的时间分辨率进行下采样，并将其隐含维度与大语言模型的隐含维度对齐。由于语义词元与文本的强对齐，语义词元被广泛用于语音分词（Zhang et al., 2023a；Borsos et al., 2023）；因此，我们使用 S3Tokenizer（Du et al., 2024a,b, 2025）作为语音分词器，将语音波形转换为语义语音词元序列 $\mathbf { S } = [ s _ { 0 } , s _ { 1 } , \cdots , s _ { T - 1 } ]$，其中 $T$ 是语音词元序列的长度。对于语音去分词，基于捕获声学细节（如音色）的说话人嵌入，Flow Matching 模型（Lipman et al., 2023）将语音词元 S 转换为给定说话人的 Mel 频谱。最后，预训练的声码器 HiFi-GAN（Kong et al., 2020）将 Mel 频谱转换回音频信号。

![](images/1.jpg)  

Figure 1: Overview of DrVoICE. User speech inputs are tokenized, grouped, and encoded by the MLLM for autoregressive text and speech token prediction. The MLLM consists of Shared LLM Layer, a Text Head, and a Speech Refined Head (SRH) for token generation. The generated speech tokens are then converted to speech waveform by the speech detokenizer. Note that SRH generates $k$ speech tokens through $k$ autoregressive forward passes, where $k$ is the grouping factor.

# 3.2 多模态大语言模型 (MLLM)

基于文本-大语言模型（text-LLMs），多模态语言模型（MLLM）学习联合语音-文本建模，以处理语音或文本输入，同时生成并行的语音和文本输出。并行联合语音-文本模型。受到Moshi（Défossez等，2024）的启发，显式文本流被纳入以辅助语音生成，作为一种共同的语义框架。我们专注于在助手端进行模态对齐。该设计遵循人机交互的非对称特征：用户输入通常是单一模态（文本或语音），而助手的响应可以是协调的多模态输出。利用大语言模型的自回归生成能力，生成的语音词元 $s _ { t }$ 和文本词元 $t _ { t }$ 在每个时间步被迭代反馈到共享的大语言模型层中。它们的嵌入被添加为模型输入，形成并行语音-文本架构。形式上，时间步 $t$ 的组合输入嵌入 $c _ { t }$ 计算如下：

$$
c _ { t } = E _ { \mathrm { s p e e c h } } ( s _ { t } ) + E _ { \mathrm { t e x t } } ( t _ { t } )
$$

其中 $E _ { \mathrm { s p e e c h } }$ 和 $E _ { \mathrm { t e x t } }$ 表示语音和文本词元的嵌入。由于语音词元和文本词元的长度通常不匹配，较短的序列会用特殊标记 <ISILl> 进行填充，以便为每个发声对齐它们。自回归生成过程如下：

$$
P ( y _ { t } | y _ { < t } , x ) = \prod _ { i = 1 } ^ { t } P ( y _ { i } | y _ { < i } , x )
$$

其中 $x$ 是输入序列，$y _ { t } = ( s _ { t } , t _ { t } )$ 表示在时间步 $t$ 的联合语音-文本输出。这种统一的公式化使得语音和文本生成可以在一个自回归框架内无缝集成。为了保留预训练文本大语言模型的内在语言理解和生成能力，同时实现跨模态交互，引入了三种关键的方法设计，以实现模态间协调，包括语音词元分组与拆分，以及语音精细头。

语音词元分组。离散词元的一个关键参数是采样率，它决定了输入/输出序列的长度。GLM-4-Voice（Zeng 等，2025）在LibriSpeech上研究了从 $6.25 \mathrm{Hz}$ 到 $50 \mathrm{Hz}$ 的采样率，结果显示 $50 \mathrm{Hz}$ 和 $25 \mathrm{Hz}$ 之间的词错误率（WER）差异很小，但在 $12.5 \mathrm{Hz}$ 和 $6.25 \mathrm{Hz}$ 时则显著下降。因此，我们的工作采用 $25 \mathrm{Hz}$ 的采样率。为了解决语音信号（25Hz）与文本词元化（$\sim 3 \mathrm{Hz}$）（Chen 等，2024a）之间的时间分辨率不匹配，设计了一种分组机制：

$$
\mathbf { g } _ { i } = \mathrm { L i n e a r } \left( \begin{array} { c } { ( i + 1 ) k - 1 } \\ { \underset { j = i k } { \parallel } } \end{array} \mathbf { s } _ { j } \right) \in \mathbb { R } ^ { d _ { \mathrm { t e x t } } }
$$

其中 ${ \bf s } _ { j }$ 表示语音词元，$\parallel$ 表示特征拼接，$k$ 是由语音词元速率与文本词元速率之间的比率决定的分组因子。该设计将语音词元的长度从 $T$ 压缩到 $T / k$，并使得生成的分组语音表示与文本更好对齐。值得注意的是，与 Chen 等（2024a）采用线性投影将音频对数转化为分组大小的表示以进行并行多词元预测不同，DRVoICE 战略性地设计了一个解组机制，并集成了一个专门的语音细化头（SRH），以支持单个语音词元的自回归生成。

语音精炼头（SRH）。SRH旨在增强语音生成能力。它利用共享大语言模型层（SLLM）最后的隐藏状态作为条件输入，并结合上下文语音信息以自回归方式生成语音词元。尽管传统的语音分组策略有效地将语音词元聚类为具有语义意义的单元，以用于语音识别和理解任务（Chen et al., 2024a；Zhang et al., 2024b），但我们的实验揭示了它们在生成场景中的固有局限性，因为语音词元的分组不可避免地会丢失一些细粒度的声学细节。为了解决这一局限，DrVoICE 执行如下的解组过程：将 SLLM 的最终隐藏状态通过线性投影和时间切分映射到组大小的嵌入，其中 $\mathbf { h } _ { u g } ^ { ( i ) } \in \mathbb { R } ^ { d _ { u g } / k }$。得到的 $\mathbf { H }$ 生成语音词元。根据我们在 SLLM 上的实践，前驱语音词元的表示和 $\mathbf { H }$ 被聚合。语音词元预测训练旨在最大化条件概率：

$$
\mathbf { h } _ { u g } = \mathbf { W } _ { p } \mathbf { h } _ { L } ^ { \mathrm { [ S L L M ] } } \quad \mathrm { w h e r e } \quad \mathbf { W } _ { p } \in \mathbb { R } ^ { d _ { g } \times d _ { h } } ,
$$

$$
\mathbf { H } = \mathrm { S p l i t } _ { k } ( \mathbf { h } _ { u g } ) = [ \mathbf { h } _ { u g } ^ { ( 1 ) } , \mathbf { h } _ { u g } ^ { ( 2 ) } , \ldots , \mathbf { h } _ { u g } ^ { ( k ) } ] ,
$$

$$
\mathcal { L } _ { \mathrm { S R H } } = - \sum _ { i = 1 } ^ { T } \log P ( s _ { i } | s _ { < i } , \mathbf { H } _ { < i } ) ,
$$

其中 $s _ { i }$ 表示第 $i$ 个语音词元。通过优化该目标，SRH 学会在前面的语音词元和从 SLLM 中提取的丰富上下文嵌入 $\mathbf { H }$ 的条件下预测后续语音词元。这一设计使 SRH 能够有效利用 SLLM 的隐藏表示中编码的语义和声学信息，因而与传统的基于分组的方法相比，能够生成更加自然和连贯的语音输出。E2E 训练目标 $\mathcal { L } _ { \mathrm { M L L M } }$ 通过多任务学习整合了这两种损失：

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { M L L M } } = \lambda \mathcal { L } _ { \mathrm { T H } } + \mu \mathcal { L } _ { \mathrm { S R H } } , } \end{array}
$$

$$
\mathcal { L } _ { \mathrm { T H } } = - \sum _ { i = 1 } ^ { T } \log P ( t _ { i } | c _ { < i } , \mathbf { g } ) ,
$$

其中 ${ \mathcal { L } } _ { \mathrm { T H } }$ 是文本头上的自回归损失，$\lambda$ 和 $\mu$ 是超参数。

Table 1: Multimodal Interaction Patterns.   

<table><tr><td>Pattern Name</td><td>Abbr.</td><td>Modality Flow</td></tr><tr><td>Speech-to-Multimodal</td><td>S2M</td><td>Speech → Joint speech-text response</td></tr><tr><td>Speech-to-Text</td><td>S2T</td><td>Speech → Text-only response</td></tr><tr><td>Text-to-Multimodal</td><td>T2M</td><td>Text → Joint speech-text response</td></tr><tr><td>Text-to-Text</td><td>T2T</td><td>Text → Text-only response</td></tr><tr><td>Speech-Text Chain</td><td>STC</td><td>Speech → Text transcription → Text response → Multimodal response</td></tr><tr><td>Speech-Assisted Chain</td><td>SAC</td><td>Speech → Text response (agent perspective) → Multimodal response</td></tr><tr><td>Speech-User Chain</td><td>SUC</td><td>Speech → Text transcription (user perspective) → Multimodal response</td></tr></table>

# 3.3 训练策略

初始化。语音编码器使用 Whisper-Large-v3 的权重进行初始化，而共享语言模型层则使用 Qwen2.5 进行初始化。此外，采用并在整个训练过程中保持固定的 CosyVoice 的预训练语音分词器和解码器。我们使用预训练的文本转语音模型初始化 SRH。

CoM-Mixing训练。链式模态（CoM）方法（Zhang等，2023a）通过利用中间文本转录可以提高性能，这特别适合于实时处理不关键的场景。此外，实际应用涉及需要同时输出语音和文本或仅输出文本的场景，要求系统根据特定需求动态生成不同的模态。针对多样的输入输出需求，表1总结了七种交互模式。系统提示用于调节模型的输出行为。例如，提示语如“[系统] 你是一个有帮助的助手，被要求同时生成文本和语音词元。”明确指导模型生成多模态输出。详细提示在附录B中展示。在推理过程中，指定这些系统提示可以生成期望的输出结果。对于CoM-Mixing训练，我们按照七种交互模式构建数据变体，并获得混合数据以用于模型训练。在推理过程中，用户可以手动将适当的系统提示加在输入序列前，以满足输出需求。这种灵活性确保了对不同模态生成需求的适应性。

核心鸡尾酒训练。我们识别出一个学习率困境：使用高学习率显著影响了多语言大模型（MLLM）的性能，而低学习率则导致训练停滞，损失减少非常缓慢。为了克服这一优化挑战，我们开发了一种专门的两阶段训练策略，称为“核心鸡尾酒”。第一阶段包括以相对较高的学习率对整个多语言大模型进行全面微调。该阶段的目标是迅速将模型参数移动到损失景观的更有利区域。然而，为了抵消这种激进初始训练可能导致的多语言大模型性能下降，我们引入了一个中间合并步骤。借鉴自Xiao等人（2024），在第一阶段训练的多语言大模型的参数 $( M _ { 1 } )$ 与基础的预训练大语言模型 $( M _ { 0 } )$ 的参数进行合并。这个合并创建了一个新的插值模型 $M _ { r }$，其定义如下方程：

$$
M _ { r } \gets \alpha M _ { 1 } + ( 1 - \alpha ) M _ { 0 }
$$

其中 $M _ { r }$ 表示合并模型，$\alpha$ 是插值权重。合并步骤有效地重新整合了基础大语言模型的稳健知识。较小的 $\alpha$ 对应于对基础大语言模型能力的更大保留。第二阶段对合并模型 $M _ { r }$ 进行全量微调，使用较小的学习率。核心鸡尾酒训练方法允许进行仔细而精准的优化，提升模型性能，而不会因学习率过高而导致不稳定。

# 4 实验

# 4.1 实验设置

数据集。本研究采用了大约100K小时的音频-文本配对数据，用于语音-文本模态对齐的预训练Speech Refined Head。对于DRVoICE的后训练，我们首先使用CosyVoice（Du et al.，2024a，b，2025）合成大约3B文本标记的语音，然后根据合成语音的词错误率（WER），选择约26K小时用于语音对话，以及约20K小时的用户语音和1.3B助手标记用于语音转文本对话。此外，为了增强模型对真实语音的理解，训练数据增加了大约10K小时的英语自动语音识别（ASR）数据，包含多个语料库，包括Common Voice（Ardila et al.，2020）、MELD（Poria et al.，2019）、LibriSpeech（Panayotov et al.，2015）、SPGISpeech（O'Neill et al.，2021）和Voxpopuli（Wang et al.，2021）。根据之前的研究（Yao et al.，2024；KimiTeam et al.，2025；OpenAI，2024b），我们在广泛使用的基准测试上评估性能，包括VoiceBench（Chen et al.，2024b）和OpenAudioBench 1进行语音转文本。

Table 2: Performance Comparison on various benchmarks in terms of benchmark-specific metrics (the best result in each row is in bold). With the exception of GLM4-Voice, whose results are cited from Xu et al. (2025) and Chen et al. (2024b), all other results were generated by running inference on the released checkpoints. $\mathbf { F R ( I n / O u t ) }$ denotes the input speech frame rate and the output speech plus text frame rate for the LLM backbone. $\tau$ denotes the average number of text tokens corresponding to one second of speech.   

<table><tr><td></td><td>GLM4-Voice</td><td>MiniCPM -0 2.6</td><td>Baichuan -Omni-1.5</td><td>Qwen2.5 -Omni</td><td>Kimi -Audio</td><td>Step-Audio2 -Mini</td><td>DrVoice</td></tr><tr><td>FR (In/Out)</td><td>12.5/12.5+τ</td><td>25/T</td><td>12.5/12.5+7</td><td>25/T</td><td>12.5/12.5</td><td>12.5/25+T</td><td>5/5</td></tr><tr><td colspan="8">OpenAudioBench (S2T)</td></tr><tr><td>AlpacaEval</td><td>57.89</td><td>64.10</td><td>77.90</td><td>72.76</td><td>75.73</td><td>59.60</td><td>78.34</td></tr><tr><td>Llama Q.</td><td>76.00</td><td>78.00</td><td>78.50</td><td>75.33</td><td>79.33</td><td>75.00</td><td>80.33</td></tr><tr><td>Reasoning QA</td><td>47.43</td><td>38.60</td><td>50.00</td><td>63.76</td><td>58.02</td><td>46.04</td><td>57.92</td></tr><tr><td>TriviaQA</td><td>51.80</td><td>63.00</td><td>57.20</td><td>57.06</td><td>62.10</td><td>57.70</td><td>61.50</td></tr><tr><td>Web Q.</td><td>55.40</td><td>69.20</td><td>59.10</td><td>62.80</td><td>70.20</td><td>65.10</td><td>68.10</td></tr><tr><td>Overall</td><td>57.70</td><td>62.58</td><td>64.54</td><td>66.34</td><td>69.08</td><td>60.69</td><td>69.24</td></tr><tr><td colspan="8">VoiceBench (S2T)</td></tr><tr><td>AlpacaEval</td><td>3.97</td><td>4.42</td><td>4.50</td><td>4.33</td><td>4.46</td><td>4.17</td><td>4.52</td></tr><tr><td>CommonEval</td><td>3.42</td><td>4.15</td><td>4.05</td><td>3.84</td><td>3.97</td><td>3.00</td><td>3.77</td></tr><tr><td>SD-QA</td><td>36.98</td><td>50.72</td><td>43.40</td><td>57.41</td><td>63.12</td><td>56.06</td><td>68.54</td></tr><tr><td>MMSU</td><td>39.75</td><td>54.78</td><td>57.25</td><td>56.38</td><td>62.17</td><td>52.18</td><td>60.31</td></tr><tr><td>OpenBookQA</td><td>53.41</td><td>78.02</td><td>74.51</td><td>79.12</td><td>83.52</td><td>64.18</td><td>79.56</td></tr><tr><td>IFEval</td><td>52.80</td><td>49.25</td><td>54.54</td><td>53.88</td><td>61.10</td><td>38.01</td><td>59.30</td></tr><tr><td>AdvBench</td><td>88.08</td><td>97.69</td><td>97.31</td><td>99.62</td><td>100.00</td><td>93.08</td><td>98.65</td></tr><tr><td>Overall</td><td>59.83</td><td>71.69</td><td>71.14</td><td>72.83</td><td>76.93</td><td>63.84</td><td>76.02</td></tr><tr><td colspan="8">UltraEval-Audio (S2S)</td></tr><tr><td>AlpacaEval</td><td>51.00</td><td>51.00</td><td>58.69</td><td>56.10</td><td>44.20</td><td>51.72</td><td>49.65</td></tr><tr><td>Llama Q.</td><td>50.00</td><td>61.00</td><td>67.33</td><td>66.30</td><td>57.33</td><td>67.67</td><td>68.00</td></tr><tr><td>TriviaQA</td><td>36.40</td><td>40.20</td><td>30.57</td><td>40.52</td><td>35.71</td><td>33.50</td><td>35.35</td></tr><tr><td>Web Q.</td><td>32.00</td><td>40.00</td><td>38.09</td><td>38.93</td><td>33.90</td><td>34.65</td><td>37.65</td></tr><tr><td>Overall</td><td>42.35</td><td>48.05</td><td>48.67</td><td>50.46</td><td>42.79</td><td>46.89</td><td>47.66</td></tr><tr><td colspan="8">Big Bench Audio (S2T &amp; S2S)</td></tr><tr><td>S2T</td><td>44.8</td><td>56.2</td><td>47.1</td><td>54.2</td><td>59.4</td><td>50.9</td><td>71.6</td></tr><tr><td>S2S</td><td>42.7</td><td>55.4</td><td>44.6</td><td>53.6</td><td>51.0</td><td>47.5</td><td>60.9</td></tr><tr><td>Overall</td><td>43.8</td><td>55.8</td><td>45.8</td><td>53.9</td><td>55.2</td><td>49.2</td><td>66.3</td></tr></table>

Table 3:Speech quality performance on the collections of UltraEval-Audio, in terms of UTMOS for assessing overal speech quality and ASR-WER for assessing alignment between generated speech and text. $\mathbf { F R ( I n / O u t ) }$ indicates the input speech frame rate and the output (speech $^ +$ text) frame rate for the LLM backbone, while $\tau$ represents the average text tokens per second of speech.   

<table><tr><td>Model</td><td>FR(In/Out)↓</td><td>UTMOS↑</td><td>ASR-WER↓</td></tr><tr><td>MiniCPM-o 2.6 (2025)</td><td>25/τ</td><td>4.18</td><td>13.17</td></tr><tr><td>Baichuan-Omni-1.5 (2025)</td><td>12.5/12.5+7</td><td>4.27</td><td>23.38</td></tr><tr><td>Qwen2.5-Omni (2025)</td><td>25/T</td><td>4.28</td><td>3.48</td></tr><tr><td>Kimi-Audio (2025)</td><td>12.5/12.5</td><td>3.06</td><td>21.06</td></tr><tr><td>Step-Audio2-mini (2025)</td><td>12.5/25+T</td><td>4.53</td><td>9.5</td></tr><tr><td>DrVoice</td><td>5/5</td><td>4.29</td><td>11.2</td></tr></table>

Table 4: Ablation study of Continuous Speech Encoder (CSE), Speech Refined Head (SRH), SRHPretraining and CoM-Mixing Training on the Llama Questions dataset. S2M, S2T, etc are defined in Table 1.   

<table><tr><td>Model</td><td>S2M (T/S)</td><td>S2T</td><td>T2M (T/S)</td><td>T2T</td><td>STC (T/S)</td><td>SAC (T/S)</td><td>SUC (T/S)</td></tr><tr><td>DRVoICE-Small</td><td>68.67 / 56.00</td><td>72.33</td><td>72.33 / 56.00</td><td>75.33</td><td>75.67 / 68.33</td><td>71.67 / 62.67</td><td>73.33 / 62.00</td></tr><tr><td>w/o. CSE</td><td>61.67 / 53.00</td><td>62.33</td><td>70.00 / 60.00</td><td>74.00</td><td>69.33 / 61.00</td><td>63.00 / 55.00</td><td>66.33 / 58.67</td></tr><tr><td>w/o. SRH-Pretraining</td><td>38.33 / 30.33</td><td>56.00</td><td>59.33 / 46.33</td><td>73.33</td><td>67.33 / 57.67</td><td>54.00 / 42.33</td><td>54.33 / 42.67</td></tr><tr><td>w/o. SRH</td><td>21.67 / 15.33 56.00</td><td></td><td>45.22 / 35.00</td><td>73.00</td><td>64.33 / 50.67</td><td>55.67 / 42.33 40.33 / 27.67</td><td></td></tr><tr><td>w/o. CoM-Mixing</td><td>58.00 / 49.00</td><td>58.00</td><td>69.33 / 55.00</td><td>68.33</td><td>-/−</td><td>-/−</td><td>-/−</td></tr></table>

文本 $S T$ 评估、用于语音到语音的 UltraEval-Audio 2 评估，以及用于两种评估的 Big Bench Audio 3。评估指标。评估遵循各自基准的既定协议。具体而言，对于 AlpacaEval 和 CommonEval 上的开放式问答任务，使用 G-Eval（Liu 等，2023）进行评分。对于 AdvBench，报告拒绝率，而所有其他基准的性能则通过准确率评估。生成的语音使用 Whisper-v3-large 模型（Radford 等，2022）进行转录，然后计算转录文本与生成文本之间的字错误率（WER，表示为 ASR-WER），以评估生成的语音与文本之间的对齐程度。使用 UTMOS（Saeki 等，2022）来评估整体语音质量，遵循 Zeng 等（2025）。基准。选择具有代表性和竞争力的开源音频语言模型作为基准，以涵盖多样的建模范式：文本驱动模型包括 MiniCPM-o 2.6 (8B)（Yao 等，2024）和 Qwen2.5-Omni (7B)（Xu 等，2025）。在联合语音-文本模型中，包括 GLM-4-Voice (9B)（Zeng 等，2025）、Baichuan-Omni-1.5 (7B)（Li 等，2025）和 Step-Audio2-Mini (8B)（Wu 等，2025）的交错模型，以及并行模型 KimiAudio (7B)（KimiTeam 等，2025）。该套件能够系统地比较主流语音-文本建模策略。实施细节可以在附录 A 中找到。

# 4.2 主要结果

整体表现。表2比较了DrVoICE (7B)与代表性和竞争性基准在语音转文本 $( S { } \mathrm { T } )$ 和语音转语音 $( S { } S )$ 生成性能上的表现。表中显示，DrVoICE在广泛任务中展现出卓越的能力，在OpenAudioBench（音频理解）上取得69.24的整体分数，成为新的最先进技术（SOTA），在Big Bench Audio（推理与理解）上获得66.3，明显超过第二名模型10多分。DrVoICE在VoiceBench（基于LLM的语音助手基准测试）上也取得了与SOTA相当的表现，得分为76.02，仅稍逊于领先者的76.93，并且在UltraEval-Audio（语音理解与生成）上保持高度竞争力。DrVoICE在多样基准上的强劲和平衡表现，确立了其作为${ \sim } 7 B$参数系列中领先的开源语音基础模型。

计算效率与语音质量。DrVoICE 的一项关键创新是其显著的计算效率，如表 2 和表 3 所示。如 FR $\mathbf { ( I n / O u t ) }$ 行所示，DRVoICE 的帧率为 5/5，表明其 LLM 主干网络每秒仅处理 5 个音频词元，用于输入和输出。这与其他模型的帧率 12.5 或 25 相比，显著降低了计算需求和潜在延迟。至关重要的是，这种效率并未影响语音质量。表 3 显示，尽管帧率较低，DrVoICE 仍能生成高质量且良好对齐的语音。其整体语音质量的 UTMOS 得分为 4.29，这在与表现优异的模型如 Qwen2.5-Omni（4.28）相比时具有竞争力，而优于其他模型如 Kimi-Audio（3.06）。模型的 ASR-WER 为 11.2，显示出稳健的语音与文本对齐能力，并超越了多个基线模型。然而，与 Qwen2.5-Omni 相比，其表现仍有不足。一个可能的解释在于架构设计：Qwen2.5-Omni 直接将文本输入“Talker”模块，而提出的模型仅将隐状态传递给 SRH。因此，将文本引入语音精细头是改善文本控制和降低 ASR-WER 的合理下一步。这种最先进性能、高质量语音生成和无与伦比的效率的独特结合，使 DrVoICE 成为现实应用中极为强大且实用的模型。

# 4.3 消融实验与分析

我们进行广泛的消融研究和分析。为了提高计算效率，一些实验是在 DRVoICE-Small (1.5B) 上进行的。关于数据质量和数据规模的更多分析可以在附录 C 中找到。核心鸡尾酒培训策略。在第一阶段的训练中，性能从文本基准的 81.77 降至 70.19，但第二阶段扭转了这一趋势，使性能恢复至 74.73。这一结果确认了该策略有效地抵消了初始退化，并导致模型优化。详细信息可在附录 C 中找到。连续语音编码器。如表 4 所示，连续语音编码器 (CSE) 对涉及语音输入的任务至关重要。从 DrVoICE-Small 中移除它会导致语音理解和语音生成方面显著的性能下降。具体而言，S2T 分数从 72.33 降至 62.33（相对下降 $1 3 . 8 \%$），而 S2M (T) 分数从 68.67 降至 61.67（相对下降 $1 0 . 2 \%$）。相比之下，文本任务的影响很小，T2T 分数仅从 75.33 稍微下降至 74.00。这确认了 CSE 在表示语音方面的有效性。

双分辨率语音表示（DRSR）。我们的双分辨率方法结合了输入分组以提高效率和理解能力，以及用于高质量生成的细化头。1) 分组因子。与降低生成质量相反，分组显著提高了语音理解（S2T）和语音到语音生成（S2M）。例如，将分组因子从1增加到5，使S2T得分从55.67提高到63.33（相对增益为$1 3 . 7 \%$）。S2M（T/S）得分显著提高，在分组因子为5时达到峰值$3 7 . 6 7 \ : / \ : 2 8 . 0 0$。此外，使用分组因子5而非1使得每个设置中GPU小时减少近$50 \%$，证明了分组机制的效率。详细结果见附录C。2) 语音细化头。正如表4所示，语音细化头（SRH）在语音标记生成任务中非常有效。通过比较带有SRH的模型（无SRH预训练）和不带的模型（无SRH），我们观察到添加SRH使得S2M（T）实现了$7 6 . 9 \%$的相对提升（从21.67到38.33），而T2M（T）实现了$3 1 . 2 \%$的相对提升（从45.22到59.33）。仅文本性能（T2T）保持稳定，确认SRH增强了语音生成而不干扰文本处理通路。我们的双分辨率架构有效地组合了这些组件，利用分组实现高效理解，并在原始帧分辨率下使用SRH进行语音生成。SRH预训练。为了检验预训练的影响，我们去除了SRH的预训练步骤，并重新训练模型。如表4所示，去除预训练（比较无CSE与无SRH预训练的效果）对语音生成的影响最大，使得S2M（T）性能下降$3 7 . 8 \%$，T2M（T）下降$1 5 . 2 \%$。对S2T的影响较小（下降$1 0 . 2 \%$），对T2T几乎没有影响。这强调了SRH预训练在细化语音标记生成中的关键重要性。CoM混合训练策略。如表4所示，由上下文系统提示指导的任务（STC/SAC/SUC）相比直接的S2M生成表现出显著改善。例如，STC（T）得分为75.67，显著超出S2M（T）基线68.67。这表明模型成功学习采用由系统提示指导的生成范式。没有CoM混合训练的情况（无CoM混合训练）在S2M（T）中导致了$1 5 . 5 \%$的相对性能下降（从68.67降至58.00），确认了我们混合模态训练方法的价值。

# 5 结论

我们介绍了 DrVoICE，这是一种新颖的并行语音-文本对话模型，利用了联合自回归建模与双分辨率语音表示。我们的实验结果表明，DrVoICE 在 OpenAudioBench 上为音频理解建立了新的最先进水平（SOTA），在 Big Bench Audio 上为推理能力也达到了新的高度。此外，它在 VoiceBench 上的语音助手任务和在 UltraEvalAudio 上的语音理解与生成方面的性能与 SOTA 比肩，验证了其强大而多样的能力。值得注意的是，其双分辨率机制显著提升了推理速度和计算效率，而不牺牲性能。我们在附录 D 中讨论了局限性和未来的工作。

参考文献 安珂宇, 陈乾, 邓冲, 杜志豪, 高长风, 高志福, 顾悦, 何婷, 胡航瑞, 胡凯, 姜胜鹏, 李亚斌, 李泽瑞, 陆恒, 罗昊能, 吕翔, 马斌, 马子阳, 倪崇佳, 宋长河, 石佳奇, 石鲜, 王浩, 王文, 王宇轩, 肖长宇, 闫志杰, 杨烨鑫, 张斌, 张清林, 张世良, 赵南, 郑思奇. 2024. Funaudiollm: 人与大语言模型之间自然交互的语音理解与生成基础模型. CoRR, abs/2407.04051. 阿尔迪拉, 布兰森, 戴维斯, 科勒, 梅耶, 亨雷提, 莫赖斯, 桑德斯, 泰尔斯, 韦伯. 2020. Common voice: 一种大规模多语言语音语料库. 在第12届语言资源与评估会议，LREC 2020，法国马赛，2020年5月11日至16日, 页42184222. 欧洲语言资源协会. 博尔索斯, 马里涅尔, 文森特, 哈里顿诺夫, 皮耶昆, 沙里菲, 罗布雷克, 特布尔, 格朗杰, 塔格利亚萨基, 泽吉杜尔. 2023. Audiolm: 一种用于音频生成的语言建模方法. IEEE ACM Trans. Audio Speech Lang. Process., 31:25232533. 陈乾, 陈亚风, 陈焱霓, 陈梦哲, 陈应达, 邓冲, 杜志豪, 高瑞泽, 高长风, 高志福, 李亚斌, 吕翔, 刘家庆, 罗昊能, 马斌, 倪崇佳, 石鲜, 唐佳龙, 王辉, 王浩, 王文, 王宇轩, 许云兰, 于帆, 闫志杰, 杨烨鑫, 杨显, 杨冠柔, 赵天宇, 张清林, 张世良, 赵培, 张崇, 周进仁. 2025. Minmo: 一种无缝语音交互的多模态大语言模型. CoRR, abs/2501.06282. 陈文熙, 马子阳, 闫瑞琦, 梁宇哲, 李西泉, 许瑞扬, 牛志康, 朱彦桥, 杨逸凡, 刘战寻, 等. 2024a. Slam-omni: 一种具有单阶段训练的音色可控语音交互系统. arXiv预印本 arXiv:2412.15649. 陈怡鸣, 岳向湖, 张晨, 高晓雪, 罗比·T·谭, 李海舟. 2024b. Voicebench: 基于大语言模型的语音助手基准测试. CoRR, abs/2410.17196. 朱云飞, 许劲, 杨倩, 魏昊杰, 魏溪平, 郭志芳, 冷易冲, 吕元俊, 贺金正, 林俊扬, 周畅, 周进仁. 2024. Qwen2-音频技术报告. CoRR, abs/2407.10759. 朱云飞, 许劲, 周晓欢, 杨倩, 张世良, 闫志杰, 周畅, 周进仁. 2023. Qwen-音频: 通过统一的大规模音频-语言模型推动通用音频理解. CoRR, abs/2311.07919. 亚历山大·德福斯, 贾德·科佩特, 加布里埃尔·辛奈夫, 约西·阿迪. 2023. 高保真神经音频压缩. Trans. Mach. Learn. Res., 2023. 亚历山大·德福斯, 劳伦特·马扎雷, 曼努·奥尔西尼, 阿梅莉·罗耶, 帕特里克·佩雷斯, 赫尔维·杰古, 爱德华·格拉夫, 泽吉杜尔. 2024. Moshi: 一种用于实时对话的语音-文本基础模型. CoRR, abs/2410.00037. 亚历山大·德福斯, 劳伦特·马扎雷, 曼努·奥尔西尼, 阿梅莉·罗耶, 帕特里克·佩雷斯, 赫尔维·杰古, 爱德华·格拉夫, 泽吉杜尔. 2024. Moshi: 一种用于实时对话的语音-文本基础模型. 技术报告. 杜志豪, 陈乾, 张世良, 胡凯, 陆恒, 杨烨鑫, 胡航瑞, 郑思奇, 顾悦, 马子阳, 高志福, 闫志杰. 2024a. Cosyvoice: 基于有监督语义词元的可扩展多语言零-shot文本到语音合成器. CoRR, abs/2407.05407. 杜志豪, 高长风, 王宇轩, 于帆, 赵天宇, 王浩, 吕翔, 王辉, 倪崇佳, 石鲜, 安珂宇, 杨冠柔, 李亚斌, 陈焱霓, 高志福, 陈乾, 顾悦, 陈梦哲, 陈亚风, 张世良, 王文, 叶洁平. 2025. Cosyvoice 3: 通过扩大规模和后训练实现野外语音生成. CoRR, abs/2505.17589. 杜志豪, 王宇轩, 陈乾, 石鲜, 吕翔, 赵天宇, 高志福, 杨烨鑫, 高长风, 王辉, 于帆, 刘华戴, 盛正炎, 顾悦, 邓冲, 王文, 张世良, 闫志杰, 周进仁. 2024b. Cosyvoice 2: 基于大语言模型的可扩展流式语音合成. CoRR, abs/2412.10117. 方青凯, 郭守涛, 周艳, 马正瑞, 张少雷, 冯扬. 2024. Llamaomni: 与大语言模型无缝语音交互. CoRR, abs/2409.06666. 傅朝友, 林浩佳, 王雄, 张逸凡, 沈韵航, 刘晓宇, 曹昊宇, 龙祖伟, 高和婷, 李科, 马龙, 郑晓武, 吉榕榕, 孙星, 单采锋, 何然. 2025. VITA-1.5: 朝着 GPT-4o 级别实时视觉与语音交互迈进. CoRR, abs/2501.01957. 迈克尔·哈西德, 塔尔·瑞梅兹, 阮安, 伊泰·加特, 亚历克西斯·孔诺, 费利克斯·克鲁克, 贾德·科佩特, 亚历山大·德福斯, 加布里埃尔·辛奈夫, 艾曼纽·杜波, 罗伊·施瓦茨, 约西·阿迪. 2023. 以文本为导向的预训练语音语言模型. 在神经信息处理系统进展 36: 2023年神经信息处理系统年会, NeurIPS 2023, 美国路易斯安那州新奥尔良, 2023年12月10日至16日. 徐维宁, 本杰明·博尔特, 蔡耀宏·霍伯特, 拉科提亚·库杀尔, 鲁斯兰·萨拉胡丁诺夫, 阿卜杜拉赫曼·穆罕默德. 2021. Hubert: 通过隐藏单元的遮挡预测进行自监督语音表示学习. IEEE ACM Trans. Audio Speech Lang. Process., 29:34513460.

艾琳·黄，博勇·吴，布鲁斯·王，超·严，晨·胡，城里·冯，飞·田，飞宇·沈，静北·李，铭瑞·陈，彭·刘，睿行·苗，王·尤，希·陈，学瑞·杨，叶畅·黄，玉翔·张，正·龚，子鑫·张，布莱恩·李，长义·万，汉鹏·胡，然晨·铭，宋·袁，雪琳·张，宇·周，秉昕·李，步云·马，康·安，伟·季，文·李，轩·温，远凯·马，远伟·梁，云·牟，巴赫提亚尔·艾哈迈迪，滨·王，博·李，长鑫·苗，晨·徐，城廷·冯，晨润·王，大鹏·石，德善·孙，鼎元·胡，杜拉·赛，恩乐·刘，冠哲·黄，谷林·阎，亨·王，浩南·贾，浩阳·张，家豪·龚，JWu·居·贾·他·齐·街·范·吴，J·W·金国·王，静扬·张，俊哲·林，凯翔·李，磊·夏，李·周，龙龙·谷，梅·陈，梦琳·吴，铭·李，铭晓·李，铭耀·梁，娜·王，聂·浩，奇灵·吴，琴媛·谭，少良·庞，世良·杨，树利·高，思琪·刘，思彤·刘，天诚·曹，天宇·王，文进·邓，文清·何，文·孙，欣·韩，小敏·邓，小佳·刘，徐·赵，亚楠·韦，彦博·余，阳·曹，阳光·李，阳震·马，彦明·徐，雅强·施，怡·王，隐敏·钟，宇·罗，远伟·陆，玉和·尹，玉婷·阎，玉翔·杨，哲·谢，正·戈，正·孙，哲维·黄，志超·常，子东·杨，子立·张，宾兴·焦，大新·姜，向阳·舜，建胜·陈，静·李，淑长·周，湘宇·张，新华·张，和博·朱。2025年。步音频：智能语音交互中的统一理解与生成。预印本，arXiv:2502.11946。迪拉杰·D·卡拉姆卡尔，德赫瓦察·穆迪盖，纳维恩·梅伦普迪，迪潘卡尔·达斯，库纳尔·班杰，萨西坎特·阿凡查，达尔玛·特贾·沃图里，纳塔拉杰·贾马拉马达卡，建宇·黄，赫克托·袁，季言·杨，钟秀·朴，亚历山大·海内克，埃万吉洛斯·乔尔加纳斯，苏达尔山·斯瑞尼瓦森，阿比塞克·昆杜，米莎·斯梅良斯基，巴拉特·考尔，普拉迪普·杜贝。2019年。一项关于深度学习训练中BFLOAT16的研究。CoRR, abs/1905.12322。KimiTeam，丁丁，泽千·举，一冲·冷，松翔·刘，彤·刘，泽宇·尚，凯·沈，伟·宋，旭·谭，和怡·唐，正涛·王，初·韦，逸飞·辛，欣然·徐，建伟·于，宇涛·张，晓宇·周，Y·查尔斯，俊·陈，艳如·陈，玉伦·杜，伟然·何，振兴·胡，国昆·赖，庆成·李，阳阳·刘，卫东·孙，建洲·王，玉之·王，岳峰·吴，宇欣·吴，东超·杨，浩·杨，樱·杨，志林·杨，傲雄·尹，瑞彬·袁，玉彤·张，载达·周。2025年。Kimi音频技术报告。预印本，arXiv:2504.18425。钟一尔，凯赫延·金，和在京·白。2020年。Hifi-gan：高效高保真语音合成的生成对抗网络。发表于神经信息处理系统大会第33届年会，NeurIPS 2020，2020年12月6-12日，虚拟会议。亚东·李，俊·刘，涛·张，松·陈，天鹏·李，泽焕·李，丽君·刘，灵峰·闵，国生·董，大·潘，等。2025年。百川-全能-1.5技术报告。arXiv预印本arXiv:2501.15368。雅龙·利普曼，瑞奇·T·Q·陈，赫莉·本-哈穆，马克西米连·尼克尔，和马修·勒。2023年。流匹配生成建模。发表于第十一届国际学习表示会议，ICLR 2023，卢旺达基加利，2023年5月1-5日。OpenReview.net。杨·刘，丹·伊特尔，怡冲·徐，硕航·王，若辰·徐，和晨光·朱。2023年。G-eval：使用gpt-4进行更好的人工对齐的NLG评估。发表于2023年自然语言处理实证方法大会，EMNLP 2023，新加坡，2023年12月6-10日，页2511-2522。计算语言学协会。佐伟·龙，云航·沈，超游·傅，和婷·高，丽江·李，佩贤·陈，梦丹·张，杭·邵，建·李，金龙·彭，浩宇·曹，柯·李，荣荣·季，和兴·孙。2025年。维塔音频：高效大型语音语言模型的快速交错跨模态词元生成。CoRR, abs/2505.03739。伊利亚·洛希奇洛夫和弗兰克·哈特。2019年。解耦权重衰减正则化。在第七届国际学习表示会议，ICLR 2019，美国路易斯安那州新奥尔良，2019年5月6-9日。OpenReview.net。OpenBMB MiniCPM-o团队。2025年。Minicpm-o 2.6：用于视听和多模态实时流的GPT-4o级MLLM。帕特里克·K·奥尼尔，维塔利·拉夫鲁金，索姆舒布拉·马朱姆达，瓦希德·诺鲁齐，岳凯·张，奥列克西·库查耶夫，贾加迪什·巴拉姆，尤利娅·多夫任科，基南·弗雷伯格，迈克尔·D·舒尔曼，博里斯·金斯堡，渡边信司，和乔治·库克斯科。2021年。Spgispeech：5000小时的财务音频转录，用于完全格式化的端到端语音识别。发表于国际语音通信协会第22届年会，Interspeech 2021，捷克布尔诺，2021年8月30日至9月3日，页1434-1438。ISCA。OpenAI。2024b。Hello GPT-40。瓦西尔·帕纳约托夫，国国·陈，丹尼尔·波维，和桑吉夫·库丹普尔。2015年。Librispeech：基于公共领域有声书的ASR语料库。在2015年IEEE国际声学、语音与信号处理大会，ICASSP 2015，澳大利亚南布里斯班，2015年4月19-24日，页5206-5210。IEEE。苏贾尼亚·波里亚，德瓦曼悠·哈扎里卡，纳沃尼尔·马朱姆德，戈塔姆·奈克，埃里克·坎布里亚，和拉达·米哈雷察。2019年。MELD：一个用于对话中情感识别的多模态多方数据集。发表于计算语言学协会第57届会议，ACL 2019，意大利佛罗伦萨，2019年7月28日至8月2日，卷1：长篇论文，页527-536。计算语言学协会。亚历克·拉德福德，钟旭·金，涛·徐，格雷格·布罗克曼，克里斯汀·麦克利维，和伊利亚·苏茨凯弗。2022年。通过大规模弱监督实现鲁棒语音识别。arXiv预印本。亚历克·拉德福德，钟旭·金，涛·徐，格雷格·布罗克曼，克里斯汀·麦克利维，和伊利亚·苏茨凯弗。2023年。通过大规模弱监督实现鲁棒语音识别。发表于机器学习国际会议，ICML 2023，2023年7月23-29日，美国夏威夷檀香山，机器学习研究汇编第202卷，页2849-228518。PMLR。萨米扬·拉吉班达里，杰夫·拉斯利，奥拉图吉·鲁瓦斯，和宇雄·何。2020年。Zero：训练万亿参数模型的内存优化。发表于高性能计算、网络、存储与分析国际会议，SC 2020，虚拟会议/美国亚特兰大，2020年11月9-19日，页20。IEEE/ACM。佐伯隆树，忻·新，若田·中田，智树·小宫山，真纪·高道，和广志·猿渡。2022年。UTMOS：东京大学萨鲁实验室的语音挑战2022系统。在国际语音通信协会第23届年会，Interspeech 2022，韩国仁川，2022年9月18-22日，页4521-4525。ISCA。长汉·王，摩根·里维埃，安·李，安·吴，查伊特纳·塔尔尼卡，丹尼尔·哈齐扎，玛丽·威廉姆森，胡安·米格尔·皮诺，和埃曼纽尔·杜普。2021年。Voxpopuli：一个大型多语言语音语料库，用于表征学习、半监督学习和解释。在计算语言学协会和第11届国际自然语言处理联合会议第59届年会的会议论文，ACL/IJCNLP 2021，(卷1：长篇论文)，虚拟会议，2021年8月1-6日，页993-1003。计算语言学协会。启超·王，子喆·孟，文倩·崔，逸飞·张，鹏程·吴，炳哲·吴，子彬·郑，欧文·金，梁·陈，和佩林·赵。2025年。鹦鹉：与双通道大型语言模型无缝实现的语音对话交互。熊·王，阳泽·李，超游·傅，云航·沈，磊·谢，柯·李，兴·孙，和龙·马。2024年。Freeze-omni：一个智能低延迟的语音到语音对话模型，使用冻结的LLM。CoRR, abs/2411.00774。

吴博永、严超、胡晨、易晟、冯承立、田飞、沈飞宇、余刚、张昊阳、李经北、陈铭瑞、刘鹏、游旺、张向宇·托尼、李星源、杨学瑞、邓雅月、黄叶畅、李宇欣、张宇欣、游赵、李布莱恩、万长毅、胡汉鹏、甄江杰、陈思宇、袁松、张雪林、姜义敏、周钰、杨宇翔、李冰鑫、马步云、宋昌禾、庞冬青、胡国强、孙海洋、安康、王娜、高淑莉、季伟、李文、孙闻、闻轩、任勇、马元凯、陆宇凡、王斌、李博、苗长新、刘车、徐辰、施大鹏、胡丁元、吴东航、刘恩乐、黄观哲、阎古麟、张汉、郝聂、贾浩南、周洪宇、孙建建、吴峤仁、吴杰、杨杰、杨金、林俊哲、李凯翔、杨雷、施丽颖、周立英、谷龙龙、李明、李明亮、李明霄、伍楠、韩琪、谭勤源、庞少良、范盛杰、刘思琪、曹天诚、陆婉莹、何文清、谢务勋、赵煦、李学琪、俞彦博、杨洋、刘毅、陆逸凡、王宜雷、丁远豪、梁远纬、陆远纬、罗煜楚、尹宇禾、展宇萌、张宇翔。2025年。《Step-audio 2技术报告》。CoRR，abs/2507.16632。萧士涛、刘铮、张佩天、邢润。2024年。《Lm-cocktail：通过模型合并实现语言模型的弹性调优》。发表于《计算语言学协会发现》，ACL 2024，泰国曼谷及虚拟会议，2024年8月11-16日，第24742488页。计算语言学协会。谢志飞、吴昌桥。2024a。《Mini-omni：语言模型可以在流媒体中思考时听和说》。CoRR，abs/2408.16725。谢志飞、吴昌桥。2024b。《Mini-omni2：朝着开放源代码的gpt-4o，具备视觉、语音和双工能力》。CoRR，abs/2410.11190。徐瑾、郭治方、何金正、胡航瑞、何婷、柏帅、陈克勤、王嘉林、范扬、邓凯、张斌、王骄、楚云飞、林俊阳。2025年。《Qwen2.5-omni技术报告》。CoRR，abs/2503.20215。姚元、余天宇、张傲、王崇毅、崔军博、朱宏吉、蔡天驰、李昊宇、赵维林、何志辉等。2024年。《Minicpm-v：可以在你的手机上运行的gpt-4v级别的mllm》。arXiv预印本arXiv:2408.01800。曾熬寒、杜正晓、刘铭道、张雷、蒋胜民、董于霄、唐杰。2025年。《用合成交错数据扩展语音-文本预训练》。发表在《第十三届国际学习表示会议》，ICLR 2025，新加坡，2025年4月24-28日。OpenReview.net。张栋、李世敏、张鑫、詹俊、王鹏宇、周雅倩、邱希鹏。2023a。《Speechgpt：赋能大型语言模型以内在跨模态对话能力》。发表于《计算语言学协会发现：EMNLP 2023》，新加坡，2023年12月6-10日，第1575715773页。计算语言学协会。张青琳、程露瑶、邓崇、陈倩、王文、郑思琪、刘佳青、余海、谭朝宏。2024a。《Omniflatten：一个无缝语音对话的端到端GPT模型》。CoRR，abs/2410.17799。张鑫、吕翔、杜志浩、陈倩、张东、胡航瑞、谭朝宏、赵天宇、王宇轩、张斌、陆恒、周雅倩、邱希鹏。2024b。《Intrinsicvoice：赋能大型语言模型以内在实时语音交互能力》。CoRR，abs/2410.08035。

# 附录

# A 实现细节

Qwen2.5-0.5B 采用 $T 2 M$ 模式，并利用语音文本对齐数据初始化 SRH (SRH-PT)。最大序列长度设置为 2K 词元，对应约 6.8 分钟的音频时长。DRVoICE 以 Qwen2.5-7B-Instruct 作为基础大语言模型，而 DrVoICE-Small 则使用 Qwen2.5-1.5B-Instruct 作为基础大语言模型。分组因子设置为 $k = 5$，核心鸡尾酒插值因子设置为 $\alpha = 0$，以极大保持基础大语言模型的能力。多个损失超参数设置为 $\lambda = 1$ 和 $\mu = 1$。预热率设置为总训练步数的 $2\%$。对于 DrVoiCE 的两阶段训练，学习率在第一阶段从 $1 \times 1 0 ^ { - 4 }$ 降低至 $1 \times 1 0 ^ { - 5 }$，在第二阶段从 $2 \times 1 0 ^ { - 5 }$ 降低至 $2 \times 1 0 ^ { - 6 }$，两个阶段均采用余弦退火调度。相比之下，DrVoICE-Small 和 SRH-PT 在单阶段中训练，采用与 DrVoICE 第一阶段相同的学习率调度。优化使用 AdamW 方法（Loshchilov 和 Hutter, 2019）。实验在 $6 4 \mathrm { x }$ NVIDIA Tesla A800 80G GPU 上运行，使用 Brain 浮点格式（BF16）（Kalamkar 等，2019）以加速训练和解码。对于 DRVoICE，实施 DeepSpeed ZeRO-2（Rajbhandari 等，2020）以防止 GPU 内存不足。在 SRH-PT 上大约需时 20 小时，而 DRVoICE 后续训练大约需时 45 小时。

# B 提示模板

多模态交互模式的系统提示。如表5所示，系统提示根据模型输出的分区被分类为五种类型。在推理过程中，指定这些系统提示能够生成期望的输出结果。

Table 5: System Prompts for Multimodal Interaction.   

<table><tr><td colspan="2">Pattern Abbr. System Prompts</td></tr><tr><td>S2M &amp; T2M</td><td>You are a helpful assistant and asked to generate both text and speech tokens at the same time.</td></tr><tr><td>S2T &amp; T2T</td><td>You are a helpful assistant and asked to generate text tokens.</td></tr><tr><td>STC</td><td>You are a helpful assistant. Let&#x27;s think step by step. Convert speech to text if the query is speech, think of an appropriate text response, and then convert the response back to both text and speech tokens at the same time.</td></tr><tr><td>SAC</td><td>You are a helpful assistant. Let&#x27;s think step by step. Think of an appropriate text response, and then convert the response back to both text and speech tokens at the same time.</td></tr><tr><td>SUC</td><td>You are a helpful assistant. Let&#x27;s think step by step. Convert speech to text if the query is speech, and then think of both appropriate text and speech responses at the same time.</td></tr></table>

# C 更多消融实验与分析

核心鸡尾酒训练策略。表6展示了我们两阶段方法的精确权衡和收益。例如，在综合的Inhouse v2、MagpiePro和InfGen数据集设置下，模型在第一阶段后的平均表现下降到70.19，相较于仅文本基线（81.77）下降超过11分。这一观察结果完美契合我们最初的假设：尽管第一阶段的激进全微调以高学习率进行，旨在快速将参数移动到损失平面更有利的区域，但却暂时损害了模型的通用能力，如所预测的那样。这一性能下降恰恰是后续步骤旨在纠正的问题。在中间融合步骤之后——该步骤将经过激进训练的模型与基础LLM“调制”，以重新整合其强大的知识——第二阶段继续以较小的学习率进行精心微调。如表中所示，第二阶段成功恢复并精炼了模型，平均得分显著提升至74.73。第一阶段的这一显著改善表明，我们的策略有效缓解了初期高学习率训练所造成的不稳定性。这一两阶段过程证实了核心鸡尾酒策略为解决学习率困境提供了强有力的方案，使快速适应成为可能而不牺牲最终性能。

Table 6: Training Strategy Performance comparison on the Voicebench benchmark with different training datasets.   

<table><tr><td>Description</td><td></td><td>AlpacaEval CommonEval SD-QA</td><td></td><td>MMSU</td><td>OpenBookQA IFEval AdvBench</td><td></td><td></td><td>Avg</td></tr><tr><td>Qwen2.5-7B-Instruct (T2T)</td><td>4.67</td><td>4.34</td><td>76.13</td><td>69.97</td><td>82.20</td><td>65.45</td><td>99.04</td><td>81.86</td></tr><tr><td colspan="9">Inhouse v1</td></tr><tr><td>OnlyText-Training (T2T)</td><td>4.15</td><td>3.57</td><td>60.15</td><td>54.50</td><td>66.50</td><td>50.22</td><td>99.40</td><td>69.31</td></tr><tr><td>Stage 2 (S2T)</td><td>4.08</td><td>3.54</td><td>55.88</td><td>52.80</td><td>69.45</td><td>38.48</td><td>99.23</td><td>66.89</td></tr><tr><td colspan="9">Inhouse v2</td></tr><tr><td>OnlyText-Training (T2T)</td><td>4.22</td><td>3.60</td><td>68.35</td><td>58.10</td><td>68.13</td><td>65.69</td><td>99.62</td><td>73.76</td></tr><tr><td>Stage 2 (S2T)</td><td>4.10</td><td>3.15</td><td>64.01</td><td>58.30</td><td>79.56</td><td>55.25</td><td>98.27</td><td>71.48</td></tr><tr><td colspan="9">Inhouse v2, MagpiePro, InfGen</td></tr><tr><td>OnlyText-Training (T2T)</td><td>4.64</td><td>4.26</td><td>71.79</td><td>69.97</td><td>84.40</td><td>68.84</td><td>99.42</td><td>81.77</td></tr><tr><td>Stage 1 (S2T)</td><td>4.25</td><td>3.09</td><td>57.32</td><td>54.29</td><td>75.82</td><td>58.07</td><td>99.04</td><td>70.19</td></tr><tr><td>Stage 2 (S2T)</td><td>4.54</td><td>3.35</td><td>64.56</td><td>61.61</td><td>80.44</td><td>559.83</td><td>98.85</td><td>74.73</td></tr><tr><td>Stage 2 w/. ASR data (S2T)</td><td>4.52</td><td>3.77</td><td>68.54</td><td>60.31</td><td>79.56</td><td>59.30</td><td>98.65</td><td>76.02</td></tr></table>

![](images/2.jpg)  

Figure 2: Computational Resources under 17K hours training data across different Grouping Factor.

数据质量。表6中的OnlyText-Training (T2T)结果强有力地指示了我们数据集中文本内容蕴含的最大潜力。数据质量与最终模型性能之间存在明显的关联：随着T2T平均分数从Inhouse v1（69.31）提升至我们最全面的数据集（81.77），最终阶段2 (S2T) 模型的性能也显著提升。值得注意的是，我们最终的数据集组合使T2T模型达到了81.77的平均分数，几乎完美匹配文本专用的Qwen2.5-7B-Instruct主干网络。这验证了我们指导数据的卓越质量，并建立了高性能的上限。我们的最终S2T模型（74.73）成功将这一潜力的显著部分转化为多模态领域。此外，结果强调了数据组成对特定任务的重要性。CommonEval基准测试是一个典型案例，它评估模型在真实人类语音上的表现。我们的标准阶段2 (S2T) 模型在这一具有挑战性的基准上获得了3.35的分数。然而，通过用ASR派生对增强训练数据（Stage 2 w/. ASR data），CommonEval分数显著提升至3.77。这一提升是因为ASR数据让模型接触到真实语音中的自然流畅性、不同的语调和环境噪声，从而增强了其鲁棒性。

![](images/3.jpg)  

Figure 3: Performance Scaling of DrVoICE-Small (w/o. Continuous Speech Encoder) on LLaMA Question Benchmark.

Table 7: Impact of the Grouping Factor on Llama Q. S2T and S2M performance, using DRVoICESmall (1.5B) w/o CSE and data subsets for faster experiments. The S2T model is trained exclusively on mixed S2T and T2T data, while the S2M model is trained on mixed S2M and T2T data.

<table><tr><td>Grouping Factor</td><td>S2T</td><td>S2M (T/S)</td></tr><tr><td>1</td><td>55.67</td><td>4.00 / 2.67</td></tr><tr><td>3</td><td>64.67</td><td>15.67 / 5.00</td></tr><tr><td>5</td><td>63.33</td><td>37.67 / 28.00</td></tr><tr><td>7</td><td>62.67</td><td>36.00 / 16.67</td></tr></table>

数据缩放。为了研究数据缩放的影响，我们通过逐步扩大 DRVoICE-Small（不含连续语音编码器）上的 S2S 训练数据进行实验。每个 S2S 实例可以扩展为 7 种不同的多模态交互模式。如图 3 所示，将数据集从 3.6K 扩展到 17.8K 样本带来了持续的改善。T2M(S) 的性能几乎呈线性增长，而其他模式尽管增长速度较慢，但在图中仍显示出上升趋势，表明通过额外数据可能进一步提升性能。分组因子。表 7 和图 2 显示我们的分组策略显著提升了性能和计算效率。与生成性能下降相反，分组显著改善了语音理解（S2T）和语音到语音生成（S2M）。例如，将分组因子从 1 提高到 5，S2T 得分从 55.67 提升至 63.33（相对增益为 $1 3 . 7 \%$）。S2M (T/S) 得分也显著提高，在分组因子为 5 时峰值达到 37.67 $/ 2 8 . 0 0$。此外，如图 2 所示，使用分组因子 5 而不是 1 能将每个设置的 GPU 小时减少近 $50 \%$，证明了分组机制的效率。

保留与遗忘。努力选择适合语音合成的开源数据集。所有结果均来自于在低学习率（1e-5）下对Qwen2.5-7B-Instruct的微调，因此主要反映保留与遗忘之间的关系，而不是绝对能力的提升。与基线的差距量化了遗忘程度：magpie_pro_llama3_3_500k_fltered和InfGen表现出最小的总体差异（在两个数据集上约为34分），而像Inf7M这样大型且噪声较多的语料库尽管体量庞大，却会导致显著的遗忘。质量和过滤比规模更为重要：openorca_gpt4在词元更少的情况下忘记的内容少于openorca_gpt3_5，且magpie_pro系列在每个词元上保持了最佳的遵循指令和事实问答能力。遗忘是有选择性的：一些子指标有所提升（例如，LlamaQ在Inf7M/tulu_3_sft中上升；AdvBench在tulu_3_sft上达到100），但这些提升通常会与指令和问答得分相抵消。总体而言，紧凑、经过良好过滤、模型邻近的数据在1e-5的情况下最小化了破坏性漂移，而大规模的词元数量则倾向于加剧遗忘，因此最稳定的方案在保持基础模型的同时进行有针对性的调整。

Table 8: Benchmark Results: VoiceBench (T2T)   

<table><tr><td rowspan="2">Data</td><td colspan="8">VoiceBench</td></tr><tr><td>AlpacaEval</td><td>CommonEval</td><td>SD-QA</td><td>MMSU</td><td>OBQA</td><td>IFEval</td><td>AdvBench</td><td>Overall</td></tr><tr><td>Qwen2.5-7B-Instruct</td><td>4.67</td><td>4.34</td><td>76.13</td><td>69.97</td><td>82.20</td><td>65.45</td><td>99.04</td><td>81.86</td></tr><tr><td>Inf7M</td><td>3.83</td><td>3.50</td><td>69.98</td><td>55.43</td><td>71.87</td><td>48.79</td><td>94.42</td><td>69.58</td></tr><tr><td>InfGen</td><td>4.51</td><td>4.13</td><td>74.14</td><td>62.46</td><td>76.04</td><td>58.35</td><td>98.46</td><td>77.46</td></tr><tr><td>openorca_gpt4</td><td>4.33</td><td>4.03</td><td>73.96</td><td>61.00</td><td>73.85</td><td>54.05</td><td>96.35</td><td>75.20</td></tr><tr><td>openorca_gpt3_5</td><td>4.35</td><td>4.17</td><td>71.07</td><td>57.84</td><td>66.59</td><td>49.39</td><td>95.00</td><td>72.90</td></tr><tr><td>Evol_Instruct_Code</td><td>4.19</td><td>4.06</td><td>69.26</td><td>63.14</td><td>81.54</td><td>56.40</td><td>90.38</td><td>75.10</td></tr><tr><td>Evol_Instruct</td><td>4.09</td><td>3.91</td><td>71.61</td><td>60.38</td><td>72.31</td><td>54.02</td><td>98.27</td><td>73.80</td></tr><tr><td>magpie_pro_llama3_1_300k</td><td>4.42</td><td>4.08</td><td>72.69</td><td>58.33</td><td>74.95</td><td>56.08</td><td>86.73</td><td>74.11</td></tr><tr><td>magpie_pro_mt_llama3_1_300k</td><td>4.46</td><td>4.05</td><td>73.60</td><td>58.07</td><td>60.00</td><td>60.05</td><td>86.35</td><td>72.61</td></tr><tr><td>numinamath_cot</td><td>4.11</td><td>3.78</td><td>70.16</td><td>55.43</td><td>68.35</td><td>52.00</td><td>97.69</td><td>71.63</td></tr><tr><td>OpenHermes_v2_5</td><td>4.05</td><td>3.71</td><td>71.61</td><td>62.20</td><td>74.07</td><td>51.59</td><td>82.88</td><td>71.08</td></tr><tr><td>Synthia_v1_3</td><td>4.30</td><td>3.96</td><td>72.33</td><td>63.63</td><td>76.70</td><td>53.32</td><td>97.69</td><td>75.55</td></tr><tr><td>tulu_3_sft</td><td>4.04</td><td>3.73</td><td>65.64</td><td>61.74</td><td>79.78</td><td>61.78</td><td>100.00</td><td>74.91</td></tr><tr><td>magpie_pro_llama3_3_500k_filtered</td><td>4.48</td><td>4.17</td><td>73.42</td><td>64.31</td><td>81.32</td><td>64.81</td><td>92.88</td><td>78.53</td></tr></table>

Table 9: Benchmark Results: UltraEval-Audio (T2T) & OpenAudioBench (T2T)   

<table><tr><td rowspan="2">Data</td><td rowspan="2">Tokens (M)</td><td colspan="5">UltraEval-Audio</td><td>OpenAudioBench</td><td rowspan="2">Total Avg</td></tr><tr><td>AlpacaEval</td><td>LlamaQ</td><td>TriviaQA</td><td>WebQ</td><td>Overall</td><td>Reasoning QA</td></tr><tr><td>Qwen2.5-7B-Instruct</td><td></td><td>81.41</td><td>82.33</td><td>53.91</td><td>52.56</td><td>67.55</td><td>58.00</td><td>75.10</td></tr><tr><td>Inf7M</td><td>2406</td><td>55.35</td><td>84.67</td><td>46.29</td><td>47.59</td><td>58.48</td><td>48.51</td><td>64.13</td></tr><tr><td>InfGen</td><td>1022.4</td><td>74.29</td><td>82.33</td><td>55.96</td><td>51.18</td><td>65.94</td><td>57.43</td><td>71.95</td></tr><tr><td>openorca_gpt4</td><td>361.8</td><td>66.21</td><td>83.00</td><td>54.39</td><td>49.56</td><td>63.29</td><td>54.46</td><td>69.50</td></tr><tr><td>openorca_gpt3_5</td><td>1101</td><td>55.91</td><td>83.00</td><td>54.69</td><td>48.47</td><td>60.52</td><td>48.51</td><td>66.74</td></tr><tr><td>Evol_Instruct_Code</td><td>28.3</td><td>52.32</td><td>81.33</td><td>52.64</td><td>44.24</td><td>57.63</td><td>52.48</td><td>67.39</td></tr><tr><td>Evol_Instruct</td><td>69.2</td><td>56.62</td><td>81.67</td><td>53.81</td><td>47.15</td><td>59.81</td><td>53.47</td><td>67.44</td></tr><tr><td>magpie_pro_llama3_1_300k</td><td>213.9</td><td>76.16</td><td>83.67</td><td>57.91</td><td>55.02</td><td>68.19</td><td>53.96</td><td>70.46</td></tr><tr><td>magpie_pro_mt_llama3_1_300k</td><td>323.2</td><td>75.35</td><td>83.00</td><td>54.00</td><td>54.53</td><td>66.72</td><td>57.43</td><td>69.38</td></tr><tr><td>numinamath_cot</td><td>463.2</td><td>64.04</td><td>80.67</td><td>54.88</td><td>50.64</td><td>62.56</td><td>50.00</td><td>66.81</td></tr><tr><td>OpenHermes_v2_5</td><td>375.9</td><td>58.54</td><td>83.67</td><td>54.49</td><td>49.85</td><td>61.64</td><td>54.46</td><td>66.55</td></tr><tr><td>Synthia_v1_3</td><td>59</td><td>61.67</td><td>83.67</td><td>53.81</td><td>47.88</td><td>61.76</td><td>55.45</td><td>69.28</td></tr><tr><td>tulu_3_sft</td><td>570.2</td><td>55.91</td><td>84.33</td><td>53.52</td><td>49.41</td><td>60.79</td><td>55.94</td><td>68.62</td></tr><tr><td>magpie_pro_llama3_3_500k_filtered</td><td>314.6</td><td>78.64</td><td>79.00</td><td>57.32</td><td>54.18</td><td>67.29</td><td>51.49</td><td>72.53</td></tr></table>

# D 限制与未来工作

我们的未来工作将解决本研究的局限性，并推动三个关键领域的发展：（1）提升语音生成质量：与 Qwen2.5-Omni 相比，当前模型在语音到语音生成中的文本与语音之间的对齐表现较弱。这一性能差距可以归因于最终语音生成阶段中的一个关键架构差异。Qwen2.5-Omni 方法直接将文本输入其讲者模块，以确保高文本保真度。相对而言，DRVoICE 仅将隐藏状态输入其语音精化头（SRH）。因此，未来工作的一条有前景的方向是将文本作为额外输入纳入 SRH。假设这一修改将提供更明确的文本指导，进一步增强语音与文本的对齐，并显著降低 ASR-WER。（2）实现全双工交互：一个重要方向是实现全双工交互，以便进行更自然的对话。受到最近进展（如 Parrot（王等，2025））的启发，未来的工作将研究使用时间分割复用（TDM）输入流。这将允许 DRVoICE 在自己的语音生成阶段接受用户语音输入，从而有效利用空闲时间段，实现响应性和可打断的对话。（3）扩展到通用音频和多模态：最后，一个有前景的途径是将模型的能力扩展到更广泛的音频理解和合成任务，包括音乐和环境声音的识别与生成。接下来整合视觉模态将成为开发更全面的多模态对话 AI 的关键一步。