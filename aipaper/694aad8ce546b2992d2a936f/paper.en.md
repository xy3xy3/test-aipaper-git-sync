# DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations

Chao-Hong Tan, Qian Chen, Wen Wang, Chong Deng, Qinglin Zhang,   
Luyao Cheng, Hai Yu, Xin Zhang, Xiang Lv, Tianyu Zhao, Chong Zhang,   
Yukun Ma, Yafeng Chen, Hui Wang, Jiaqing Liu, Xiangang Li Jieping Ye Tongyi Lab, Alibaba Group {tanchaohong.ch, tanqing.cq, w.wang}@alibaba-inc.com

# Abstract

Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speechtext tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speechtext voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly $1 2 . 5 \mathrm { H z }$ input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to $\mathbf { 5 H z }$ significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DRVoICE-7B establishes new state-of-the-art (SOTA) on OpenAudioBench and Big Bench Audio benchmarks, while achieving performance comparable to the SOTA on VoiceBench and UltraEval-Audio benchmarks, making it a leading open-source speech foundation model in ${ \sim } 7 \mathrm { B }$ models.

# 1 Introduction

Developments in spoken dialogue systems are critical to human-computer interaction, as natural human communication inherently relies on verbal exchanges. Recently, Large Language Model (LLM) based spoken dialogue systems, exemplified by systems like GPT-4o (OpenAI, 2024b), demonstrate great potential for seamless and natural interactions with users. LLM-based spoken dialogue systems can be generally categorized into cascaded systems and end-to-end (E2E) systems, with the distinction lying in whether the backbone LLM can directly comprehend speech representations and generate speech outputs. Early cascaded systems integrate separately trained Automatic Speech Recognition (ASR), LLM, and Text-to-Speech (TTS) modules. Such systems inherently suffer from error accumulation, loss of acoustic details (e.g., emotion), and significant latency. Alternatively, the ASR module could be eliminated by introducing audio understanding foundation models (Chu et al., 2023, 2024). E2E systems have emerged to further eliminate ASR and TTS modules, and establish direct connections between speech representations and LLMs. However, training LLMs to generate highly intelligent speech outputs remains challenging. E2E systems struggle with the quality of speech token generation, primarily due to inefficient utilization of textual information during speech token generation.

Recent works on E2E models to address these challenges have focused on two primary directions (Chen et al., 2024a): Text-Driven Speech Models (Yao et al., 2024; Li et al., 2025) and Joint Speech-Text Models. Text-Driven Speech Models refer to systems in which LLMs process speech representations as input, produce textual responses, and utilize the LLM's representations as input to a speech decoder for speech generation. In contrast, Joint Speech-Text Models involve LLMs taking speech representations as input and generating both text tokens and speech tokens simultaneously. The key distinction lies in whether speech token generation can influence the subsequent generation of text tokens within the LLM: this feedback loop of speech tokens to LLM is present in Joint models but absent in Text-Driven models. Joint Speech-Text Models can be further categorized into interleaved speech-text modeling (Zeng et al., 2025; Zhang et al., 2024a) and parallel speech-text modeling (Défossez et al., 2024; Chen et al., 2024a; KimiTeam et al., 2025). Interleaved speech-text modeling alternates speech and text representations as inputs to the LLM, while parallel speech-text modeling fuses speech and text representations before feeding them into the LLM.

While both Text-Driven Speech Models and Joint Speech-Text Models explicitly incorporate textual guidance into speech token generation to leverage the LLM's capabilities, they have distinct limitations. The architecture of Text-Driven Speech Models creates a unidirectional information flow where text generation is completed before speech synthesis begins. This prevents the LLM from conditioning its textual output on the generated speech tokens, thus limiting its ability to explore fine-grained paralinguistic atributes like emotion and prosody. On the other hand, Joint Speech-Text Models degrade the original text generation capabilities due to speech token interference, making the preservation of text capabilities a critical challenge. Nevertheless, Joint Speech-Text Models enforce multimodal interaction and generation and empower greater potentials (Chen et al., 2024a); hence, in this work, we focus on enhancing joint speech-text modeling. Recently, Kimi-Audio (KimiTeam et al., 2025) sets a new state-of-the-art (SOTA) in joint speech-text modeling. However, this approach still suffers from notable limitations. It not only requires extensive training data but also incurs significant computational costs due to its $1 2 . 5 \mathrm { H z }$ audio representation. Furthermore, the high token rate creates a frequency mismatch with the much lower rate of text tokens $( \sim 3 \mathrm { { H z } ) }$ (Chen et al., 2024a), which can dilute semantic information and consequently hinder the full utilization of the LLM's core capabilities. In this work, we propose DrVoiCE, a novel parallel speech-text voice conversation model with Dual-Resolution Speech Representations (DRSR). For speech comprehension, we introduce a grouping mechanism that maps $2 5 \mathrm { H z }$ discrete audio tokens to $5 \mathrm { H z }$ speech representations, effectively alleviating the temporal resolution discrepancy between speech and text tokens. During generation, combined speech-text embeddings form the assistant's autoregressive input. The hidden states captured from shared LLM layer are then passed in parallel to a Text Head for text token prediction and a Speech Refined Head (SRH) to generate the corresponding ungrouped speech tokens. To further enhance the model's reasoning and coherence of its output, we incorporate a Chain-ofModality (CoM) (Zhang et al., 2023a) strategy. CoM prompts the model to first generate a complete response in text, allowing it to structure its thoughts beforeengaging in parallel speech-text generation. This intermediate reasoning step improves modality alignment and reduces errors. We design system prompts to control the output modes, enabling text-only output, direct parallel speech-text output, or the CoM-enhanced parallel output. We also develop a Core-Cocktail training strategy to refine model optimization and LLM knowledge retention.

Our contributions are three-fold: 1) We propose DrVoICE, a novel parallel speech-text conversation model featuring Dual-Resolution Speech Representations (DRSR). This core architectural innovation effectively alleviates the temporal resolution mismatch between speech and text tokens, reducing computational costs and better preserving the LLM's semantic processing capabilities. 2) We introduce two new training strategy, including a CoM-Mixing training strategy acting as a curriculum, using the structured CoM reasoning to scaffold speech generation, and a Core-Cocktail training strategy for retaining the knowledge of LLMs. 3) Our comprehensive experimental results reveal that DrVoICE-7B achieves new SOTA performance on prominent benchmarks like OpenAudioBench (audio understanding) and Big Bench Audio (reasoning and understanding capabilities of audio-processing models), alongside performance on par with the SOTA on other benchmarks such as VoiceBench (benchmarking LLM-based voice assistants) and UltraEval-Audio (both speech understanding and generation), solidifying its position as a premier open-source speech foundation model among ${ \sim } 7 \mathrm { B }$ models.

# 2 Related Work

Speech Tokenization. Two primary directions exist for converting continuous speech signal into processable sequences: continuous representations, e.g., Whisper (Radford et al., 2022), and discrete representations. Since LLMs generate discrete tokens, continuous representations face challenges in direct integration with LLMs for speech generation. In contrast, discrete tokens enable LLMs to handle speech similar to text tokens, and are typically categorized into two categories: 1) acoustic tokens optimized for reconstructing high-quality audio through neural audio codecs (Défossez et al., 2023), and 2) semantic tokens typically derived from self-supervised pre-trained models with masked language modeling as training objective (Hassid et al., 2023) or supervised learning as S3Tokenizer using in CosyVoice (Du et al., 2024a,b, 2025), and prioritized for linguistic content (Hsu et al., 2021). While acoustic tokens achieve superior acoustic fidelity in sound reconstruction, semantic tokens demonstrate stronger alignment with semantic representations (Zhang et al., 2023b), thereby facilitating more effective extension of MLLMs' capabilities in both speech comprehension and generation (Zeng et al., 2025; Zhang et al., 2024b, 2023a; Borsos et al., 2023). In this work, we use S3Tokenizer as the speech tokenizer due to its solid semantic capabilities and compatibility with the strong Speech Detokenizer from CosyVoice for synthesis. S3Tokenizer is a supervised semantic speech tokenizer based on the pre-trained SenseVoice-Large model (An et al., 2024) that enhances the semantic relationship of extracted tokens. S3Tokenizer is robust to data noise, and reduces the reliance on clean data. E2E Speech Foundation Models. Text-Driven Speech Models (Fang et al., 2024; Wang et al., 2024; Fu et al., 2025; Ya0 et al., 2024; Huang et al., 2025; Chen et al., 2025) integrate speech encoder, adapter, LLM, and a streaming speech decoder, and can simultaneously generate text and speech. Qwen2.5-Omni (Xu et al., 2025) employs the Thinker-Talker architecture, with Thinker handling multimodal understanding and text generation and Talker handling speech token production. Since Thinker cannot receive speech tokens during generation, the framework inherently limits awareness of speech token generation states, constraining applications such as full-duplex voice conversation.

Joint Speech-Text Models explore two different architectures, Interleaved and Parallel. Interleaved models (Zhang et al., 2024a; Zeng et al., 2025; Li et al., 2025; Long et al., 2025; Wu et al., 2025) adopt interleaved decoding to support simultaneous generation of speech and text tokens, while parallel models (Défossez et al., 2024; Xie and Wu, 2024a,b; Chen et al., 2024a; KimiTeam et al., 2025) conduct parallel decoding. The parallel model Moshi (Défossez et al., 2024) employs a compact Deep Transformer to predict $k$ tokens while relying solely on current-step representations. To mitigate limitations of discrete tokens in audio understanding, Kimi-Audio (KimiTeam et al., 2025) introduces dual-tokenizer combining discrete semantic tokens with continuous Whisper (Radford et al., 2023) features, preserving both semantic and acoustic information. In order to further enhance the efficiency of both training and inference and alleviate the issue of granularity misalignment between the text and speech modalities, our approach leverages DRSR to reduce the LLM input frame rate to $5 \mathrm { H z }$ without sacrificing performance.

# 3 Methodology

Figure 1 illustrates the architecture of DrVoICE. The system consists of three main components: (1) Speech Encoder and Speech Tokenizer process the speech wave into hidden representations for the user end and the assistant end respectively, (2) a Multimodal Large Language Model (MLLM) consists of shared LLM layer, a Text Head, and a Speech Refined Head (SRH) for token generation, and (3) Speech Detokenizer to generate wave from speech tokens. The system operates through multimodal input encoding and coordinated speech-text output generation. During inference, user inputs (text or speech) are first mapped to a unified semantic space, processed by MLLM to produce parallel speech-text responses through SRH and text head. To effectively train this system, we introduce the CoM-Mixing Training and Core-Cocktail Training strategies.

# 3.1 Speech Tokenization and Detokenization

Aiming for enhanced audio understanding, we utilize Whisper-Large-v3 Speech Encoder to extract continuous audio representations at the user end. Subsequently, an Adapter is introduced to downsample the temporal resolution of these representations and align their hidden dimension with that of the LLM. Semantic tokens have been widely used for speech tokenization (Zhang et al., $2 0 2 3 \mathrm { a }$ ;Borsos et al., 2023), due to their strong alignment with text (Zhang et al., 2023b); therefore, we employ S3Tokenizer (Du et al., 2024a,b, 2025) as the Speech Tokenizer to convert speech waveform to semantic speech token sequence $\mathbf { S } = [ s _ { 0 } , s _ { 1 } , \cdots , s _ { T - 1 } ]$ at the assistant end, where $T$ is the speech token sequence length. For speech detokenization, conditioned on speaker embeddings capturing acoustic details such as timbre, the Flow Matching model (Lipman et al., 2023) converts speech tokens S into the Mel spectrum for a given speaker. Finally, a pre-trained vocoder HiFi-GAN (Kong et al., 2020) transforms the Mel spectrum back into audio signal.

![](images/1.jpg)  

Figure 1: Overview of DrVoICE. User speech inputs are tokenized, grouped, and encoded by the MLLM for autoregressive text and speech token prediction. The MLLM consists of Shared LLM Layer, a Text Head, and a Speech Refined Head (SRH) for token generation. The generated speech tokens are then converted to speech waveform by the speech detokenizer. Note that SRH generates $k$ speech tokens through $k$ autoregressive forward passes, where $k$ is the grouping factor.

# 3.2 Multimodal Large Language Model (MLLM)

Built upon text-LLMs, the MLLM learns joint speech-text modeling to process speech or text inputs while producing parallel speech and text outputs. Parallel Joint Speech-Text Model. Inspired by Moshi (Défossez et al., 2024), explicit text streaming is incorporated to aid speech generation as a common semantic scaffold. We focus on performing modality alignment exclusively at the assistant end. This design adheres to the asymmetric nature of human-machine interactions: while user inputs are typically unimodal (either text or speech), the assistant's responses can be a coordinated multimodal output. Leveraging the autoregressive generation capability of LLMs, both generated speech tokens $s _ { t }$ and text tokens $t _ { t }$ are iteratively fed back into the shared LLM layer at each timestep. Their embeddings are added as model input, forming a parallel speech-text architecture. Formally, the combined input embedding $c _ { t }$ at timestep $t$ is computed as:

$$
c _ { t } = E _ { \mathrm { s p e e c h } } ( s _ { t } ) + E _ { \mathrm { t e x t } } ( t _ { t } )
$$

where $E _ { \mathrm { s p e e c h } }$ and $E _ { \mathrm { t e x t } }$ denote the embeddings of speech and text tokens, respectively. Since the lengths of speech tokens and text tokens are typically mismatched, the shorter sequence is padded with a special token <ISILl>to align them for each utterance. The autoregressive generation process is as follows:

$$
P ( y _ { t } | y _ { < t } , x ) = \prod _ { i = 1 } ^ { t } P ( y _ { i } | y _ { < i } , x )
$$

where $x$ is the input sequence and $y _ { t } = ( s _ { t } , t _ { t } )$ denotes the joint speech-text output at timestep $t$ This unified formulation enables seamless integration of speech and text generation within a single autoregressive framework. To preserve the intrinsic linguistic understanding and generation capabilities of pretrained text-LLMs while enabling cross-modal interactions, three key methodological designs are introduced for intermodal coordination, including Speech Token Grouping and Ungrouping, and Speech Refined Head.

Speech Token Grouping. A crucial parameter for discrete tokens is the sampling rate, which determines the input/output sequence length. GLM-4-Voice (Zeng et al., 2025) investigates sampling rates from $6 . 2 5 \mathrm { H z }$ to $5 0 \mathrm { H z }$ on LibriSpeech, revealing minimal differences in Word Error Rate (WER) between $5 0 \mathrm { H z }$ and $2 5 \mathrm { H z }$ but significant degradation at $1 2 . 5 \mathrm { H z }$ and $6 . 2 5 \mathrm { H z }$ Hence, our work adopts $2 5 \mathrm { H z }$ sampling rate. To resolve the temporal resolution mismatch between speech signals (25Hz) and text tokenization $( \sim 3 \mathrm { { H z } ) }$ (Chen et al., 2024a), a grouping mechanism is designed:

$$
\mathbf { g } _ { i } = \mathrm { L i n e a r } \left( \begin{array} { c } { ( i + 1 ) k - 1 } \\ { \underset { j = i k } { \parallel } } \end{array} \mathbf { s } _ { j } \right) \in \mathbb { R } ^ { d _ { \mathrm { t e x t } } }
$$

where ${ \bf s } _ { j }$ denotes speech tokens, $\parallel$ represents feature concatenation, and $k$ is the grouping factor determined by the ratio between speech token rates and text token rates. This design compresses the speech token length from $T$ to $T / k$ and the resulting grouped speech representations align better with text. Notably, different from Chen et al. (2024a) that employs linear projection of audio logits into group-sized representations for parallel multi-token prediction, DRVoICE strategically designs an ungrouping mechanism and incorporates a dedicated Speech Refined Head (SRH) to enable autoregressive generation of individual speech tokens.

Speech Refined Head (SRH). SRH is proposed to enhance speech generation capabilities. It utilizes the last hidden state of the shared LLM layer (SLLM) as conditional input and incorporates contextual speech information to autoregressively generate speech tokens. While conventional speech grouping strategieswhich cluster speech tokens into semantically meaningful unitshave proven effective for speech recognition and understanding tasks (Chen et al., 2024a; Zhang et al., 2024b), our experiments reveal their inherent limitations in generative scenarios, since speech token grouping inevitably loses some fine-grained acoustic details. To address this limitation, DrVoICE performs an ungrouping process as follows: the SLLM's final hidden state is mapped to group-sized embeddings via linear projection and time splitting where $\mathbf { h } _ { u g } ^ { ( i ) } \in \mathbb { R } ^ { d _ { u g } / k }$ The resulting $\mathbf { H }$ generates speech tokens. Following our practice on SLLM, representations of preceding speech tokens and $\mathbf { H }$ are aggregated. Speech token prediction is trained to maximize the conditional probability:

$$
\mathbf { h } _ { u g } = \mathbf { W } _ { p } \mathbf { h } _ { L } ^ { \mathrm { [ S L L M ] } } \quad \mathrm { w h e r e } \quad \mathbf { W } _ { p } \in \mathbb { R } ^ { d _ { g } \times d _ { h } } ,
$$

$$
\mathbf { H } = \mathrm { S p l i t } _ { k } ( \mathbf { h } _ { u g } ) = [ \mathbf { h } _ { u g } ^ { ( 1 ) } , \mathbf { h } _ { u g } ^ { ( 2 ) } , \ldots , \mathbf { h } _ { u g } ^ { ( k ) } ] ,
$$

$$
\mathcal { L } _ { \mathrm { S R H } } = - \sum _ { i = 1 } ^ { T } \log P ( s _ { i } | s _ { < i } , \mathbf { H } _ { < i } ) ,
$$

where $s _ { i }$ represents the $i$ -th speech token. By optimizing this objective, SRH learns to predict subsequent speech tokens conditioned on both preceding speech tokens and the rich contextual embeddings $\mathbf { H }$ derived from SLLM. This design enables SRH to effectively leverage the semantic and acoustic information encoded in the hidden representations of SLLM, thereby producing more natural and coherent speech outputs compared to conventional grouping-based approaches. The E2E training objective $\mathcal { L } _ { \mathrm { M L L M } }$ integrates the two losses through multi-task learning:

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { M L L M } } = \lambda \mathcal { L } _ { \mathrm { T H } } + \mu \mathcal { L } _ { \mathrm { S R H } } , } \end{array}
$$

$$
\mathcal { L } _ { \mathrm { T H } } = - \sum _ { i = 1 } ^ { T } \log P ( t _ { i } | c _ { < i } , \mathbf { g } ) ,
$$

where ${ \mathcal { L } } _ { \mathrm { T H } }$ is the autoregressive loss over the text head, $\lambda$ and $\mu$ are hyperparameters.

Table 1: Multimodal Interaction Patterns.   

<table><tr><td>Pattern Name</td><td>Abbr.</td><td>Modality Flow</td></tr><tr><td>Speech-to-Multimodal</td><td>S2M</td><td>Speech → Joint speech-text response</td></tr><tr><td>Speech-to-Text</td><td>S2T</td><td>Speech → Text-only response</td></tr><tr><td>Text-to-Multimodal</td><td>T2M</td><td>Text → Joint speech-text response</td></tr><tr><td>Text-to-Text</td><td>T2T</td><td>Text → Text-only response</td></tr><tr><td>Speech-Text Chain</td><td>STC</td><td>Speech → Text transcription → Text response → Multimodal response</td></tr><tr><td>Speech-Assisted Chain</td><td>SAC</td><td>Speech → Text response (agent perspective) → Multimodal response</td></tr><tr><td>Speech-User Chain</td><td>SUC</td><td>Speech → Text transcription (user perspective) → Multimodal response</td></tr></table>

# 3.3 Training Strategy

Initialization. The Speech Encoder is initialized with the weights of Whisper-Large-v3, while the Shared LLM Layer is initialized using Qwen2.5. Additionally, the pre-trained Speech Tokenizer and Detokenizer from CosyVoice are employed and kept frozen throughout the entire training process. We initialize SRH with a pre-trained TTS model.

CoM-Mixing Training. The chain-of-modality (CoM) methodology (Zhang et al., 2023a) can enhance performance by leveraging intermediate textual transcriptions, which is particularly suitable for scenarios where real-time processing is not critical. Furthermore, practical applications involve scenarios where both speech and text output or text-only output are required, necessitating the system to dynamically generate different modalities based on specific needs. Seven interaction patterns for diverse input-output requirements are summarized in Table 1. System prompts are employed to regulate the model's output behavior. For example, prompts such as "[System] You are a helpful assistant and asked to generate both text and speech tokens at the same time." explicitly guide the model to produce multimodal outputs. Detailed prompts are shown in Appendix B. During inference, specifying these system prompts enables the generation of desired output results. For CoM-Mixing training, we construct data variants following the seven interaction patterns and obtain a mixture of data for training the model. During inference, user can manually prepend the appropriate system prompt to the input sequence to meet the output requirement. This flexibility ensures adaptability to varying modality generation demands.

Core-Cocktail Training. We identify a learning rate dilemma: employing a high learning rate significantly compromises the performance of the MLLM, whereas using a low learning rate leads to training stagnation, with the loss decreasing quite slowly. To overcome this optimization challenge, we develop a specialized two-stage training strategy, termed "Core-Cocktail". The first stage involves full fine-tuning the entire MLLM with a relatively high learning rate. The goal of this phase is to rapidly move the model's parameters into a more favorable region of the loss landscape. However, to counteract the potential performance degradation of the MLLM caused by this aggressive initial training, an intermediate merging step is introduced. Drawing inspiration from Xiao et al. (2024), the parameters of the MLLM trained in the first stage $( M _ { 1 } )$ are merged with the parameters of the base, pre-trained LLM $( M _ { 0 } )$ . This merging creates a new, interpolated model, $M _ { r }$ , as defined by the following equation:

$$
M _ { r } \gets \alpha M _ { 1 } + ( 1 - \alpha ) M _ { 0 }
$$

where $M _ { r }$ denotes the merged model and $\alpha$ is the interpolation weight. The merging step effectively re-integrates the robust knowledge of the base LLM. A smaller $\alpha$ corresponds to a greater preservation of the base LLM's capabilities. The second stage conducts full fine-tuning on the merged model $M _ { r }$ using a small learning rate. The core-cocktail training approach allows for careful and precise optimization, refining the model's performance without instability from a high learning rate.

# 4 Experiments

# 4.1 Experimental Setup

Datasets. Approximately 100K hours of audio-text paired data for speech-text modality alignment is adopted for pre-training Speech Refined Head. For DRVoICE post-training, we first synthesize speech for about 3B text tokens using CosyVoice (Du et al., 2024a,b, 2025), then select about 26K hours for speech-to-speech conversation and about 20K hours user speech plus 1.3B assistant tokens for speechto-text conversation, based on Word Error Rate (WER) of the synthesized speech. Furthermore, to enhance the model's comprehension of real-world speech, the training data was augmented with about 10K hours of English Automatic Speech Recognition (ASR) data, comprising several corpora, including Common Voice (Ardila et al., 2020), MELD (Poria et al., 2019), LibriSpeech (Panayotov et al., 2015), SPGISpeech (O'Neill et al., 2021), and Voxpopuli (Wang et al., 2021). Following prior works (Yao et al., 2024; KimiTeam et al., 2025; OpenAI, 2024b), we evaluate the performance on the widely used benchmarks, VoiceBench (Chen et al., 2024b) and OpenAudioBench 1 for Speech-To

Table 2: Performance Comparison on various benchmarks in terms of benchmark-specific metrics (the best result in each row is in bold). With the exception of GLM4-Voice, whose results are cited from Xu et al. (2025) and Chen et al. (2024b), all other results were generated by running inference on the released checkpoints. $\mathbf { F R ( I n / O u t ) }$ denotes the input speech frame rate and the output speech plus text frame rate for the LLM backbone. $\tau$ denotes the average number of text tokens corresponding to one second of speech.   

<table><tr><td></td><td>GLM4-Voice</td><td>MiniCPM -0 2.6</td><td>Baichuan -Omni-1.5</td><td>Qwen2.5 -Omni</td><td>Kimi -Audio</td><td>Step-Audio2 -Mini</td><td>DrVoice</td></tr><tr><td>FR (In/Out)</td><td>12.5/12.5+τ</td><td>25/T</td><td>12.5/12.5+7</td><td>25/T</td><td>12.5/12.5</td><td>12.5/25+T</td><td>5/5</td></tr><tr><td colspan="8">OpenAudioBench (S2T)</td></tr><tr><td>AlpacaEval</td><td>57.89</td><td>64.10</td><td>77.90</td><td>72.76</td><td>75.73</td><td>59.60</td><td>78.34</td></tr><tr><td>Llama Q.</td><td>76.00</td><td>78.00</td><td>78.50</td><td>75.33</td><td>79.33</td><td>75.00</td><td>80.33</td></tr><tr><td>Reasoning QA</td><td>47.43</td><td>38.60</td><td>50.00</td><td>63.76</td><td>58.02</td><td>46.04</td><td>57.92</td></tr><tr><td>TriviaQA</td><td>51.80</td><td>63.00</td><td>57.20</td><td>57.06</td><td>62.10</td><td>57.70</td><td>61.50</td></tr><tr><td>Web Q.</td><td>55.40</td><td>69.20</td><td>59.10</td><td>62.80</td><td>70.20</td><td>65.10</td><td>68.10</td></tr><tr><td>Overall</td><td>57.70</td><td>62.58</td><td>64.54</td><td>66.34</td><td>69.08</td><td>60.69</td><td>69.24</td></tr><tr><td colspan="8">VoiceBench (S2T)</td></tr><tr><td>AlpacaEval</td><td>3.97</td><td>4.42</td><td>4.50</td><td>4.33</td><td>4.46</td><td>4.17</td><td>4.52</td></tr><tr><td>CommonEval</td><td>3.42</td><td>4.15</td><td>4.05</td><td>3.84</td><td>3.97</td><td>3.00</td><td>3.77</td></tr><tr><td>SD-QA</td><td>36.98</td><td>50.72</td><td>43.40</td><td>57.41</td><td>63.12</td><td>56.06</td><td>68.54</td></tr><tr><td>MMSU</td><td>39.75</td><td>54.78</td><td>57.25</td><td>56.38</td><td>62.17</td><td>52.18</td><td>60.31</td></tr><tr><td>OpenBookQA</td><td>53.41</td><td>78.02</td><td>74.51</td><td>79.12</td><td>83.52</td><td>64.18</td><td>79.56</td></tr><tr><td>IFEval</td><td>52.80</td><td>49.25</td><td>54.54</td><td>53.88</td><td>61.10</td><td>38.01</td><td>59.30</td></tr><tr><td>AdvBench</td><td>88.08</td><td>97.69</td><td>97.31</td><td>99.62</td><td>100.00</td><td>93.08</td><td>98.65</td></tr><tr><td>Overall</td><td>59.83</td><td>71.69</td><td>71.14</td><td>72.83</td><td>76.93</td><td>63.84</td><td>76.02</td></tr><tr><td colspan="8">UltraEval-Audio (S2S)</td></tr><tr><td>AlpacaEval</td><td>51.00</td><td>51.00</td><td>58.69</td><td>56.10</td><td>44.20</td><td>51.72</td><td>49.65</td></tr><tr><td>Llama Q.</td><td>50.00</td><td>61.00</td><td>67.33</td><td>66.30</td><td>57.33</td><td>67.67</td><td>68.00</td></tr><tr><td>TriviaQA</td><td>36.40</td><td>40.20</td><td>30.57</td><td>40.52</td><td>35.71</td><td>33.50</td><td>35.35</td></tr><tr><td>Web Q.</td><td>32.00</td><td>40.00</td><td>38.09</td><td>38.93</td><td>33.90</td><td>34.65</td><td>37.65</td></tr><tr><td>Overall</td><td>42.35</td><td>48.05</td><td>48.67</td><td>50.46</td><td>42.79</td><td>46.89</td><td>47.66</td></tr><tr><td colspan="8">Big Bench Audio (S2T &amp; S2S)</td></tr><tr><td>S2T</td><td>44.8</td><td>56.2</td><td>47.1</td><td>54.2</td><td>59.4</td><td>50.9</td><td>71.6</td></tr><tr><td>S2S</td><td>42.7</td><td>55.4</td><td>44.6</td><td>53.6</td><td>51.0</td><td>47.5</td><td>60.9</td></tr><tr><td>Overall</td><td>43.8</td><td>55.8</td><td>45.8</td><td>53.9</td><td>55.2</td><td>49.2</td><td>66.3</td></tr></table>

Table 3:Speech quality performance on the collections of UltraEval-Audio, in terms of UTMOS for assessing overal speech quality and ASR-WER for assessing alignment between generated speech and text. $\mathbf { F R ( I n / O u t ) }$ indicates the input speech frame rate and the output (speech $^ +$ text) frame rate for the LLM backbone, while $\tau$ represents the average text tokens per second of speech.   

<table><tr><td>Model</td><td>FR(In/Out)↓</td><td>UTMOS↑</td><td>ASR-WER↓</td></tr><tr><td>MiniCPM-o 2.6 (2025)</td><td>25/τ</td><td>4.18</td><td>13.17</td></tr><tr><td>Baichuan-Omni-1.5 (2025)</td><td>12.5/12.5+7</td><td>4.27</td><td>23.38</td></tr><tr><td>Qwen2.5-Omni (2025)</td><td>25/T</td><td>4.28</td><td>3.48</td></tr><tr><td>Kimi-Audio (2025)</td><td>12.5/12.5</td><td>3.06</td><td>21.06</td></tr><tr><td>Step-Audio2-mini (2025)</td><td>12.5/25+T</td><td>4.53</td><td>9.5</td></tr><tr><td>DrVoice</td><td>5/5</td><td>4.29</td><td>11.2</td></tr></table>

Table 4: Ablation study of Continuous Speech Encoder (CSE), Speech Refined Head (SRH), SRHPretraining and CoM-Mixing Training on the Llama Questions dataset. S2M, S2T, etc are defined in Table 1.   

<table><tr><td>Model</td><td>S2M (T/S)</td><td>S2T</td><td>T2M (T/S)</td><td>T2T</td><td>STC (T/S)</td><td>SAC (T/S)</td><td>SUC (T/S)</td></tr><tr><td>DRVoICE-Small</td><td>68.67 / 56.00</td><td>72.33</td><td>72.33 / 56.00</td><td>75.33</td><td>75.67 / 68.33</td><td>71.67 / 62.67</td><td>73.33 / 62.00</td></tr><tr><td>w/o. CSE</td><td>61.67 / 53.00</td><td>62.33</td><td>70.00 / 60.00</td><td>74.00</td><td>69.33 / 61.00</td><td>63.00 / 55.00</td><td>66.33 / 58.67</td></tr><tr><td>w/o. SRH-Pretraining</td><td>38.33 / 30.33</td><td>56.00</td><td>59.33 / 46.33</td><td>73.33</td><td>67.33 / 57.67</td><td>54.00 / 42.33</td><td>54.33 / 42.67</td></tr><tr><td>w/o. SRH</td><td>21.67 / 15.33 56.00</td><td></td><td>45.22 / 35.00</td><td>73.00</td><td>64.33 / 50.67</td><td>55.67 / 42.33 40.33 / 27.67</td><td></td></tr><tr><td>w/o. CoM-Mixing</td><td>58.00 / 49.00</td><td>58.00</td><td>69.33 / 55.00</td><td>68.33</td><td>-/−</td><td>-/−</td><td>-/−</td></tr></table>

Text $S  T$ evaluation, UltraEval-Audio 2 for Speech-to-Speech $[ S  S ]$ evaluation, and Big Bench Audio 3 for both evaluations. Evaluation Metrics. Evaluations adhere to the established protocols for each respective benchmark. Specifically, for the open-ended QA tasks on AlpacaEval and CommonEval, G-Eval (Liu et al., 2023) is used for scoring. For AdvBench, the Refusal Rate is reported, while performance on all other benchmarks is assessed with Accuracy. The generated speech is transcribed using Whisper-v3-large model (Radford et al., 2022), then WER (denoted by ASR-WER) of transcripts is computed against the generated text to assess the alignment between generated speech and text. UTMOS (Saeki et al., 2022) is used to evaluate the overall speech quality, following Zeng et al. (2025). Baselines. Representive and competitive open-source audio language models are selected as baselines to cover diverse modeling paradigms: Text-Driven models including MiniCPM-o 2.6 (8B) (Yao et al., 2024) and Qwen2.5-Omni (7B) (Xu et al., 2025). Among Joint Speech-Text models, interleaved models including GLM-4-Voice (9B) (Zeng et al., 2025), Baichuan-Omni-1.5 (7B) (Li et al., 2025), and Step-Audio2-Mini (8B) (Wu et al., 2025), alongside the parallel model KimiAudio (7B) (KimiTeam et al., 2025). This suite enables systematic comparisons across mainstream speech-text modeling strategies. Implementation Details can be found in Appendix A.

# 4.2 Main Results

Overall Performance. Table 2 compares DrVoICE (7B) with representative and competitive baselines on speech-to-text $( S {  } \mathrm { T } )$ and speech-to-speech $( S {  } S )$ generation performance. As shown in the table, DrVoICE demonstrates exceptional capabilities across a wide range of tasks, achieving new SOTA on OpenAudioBench (audio understanding) with an overall score of 69.24 and on Big Bench Audio (reasoning and understanding) with a score of 66.3, significantly outperforming the second best model by over 10 points. DrVo1CE also achieves performance comparable to the SOTA on VoiceBench (benchmarking LLM-based voice assistants) with a score of 76.02, narrowly behind the leader's 76.93, and remains highly competitive on UltraEval-Audio (both speech understanding and generation). The strong and balanced performance across diverse benchmarks establishes DrVoICE as a leading open-source speech foundation model in the ${ \sim } 7 B$ parameter class.

Computational Efficiency and Speech Quality. A key innovation of DrVoICE is its remarkable computational efficiency, highlighted in Table 2 and Table 3. As shown in the FR $\mathbf { ( I n / O u t ) }$ rows, DRVoICE operates at a frame rate of 5/5, indicating that the LLM backbone processes only 5 audio tokens per second for both input and output. This is a substantial reduction compared to other models, which operate at frame rates of 12.5 or 25, thereby significantly lowering computational requirements and potential latency. Crucially, this efficiency does not come at the cost of speech quality. Table 3 shows that despite its low frame rate, DrVoicE produces high-quality and well-aligned speech. It achieves UTMOS score of 4.29 for overall speech quality, which is competitive with top-performing models like Qwen2.5-Omni (4.28) and superior to others like Kimi-Audio (3.06). With an ASR-WER of 11.2, the model shows robust speech-text alignment and surpasses several baselines. Still, it underperforms compared to Qwen2.5-Omni. A potential explanation lies in the architectural design: Qwen2.5-Omni's feeding text directly to its "Talker" module, whereas the proposed model only sends hidden states to SRH. Therefore, introducing text to the speech refined head is a logical next step for improving textual control and lowering the ASR-WER. This unique combination of SOTA performance, high-quality speech generation, and unparalleled efficiency makes DrVoICE a highly powerful and practical model for real-world applications.

# 4.3 Ablation and Analyses

We conduct extensive ablation study and analyses. For computational efficiency, some experiments are performed on DRVoICE-Small (1.5B). More analyses about the data quality and data scaling can be found in Appendix C. Core-Cocktail Training Strategy. While Stage 1 training causes a performance drop from text baseline of 81.77 to 70.19, Stage 2 reverses this trend, recovering the performance to 74.73. This result confirms that the strategy effectively counteracts the initial degradation and leads to an optimized model. Further details are available in Appendix C. Continuous Speech Encoder. As shown in Table 4, the Continuous Speech Encoder (CSE) is vital for tasks involving speech inputs. Removing it from DrVoICE-Small leads to a significant performance drop in speech understanding and speech generation. Specifically, the S2T score decreases from 72.33 to 62.33 ( $1 3 . 8 \%$ relatively), and the S2M (T) score falls from 68.67 to 61.67 ( $1 0 . 2 \%$ relatively). In contrast, the impact on text-only tasks is minimal, with the T2T score only slightly decreasing from 75.33 to 74.00. This confirms the CSE's effectiveness in representing speech.

Dual-Resolution Speech Representations (DRSR). Our dual-resolution approach combines input grouping for efficiency and comprehension with a refinement head for high-quality generation. 1) Grouping Factor. Contrary to degrading generation, grouping substantially improves both speech understanding (S2T) and speech-to-speech generation (S2M). For instance, increasing the grouping factor from 1 to 5 raises the S2T score from 55.67 to 63.33 (a $1 3 . 7 \%$ relative gain). The S2M (T/S) score sees a significant improvement, peaking at $3 7 . 6 7 \ : / \ : 2 8 . 0 0$ with a grouping factor of 5. Furthermore, usinga roupig factor of 5 insted o 1redues neary $50 \%$ GPU hours in each setting, proving the efficiency of grouping machinism. Detail results can be found in Appendix C. 2) Speech Refinement Head. As shown in Table 4, the Speech Refinement Head (SRH) is remarkably effective for speech token generation tasks. By comparing the model with SRH (w/o. SRH-Pretraining) to one without (w/o. SRH), we observe that adding SRH achieves a $7 6 . 9 \%$ relative improvement in S2M (T) (from 21.67 to 38.33) and a $3 1 . 2 \%$ relative improvement in T2M (T) (from 45.22 to 59.33). The textonly performance (T2T) remains stable, confirming that SRH enhances speech generation without interfering with text processing pathways. Our dual-resolution architecture effectively combines these components, using grouping for efficient comprehension and SRH at the raw frame resolution for speech generation. SRH-Pretraining. To examine the impact of pretraining, we remove the SRH pretraining step and retrain the model. As shown in Table 4, removing pretraining (comparing w/o. CSE with w/o. SRHPretraining) has the most significant impact on speech generation, causing S2M (T) performance to drop by $3 7 . 8 \%$ and T2M (T) by $1 5 . 2 \%$ .The effect on S2T is smaller ( $1 0 . 2 \%$ drop), and negligible for T2T. This underscores the critical importance of SRH pretraining for refining speech token generation. CoM-Mixing Training Strategy. As shown in Table 4, where tasks guided by contextual system prompts (STC/SAC/SUC) demonstrate substantial improvements over direct S2M generation. For example, STC (T) achieves a score of 75.67, significantly surpassing the S2M (T) baseline of 68.67. This shows the model successfully learns to adopt the generation paradigm guided by system prompts. Thedat    by blat h CoM-Mixi a ney. Tra without it (w/o. CoM-Mixing Training) results in a $1 5 . 5 \%$ relative performance degradation in S2M (T) (from 68.67 to 58.00), confirming the value of our mixed-modality training approach.

# 5 Conclusions

We introduce DrVoICE, a novel parallel speech-text voice conversation model that leverages joint autoregressive modeling with dual-resolution speech representations. Our experimental results demonstrated that DRVoICE establishes a new state-of-the-art (SOTA) on OpenAudioBench for audio understanding and on Big Bench Audio for reasoning capabilities. Furthermore, it achieved performance comparable to the SOTA on VoiceBench for voice assistant tasks and on UltraEvalAudio for both speech understanding and generation, confirming its strong and versatile capabilities. Notably, its dual-resolution mechanism significantly improves inference speed and computational efficiency without sacrificing performance. We discuss limitations and future work in Appendix D.

References   
Keyu An, Qian Chen, Chong Deng, Zhihao Du, Changfeng Gao, Zhifu Gao, Yue Gu, Ting He, Hangrui Hu, Kai Hu, Shengpeng Ji, Yabin Li, Zerui Li, Heng Lu, Haoneng Luo, Xiang Lv, Bin Ma, Ziyang Ma, Chongjia Ni, Changhe Song, Jiaqi Shi, Xian Shi, Hao Wang, Wen Wang, Yuxuan Wang, Zhangyu Xiao, Zhijie Yan, Yexin Yang, Bin Zhang, Qinglin Zhang, Shiliang Zhang, Nan Zhao, and Siqi Zheng. 2024. Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms. CoRR, abs/2407.04051.   
Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber. 2020. Common voice: A massively-multilingual speech corpus. In Proceedings of The 12th Language Resources and Evaluation Conference, LREC 2020, Marseille, France, May 11-16, 2020, pages 42184222. European Language Resources Association.   
Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matthew Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. 2023. Audiolm: A language modeling approach to audio generation. IEEE ACM Trans. Audio Speech Lang. Process., 31:25232533.   
Qian Chen, Yafeng Chen, Yanni Chen, Mengzhe Chen, Yingda Chen, Chong Deng, Zhihao Du, Ruize Gao, Changfeng Gao, Zhifu Gao, Yabin Li, Xiang Lv, Jiaqing Liu, Haoneng Luo, Bin Ma, Chongjia Ni, Xian Shi, Jialong Tang, Hui Wang, Hao Wang, Wen Wang, Yuxuan Wang, Yunlan Xu, Fan Yu, Zhijie Yan, Yexin Yang, Baosong Yang, Xian Yang, Guanrou Yang, Tianyu Zhao, Qinglin Zhang, Shiliang Zhang, Nan Zhao, Pei Zhang, Chong Zhang, and Jinren Zhou. 2025. Minmo: A multimodal large language model for seamless voice interaction. CoRR, abs/2501.06282.   
Wenxi Chen, Ziyang Ma, Ruiqi Yan, Yuzhe Liang, Xiquan Li, Ruiyang Xu, Zhikang Niu, Yanqiao Zhu, Yifan Yang, Zhanxun Liu, et al. 2024a. Slam-omni: Timbre-controllable voice interaction system with single-stage training. arXiv preprint arXiv:2412.15649.   
Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, and Haizhou Li. 2024b. Voicebench: Benchmarking llm-based voice assistants. CoRR, abs/2410.17196.   
Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. 2024. Qwen2-audio technical report. CoRR, abs/2407.10759.   
Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. 2023. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. CoRR, abs/2311.07919.   
Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. 2023. High fidelity neural audio compression. Trans. Mach. Learn. Res., 2023.   
Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. 2024. Moshi: a speech-text foundation model for real-time dialogue. CoRR, abs/2410.00037.   
Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. 2024. Moshi: a speech-text foundation model for real-time dialogue. Technical report.   
Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, Zhifu Gao, and Zhijie Yan. 2024a. Cosyvoice: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens. CoRR, abs/2407.05407.   
Zhihao Du, Changfeng Gao, Yuxuan Wang, Fan Yu, Tianyu Zhao, Hao Wang, Xiang Lv, Hui Wang, Chongjia Ni, Xian Shi, Keyu An, Guanrou Yang, Yabin Li, Yanni Chen, Zhifu Gao, Qian Chen, Yue Gu, Mengzhe Chen, Yafeng Chen, Shiliang Zhang, Wen Wang, and Jieping Ye. 2025. Cosyvoice 3: Towards in-the-wild speech generation via scaling-up and post-training. CoRR, abs/2505.17589. Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, Fan Yu, Huadai Liu, Zhengyan Sheng, Yue Gu, Chong Deng, Wen Wang, Shiliang Zhang, Zhijie Yan, and Jingren Zhou. 2024b. Cosyvoice 2: Scalable streaming speech synthesis with large language models. CoRR, abs/2412.10117. Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. 2024. Llamaomni: Seamless speech interaction with large language models. CoRR, abs/2409.06666. Chaoyou Fu, Haojia Lin, Xiong Wang, Yifan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, and Ran He. 2025. VITA-1.5: towards gpt-4o level real-time vision and speech interaction. CoRR, abs/2501.01957. Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Défossez, Gabriel Synnaeve, Emmanuel Dupoux, Roy Schwartz, and Yossi Adi. 2023. Textually pretrained speech language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE ACM Trans. Audio Speech Lang. Process., 29:34513460.

Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Brian Li, Changyi Wan, Hanpeng Hu, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Kang An, Wei Ji, Wen Li, Xuan Wen, Yuankai Ma, Yuanwei Liang, Yun Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxin Miao, Chen Xu, Chengting Feng, Chenrun Wang, Dapeng Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Haonan Jia, Haoyang Zhang, Jiahao Gong, JWu Ju Ja u Ja he JiFe  Wu,J W Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kaixiang Li, Lei Xia, Li Zhou, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingyao Liang, Na Wang, Nie Hao, Qiling Wu, Qinyuan Tan, Shaoliang Pang, Shiliang Yang, Shuli Gao, Siqi Liu, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wenjin Deng, Wenqing He, Wen Sun, Xin Han, Xiaomin Deng, Xiaojia Liu, Xu Zhao, Yanan Wei, Yanbo Yu, Yang Cao, Yangguang Li, Yangzhen Ma, Yanming Xu, Yaqiang Shi, Yilei Wang, Yinmin Zhong, Yu Luo, Yuanwei Lu, Yuhe Yin, Yuting Yan, Yuxiang Yang, Zhe Xie, Zheng Ge, Zheng Sun, Zhewei Huang, Zhichao Chang, Zidong Yang, Zili Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, and Yibo Zhu. 2025. Step-audio: Unified understanding and generation in intelligent speech interaction. Preprint, arXiv:2502.11946. Dhiraj D. Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander Heinecke, Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy, Bharat Kaul, and Pradeep Dubey. 2019. A study of BFLOAT16 for deep learning training. CoRR, abs/1905.12322. KimiTeam, Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, Zhengtao Wang, Chu Wei, Yifei Xin, Xinran Xu, Jianwei Yu, Yutao Zhang, Xinyu Zhou, Y. Charles, Jun Chen, Yanru Chen, Yulun Du, Weiran He, Zhenxing Hu, Guokun Lai, Qingcheng Li, Yangyang Liu, Weidong Sun, Jianzhou Wang, Yuzhi Wang, Yuefeng Wu, Yuxin Wu, Dongchao Yang, Hao Yang, Ying Yang, Zhilin Yang, Aoxiong Yin, Ruibin Yuan, Yutong Zhang, and Zaida Zhou. 2025. Kimi-audio technical report. Preprint, arXiv:2504.18425. Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. 2020. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Yadong Li, Jun Liu, Tao Zhang, Song Chen, Tianpeng Li, Zehuan Li, Lijun Liu, Lingfeng Ming, Guosheng Dong, Da Pan, et al. 2025. Baichuan-omni-1.5 technical report. arXiv preprint arXiv:2501.15368. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. 2023. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 25112522. Association for Computational Linguistics. Zuwei Long, Yunhang Shen, Chaoyou Fu, Heting Gao, Lijiang Li, Peixian Chen, Mengdan Zhang, Hang Shao, Jian Li, Jinlong Peng, Haoyu Cao, Ke Li, Rongrong Ji, and Xing Sun. 2025. Vita-audio: Fast interleaved cross-modal token generation for efficient large speech-language model. CoRR, abs/2505.03739. Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. OpenBMB MiniCPM-o Team. 2025. Minicpm-o 2.6: A gpt-4o level mllm for vision, speech, and multimodal live streaming on your phone. Patrick K. O'Neill, Vitaly Lavrukhin, Somshubra Majumdar, Vahid Noroozi, Yuekai Zhang, Oleksii Kuchaiev, Jagadeesh Balam, Yuliya Dovzhenko, Keenan Freyberg, Michael D. Shulman, Boris Ginsburg, Shinji Watanabe, and Georg Kucsko. 2021. Spgispeech: 5, 000 hours of transcribed financial audio for fully formatted end-to-end speech recognition. In 22nd Annual Conference of the International Speech Communication Association, Interspeech 2021, Brno, Czechia, August 30 - September 3, 2021, pages 14341438. ISCA. OpenAI. 2024b. Hello GPT-40. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An ASR corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015, pages 52065210. IEEE. Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. 2019. MELD: A multimodal multi-party dataset for emotion recognition in conversations. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 527536. Association for Computational Linguistics. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. Robust speech recognition via large-scale weak supervision. arXiv preprint. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2849228518. PMLR. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, page 20. IEEE/ACM. Takaaki Saeki, Detai Xin, Wataru Nakata, Tomoki Koriyama, Shinnosuke Takamichi, and Hiroshi Saruwatari. 2022. UTMOS: utokyo-sarulab system for voicemos challenge 2022. In 23rd Annual Conference of the International Speech Communication Association, Interspeech 2022, Incheon, Korea, September 18-22, 2022, pages 45214525. ISCA. Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Miguel Pino, and Emmanuel Dupoux. 2021. Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 9931003. Association for Computational Linguistics. Qichao Wang, Ziqiao Meng, Wenqian Cui, Yifei Zhang, Pengcheng Wu, Bingzhe Wu, Zibin Zheng, Irwin King, Liang Chen, and Peilin Zhao. 2025. Parrot: Seamless spoken dialogue interaction with double-channel large language models. Xiong Wang, Yangze Li, Chaoyou Fu, Yunhang Shen, Lei Xie, Ke Li, Xing Sun, and Long Ma. 2024. Freeze-omni: A smart and low latency speech-to-speech dialogue model with frozen LLM. CoRR, abs/2411.00774.

Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang Zhang, Jingbei Li, Mingrui Chen, Peng Liu, Wang You, Xiangyu Tony Zhang, Xingyuan Li, Xuerui Yang, Yayue Deng, Yechang Huang, Yuxin Li, Yuxin Zhang, Zhao You, Brian Li, Changyi Wan, Hanpeng Hu, Jiangjie Zhen, Siyu Chen, Song Yuan, Xuelin Zhang, Yimin Jiang, Yu Zhou, Yuxiang Yang, Bingxin Li, Buyun Ma, Changhe Song, Dongqing Pang, Guoqiang Hu, Haiyang Sun, Kang An, Na Wang, Shuli Gao, Wei Ji, Wen Li, Wen Sun, Xuan Wen, Yong Ren, Yuankai Ma, Yufan Lu, Bin Wang, Bo Li, Changxin Miao, Che Liu, Chen Xu, Dapeng Shi, Dingyuan Hu, Donghang Wu, Enle Liu, Guanzhe Huang, Gulin Yan, Han Zhang, Nie Hao, Haonan Jia, Hongyu Zhou, Jianjian Sun, Jiaoren Wu, Jie Wu, Jie Yang, Jin Yang, Junzhe Lin, Kaixiang Li, Lei Yang, Liying Shi, Li Zhou, Longlong Gu, Ming Li, Mingliang Li, Mingxiao Li, Nan Wu, Qi Han, Qinyuan Tan, Shaoliang Pang, Shengjie Fan, Siqi Liu, Tiancheng Cao, Wanying Lu, Wenqing He, Wuxun Xie, Xu Zhao, Xueqi Li, Yanbo Yu, Yang Yang, Yi Liu, Yifan Lu, Yilei Wang, Yuanhao Ding, Yuanwei Liang, Yuanwei Lu, Yuchu Luo, Yuhe Yin, Yumeng Zhan, and Yuxiang Zhang. 2025. Step-audio 2 technical report. CoRR, abs/2507.16632. Shitao Xiao, Zheng Liu, Peitian Zhang, and Xingrun Xing. 2024. Lm-cocktail: Resilient tuning of language models via model merging. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 24742488. Association for Computational Linguistics. Zhifei Xie and Changqiao Wu. 2024a. Mini-omni: Language models can hear, talk while thinking in streaming. CoRR, abs/2408.16725. Zhifei Xie and Changqiao Wu. 2024b. Mini-omni2: Towards open-source gpt-4o with vision, speech and duplex capabilities. CoRR, abs/2410.11190. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. 2025. Qwen2.5-omni technical report. CoRR, abs/2503.20215. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. 2024. Minicpm-v: A gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800. Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Shengmin Jiang, Yuxiao Dong, and Jie Tang. 2025. Scaling speech-text pre-training with synthetic interleaved data. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023a. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 1575715773. Association for Computational Linguistics. Qinglin Zhang, Luyao Cheng, Chong Deng, Qian Chen, Wen Wang, Siqi Zheng, Jiaqing Liu, Hai Yu, and Chaohong Tan. 2024a. Omniflatten: An end-to-end GPT model for seamless voice conversation. CoRR, abs/2410.17799. Xin Zhang, Xiang Lyu, Zhihao Du, Qian Chen, Dong Zhang, Hangrui Hu, Chaohong Tan, Tianyu Zhao, Yuxuan Wang, Bin Zhang, Heng Lu, Yaqian Zhou, and Xipeng Qiu. 2024b. Intrinsicvoice: Empowering llms with intrinsic real-time voice interaction abilities. CoRR, abs/2410.08035.

# Appendices

# A Implementation Details

Qwen2.5-0.5B is trained following $T 2 M$ paradigm with Speech-text Alignment data to initialize SRH (SRH-PT). The maximum sequence length is set to 2K tokens, which corresponds to an audio duration of approximately 6.8 minutes. DRVoICE uses Qwen2.5-7B-Instruct as its base LLM, while DrVoICE-Small utilizes Qwen2.5-1.5B-Instruct as the base LLM. Grouping factor is set to $k = 5$ Core-cocktail interpolated factor is set to $\alpha = 0$ for extremely maintaining the base LLM capability. Multiple loss hyperparameters are set to $\lambda = 1$ and $\mu = 1$ . The warmup rate is set to $2 \%$ of the total training steps. For the two-stage training of DrVoiCE, the learning rate is decayed from $1 \times 1 0 ^ { - 4 }$ to $1 \times \mathrm { 1 0 ^ { - 5 } }$ in stage one, and subsequently from $2 \times 1 0 ^ { - 5 }$ to $2 \times 1 0 ^ { - 6 }$ in stage two, with both stages utilizing a cosine annealing schedule. In contrast, DrVoICE-Small and SRH-PT are trained in a single stage, which adopts the same learning rate schedule as the first stage of DrVoICE. The AdamW method (Loshchilov and Hutter, 2019) is used for optimization. Experiments are run on $6 4 \mathrm { x }$ NVIDIA Tesla A800 80G GPUs with Brain floating-point format (BF16) (Kalamkar et al., 2019) to accelerate training and decoding. For DRVoICE, DeepSpeed ZeRO-2 (Rajbhandari et al., 2020) is implemented to prevent GPU out-of-memory. It takes about 20 hours on SRH-PT, and about 45 hours on DRVoICE post-training.

# B Prompt Templates

System prompts for multimodal interaction patterns. As shown in Table 5, there are five types of system prompts categorized based on the model's output partitioning. During inference, specifying these system prompts enables the generation of desired output results.

Table 5: System Prompts for Multimodal Interaction.   

<table><tr><td colspan="2">Pattern Abbr. System Prompts</td></tr><tr><td>S2M &amp; T2M</td><td>You are a helpful assistant and asked to generate both text and speech tokens at the same time.</td></tr><tr><td>S2T &amp; T2T</td><td>You are a helpful assistant and asked to generate text tokens.</td></tr><tr><td>STC</td><td>You are a helpful assistant. Let&#x27;s think step by step. Convert speech to text if the query is speech, think of an appropriate text response, and then convert the response back to both text and speech tokens at the same time.</td></tr><tr><td>SAC</td><td>You are a helpful assistant. Let&#x27;s think step by step. Think of an appropriate text response, and then convert the response back to both text and speech tokens at the same time.</td></tr><tr><td>SUC</td><td>You are a helpful assistant. Let&#x27;s think step by step. Convert speech to text if the query is speech, and then think of both appropriate text and speech responses at the same time.</td></tr></table>

# C More Ablation and Analyses

Core-Cocktail Training Strategy. Table 6 shows the precise trade-offs and benefits of our twostage approach. For instance, under the comprehensive Inhouse v2, MagpiePro, InfGen dataset setting, the model's average performance after Stage 1 drops to 70.19, a significant decline of over 11 points compared to the text-only baseline (81.77). This observation perfectly aligns with our initial hypothesis: the aggressive full fine-tuning with a high learning rate in Stage 1, while intended to rapidly move parameters into a more favorable region of the loss landscape, temporarily compromises the model's general capabilities, as predicted. This performance degradation is precisely the problem that the subsequent steps are designed to rectify. Following the intermediate merging step—which "cocktails" the aggressively trained model with the base LLM to re-integrate its robust knowledge—Stage 2 proceeds with careful fine-tuning using a small learning rate. As shown in the table, this second stage successfully recovers and refines the model, boosting the average score significantly to 74.73. This substantial improvement from Stage 1 demonstrates that our strategy effectively mitigates the instability caused by the initial high-learning-rate training. This two-stage process confirms that the Core-Cocktail strategy provides a powerful solution to the learning rate dilemma, enabling rapid adaptation without sacrificing final performance.

Table 6: Training Strategy Performance comparison on the Voicebench benchmark with different training datasets.   

<table><tr><td>Description</td><td></td><td>AlpacaEval CommonEval SD-QA</td><td></td><td>MMSU</td><td>OpenBookQA IFEval AdvBench</td><td></td><td></td><td>Avg</td></tr><tr><td>Qwen2.5-7B-Instruct (T2T)</td><td>4.67</td><td>4.34</td><td>76.13</td><td>69.97</td><td>82.20</td><td>65.45</td><td>99.04</td><td>81.86</td></tr><tr><td colspan="9">Inhouse v1</td></tr><tr><td>OnlyText-Training (T2T)</td><td>4.15</td><td>3.57</td><td>60.15</td><td>54.50</td><td>66.50</td><td>50.22</td><td>99.40</td><td>69.31</td></tr><tr><td>Stage 2 (S2T)</td><td>4.08</td><td>3.54</td><td>55.88</td><td>52.80</td><td>69.45</td><td>38.48</td><td>99.23</td><td>66.89</td></tr><tr><td colspan="9">Inhouse v2</td></tr><tr><td>OnlyText-Training (T2T)</td><td>4.22</td><td>3.60</td><td>68.35</td><td>58.10</td><td>68.13</td><td>65.69</td><td>99.62</td><td>73.76</td></tr><tr><td>Stage 2 (S2T)</td><td>4.10</td><td>3.15</td><td>64.01</td><td>58.30</td><td>79.56</td><td>55.25</td><td>98.27</td><td>71.48</td></tr><tr><td colspan="9">Inhouse v2, MagpiePro, InfGen</td></tr><tr><td>OnlyText-Training (T2T)</td><td>4.64</td><td>4.26</td><td>71.79</td><td>69.97</td><td>84.40</td><td>68.84</td><td>99.42</td><td>81.77</td></tr><tr><td>Stage 1 (S2T)</td><td>4.25</td><td>3.09</td><td>57.32</td><td>54.29</td><td>75.82</td><td>58.07</td><td>99.04</td><td>70.19</td></tr><tr><td>Stage 2 (S2T)</td><td>4.54</td><td>3.35</td><td>64.56</td><td>61.61</td><td>80.44</td><td>559.83</td><td>98.85</td><td>74.73</td></tr><tr><td>Stage 2 w/. ASR data (S2T)</td><td>4.52</td><td>3.77</td><td>68.54</td><td>60.31</td><td>79.56</td><td>59.30</td><td>98.65</td><td>76.02</td></tr></table>

![](images/2.jpg)  

Figure 2: Computational Resources under 17K hours training data across different Grouping Factor.

Data Quality. The OnlyText-Training (T2T) results in Table 6 serve as a powerful indicator of the maximum potential embedded within the textual content of our datasets. There is a clear correlation between data quality and the final model's performance: as the T2T average score improves from Inhouse v1 (69.31) to our most comprehensive dataset (81.77), the final Stage 2 (S2T) model performance also sees a substantial lift. Notably, our final dataset mixture enables the T2T model to reach an average score of 81.77, almost perfectly matching the text-only Qwen2.5-7B-Instruct backbone. This confirms the exceptional quality of our instructional data and establishes a highperformance ceiling. Our final S2T model (74.73) successfully translates a significant portion of this potential into the multimodal domain. Furthermore, the results underscore the critical importance of data composition for specific tasks. The CommonEval benchmark, which evaluates models on real human speech, is a case in point. Our standard Stage 2 (S2T) model achieves a score of 3.35 on this challenging benchmark. However, by augmenting the training data with ASR-derived pairs (Stage 2 w/. ASR data), the CommonEval score noticeably increases to 3.77. This gain is because ASR data exposes the model to the natural disfluencies, varied intonations, and ambient noise present in real-world speech, enhancing its robustness.

![](images/3.jpg)  

Figure 3: Performance Scaling of DrVoICE-Small (w/o. Continuous Speech Encoder) on LLaMA Question Benchmark.

Table 7: Impact of the Grouping Factor on Llama Q. S2T and S2M performance, using DRVoICESmall (1.5B) w/o CSE and data subsets for faster experiments. The S2T model is trained exclusively on mixed S2T and T2T data, while the S2M model is trained on mixed S2M and T2T data.

<table><tr><td>Grouping Factor</td><td>S2T</td><td>S2M (T/S)</td></tr><tr><td>1</td><td>55.67</td><td>4.00 / 2.67</td></tr><tr><td>3</td><td>64.67</td><td>15.67 / 5.00</td></tr><tr><td>5</td><td>63.33</td><td>37.67 / 28.00</td></tr><tr><td>7</td><td>62.67</td><td>36.00 / 16.67</td></tr></table>

Data Scaling. To investigate the impact of data scaling, we conduct experiments by progressively expanding the S2S training data on DRVoICE-Small (w/o. Continous Speech Encoder). Each S2S instance can be augmented into 7 distinct multimodal interaction patterns. As illustrated in Figure 3, expanding the dataset from 3.6K to 17.8K samples yields consistent improvements. The performance of T2M(S) exhibits nearly linear growth, while other patterns, despite showing slower growth rates, still demonstrate upward trends in the figure, suggesting potential for further performance enhancement through additional data. Grouping Factor. Table 7 and Figure 2 demonstrate that our grouping strategy significantly enhances both performance and computational efficiency. Contrary to degrading generation, grouping substantially improves both speech understanding (S2T) and speech-to-speech generation (S2M). For instance, increasing the grouping factor from 1 to 5 raises the S2T score from 55.67 to 63.33 (a $1 3 . 7 \%$ relative gain). The S2M (T/S) score sees a significant improvement, peaking at 37.67 $/ 2 8 . 0 0$ with a grouping factor of 5. Furthermore, as shown in Figure 2, using a grouping factor of 5 instead of 1 reduces nearly $50 \%$ GPU hours in each setting, proving the efficiency of grouping machinism.

Retention vs Forgetting. An attempt is made to select suitable open-source datasets for speech synthesis. All results are from fine-tuning Qwen2.5-7B-Instruct at a low learning rate (1e-5), so they primarily indicate retention vs. forgetting rather than absolute capability gains. The gap to the baseline quantifies forgetting: magpie_pro_llama3_3_500k_fltered and InfGen exhibit the smallest overall deltas (about 34 points on both suites), while very large, noisier corpora like Inf7M induce substantial forgetting despite their size. Quality and filtering matter more than scale: openorca_gpt4 forgets less than openorca_gpt3_5 with fewer tokens, and the magpie_pro family maintains instruction-following and factual QA best per token. Forgetting is selective: some submetrics improve (e.g., LlamaQ rises for Inf7M/tulu_3_sft; AdvBench reaches 100 for tulu_3_sft), but these gains often trade off against instruction and QA scores. Overall, compact, well-filtered, model-proximal data minimizes destructive drift at 1e-5, whereas bulk token count tends to amplify forgetting, so the most stable recipes preserve the base model while making targeted adjustments.

Table 8: Benchmark Results: VoiceBench (T2T)   

<table><tr><td rowspan="2">Data</td><td colspan="8">VoiceBench</td></tr><tr><td>AlpacaEval</td><td>CommonEval</td><td>SD-QA</td><td>MMSU</td><td>OBQA</td><td>IFEval</td><td>AdvBench</td><td>Overall</td></tr><tr><td>Qwen2.5-7B-Instruct</td><td>4.67</td><td>4.34</td><td>76.13</td><td>69.97</td><td>82.20</td><td>65.45</td><td>99.04</td><td>81.86</td></tr><tr><td>Inf7M</td><td>3.83</td><td>3.50</td><td>69.98</td><td>55.43</td><td>71.87</td><td>48.79</td><td>94.42</td><td>69.58</td></tr><tr><td>InfGen</td><td>4.51</td><td>4.13</td><td>74.14</td><td>62.46</td><td>76.04</td><td>58.35</td><td>98.46</td><td>77.46</td></tr><tr><td>openorca_gpt4</td><td>4.33</td><td>4.03</td><td>73.96</td><td>61.00</td><td>73.85</td><td>54.05</td><td>96.35</td><td>75.20</td></tr><tr><td>openorca_gpt3_5</td><td>4.35</td><td>4.17</td><td>71.07</td><td>57.84</td><td>66.59</td><td>49.39</td><td>95.00</td><td>72.90</td></tr><tr><td>Evol_Instruct_Code</td><td>4.19</td><td>4.06</td><td>69.26</td><td>63.14</td><td>81.54</td><td>56.40</td><td>90.38</td><td>75.10</td></tr><tr><td>Evol_Instruct</td><td>4.09</td><td>3.91</td><td>71.61</td><td>60.38</td><td>72.31</td><td>54.02</td><td>98.27</td><td>73.80</td></tr><tr><td>magpie_pro_llama3_1_300k</td><td>4.42</td><td>4.08</td><td>72.69</td><td>58.33</td><td>74.95</td><td>56.08</td><td>86.73</td><td>74.11</td></tr><tr><td>magpie_pro_mt_llama3_1_300k</td><td>4.46</td><td>4.05</td><td>73.60</td><td>58.07</td><td>60.00</td><td>60.05</td><td>86.35</td><td>72.61</td></tr><tr><td>numinamath_cot</td><td>4.11</td><td>3.78</td><td>70.16</td><td>55.43</td><td>68.35</td><td>52.00</td><td>97.69</td><td>71.63</td></tr><tr><td>OpenHermes_v2_5</td><td>4.05</td><td>3.71</td><td>71.61</td><td>62.20</td><td>74.07</td><td>51.59</td><td>82.88</td><td>71.08</td></tr><tr><td>Synthia_v1_3</td><td>4.30</td><td>3.96</td><td>72.33</td><td>63.63</td><td>76.70</td><td>53.32</td><td>97.69</td><td>75.55</td></tr><tr><td>tulu_3_sft</td><td>4.04</td><td>3.73</td><td>65.64</td><td>61.74</td><td>79.78</td><td>61.78</td><td>100.00</td><td>74.91</td></tr><tr><td>magpie_pro_llama3_3_500k_filtered</td><td>4.48</td><td>4.17</td><td>73.42</td><td>64.31</td><td>81.32</td><td>64.81</td><td>92.88</td><td>78.53</td></tr></table>

Table 9: Benchmark Results: UltraEval-Audio (T2T) & OpenAudioBench (T2T)   

<table><tr><td rowspan="2">Data</td><td rowspan="2">Tokens (M)</td><td colspan="5">UltraEval-Audio</td><td>OpenAudioBench</td><td rowspan="2">Total Avg</td></tr><tr><td>AlpacaEval</td><td>LlamaQ</td><td>TriviaQA</td><td>WebQ</td><td>Overall</td><td>Reasoning QA</td></tr><tr><td>Qwen2.5-7B-Instruct</td><td></td><td>81.41</td><td>82.33</td><td>53.91</td><td>52.56</td><td>67.55</td><td>58.00</td><td>75.10</td></tr><tr><td>Inf7M</td><td>2406</td><td>55.35</td><td>84.67</td><td>46.29</td><td>47.59</td><td>58.48</td><td>48.51</td><td>64.13</td></tr><tr><td>InfGen</td><td>1022.4</td><td>74.29</td><td>82.33</td><td>55.96</td><td>51.18</td><td>65.94</td><td>57.43</td><td>71.95</td></tr><tr><td>openorca_gpt4</td><td>361.8</td><td>66.21</td><td>83.00</td><td>54.39</td><td>49.56</td><td>63.29</td><td>54.46</td><td>69.50</td></tr><tr><td>openorca_gpt3_5</td><td>1101</td><td>55.91</td><td>83.00</td><td>54.69</td><td>48.47</td><td>60.52</td><td>48.51</td><td>66.74</td></tr><tr><td>Evol_Instruct_Code</td><td>28.3</td><td>52.32</td><td>81.33</td><td>52.64</td><td>44.24</td><td>57.63</td><td>52.48</td><td>67.39</td></tr><tr><td>Evol_Instruct</td><td>69.2</td><td>56.62</td><td>81.67</td><td>53.81</td><td>47.15</td><td>59.81</td><td>53.47</td><td>67.44</td></tr><tr><td>magpie_pro_llama3_1_300k</td><td>213.9</td><td>76.16</td><td>83.67</td><td>57.91</td><td>55.02</td><td>68.19</td><td>53.96</td><td>70.46</td></tr><tr><td>magpie_pro_mt_llama3_1_300k</td><td>323.2</td><td>75.35</td><td>83.00</td><td>54.00</td><td>54.53</td><td>66.72</td><td>57.43</td><td>69.38</td></tr><tr><td>numinamath_cot</td><td>463.2</td><td>64.04</td><td>80.67</td><td>54.88</td><td>50.64</td><td>62.56</td><td>50.00</td><td>66.81</td></tr><tr><td>OpenHermes_v2_5</td><td>375.9</td><td>58.54</td><td>83.67</td><td>54.49</td><td>49.85</td><td>61.64</td><td>54.46</td><td>66.55</td></tr><tr><td>Synthia_v1_3</td><td>59</td><td>61.67</td><td>83.67</td><td>53.81</td><td>47.88</td><td>61.76</td><td>55.45</td><td>69.28</td></tr><tr><td>tulu_3_sft</td><td>570.2</td><td>55.91</td><td>84.33</td><td>53.52</td><td>49.41</td><td>60.79</td><td>55.94</td><td>68.62</td></tr><tr><td>magpie_pro_llama3_3_500k_filtered</td><td>314.6</td><td>78.64</td><td>79.00</td><td>57.32</td><td>54.18</td><td>67.29</td><td>51.49</td><td>72.53</td></tr></table>

# D Limitations and Future Work

Our future work will address limitations of this work and advance three key areas: (1) Enhancing Speech Generation Quality: The model currently exhibits weaker alignment between text and speech in speech-to-speech generation when compared to Qwen2.5-Omni. This performance gap can be atributed to a key architectural difference in the final speech generation stage. The Qwen2.5- Omni method directly feeds text into its talker module to ensure high textual fidelity. In contrast, DRVoICE only inputs hidden states into its speech refined head (SRH). Therefore, a promising direction for future work is to incorporate text as an additional input to the SRH. It is hypothesized that this modification will provide more explicit textual guidance, further enhancing speech-text alignment and leading to a significant reduction in ASR-WER. (2) Enabling Full-Duplex Interaction: A crucial direction is to enable full-duplex interaction for more natural conversations. Inspired by recent advancements like Parrot (Wang et al., 2025), future work will investigate the use of a time-division multiplexing (TDM) input stream. This would permit DRVoICE to accept user speech inputs during its own speech generation phase, thereby efficiently utilizing idle time slots and allowing for responsive, interruptible dialogue. (3) Expanding to General Audio and Multimodality: Finally, a promising avenue is to extend the model's capabilities beyond speech-centric tasks to broader audio comprehension and synthesis, including the recognition and generation of music and environmental sounds. The subsequent integration of the visual modality will be a critical step toward developing a more comprehensive, multimodal conversational AI.